{"pages":[{"title":"","text":"GitHub Tableau Public","link":"/about/index.html"}],"posts":[{"title":"[统计学习]第六章逻辑斯蒂回归与最大熵模型","text":"逻辑斯蒂回归，即逻辑回归（Logistic Regression），虽然是使用了回归方程但实际该模型是用于解决分类问题的经典算法。逻辑回归的方程 $f(x)=\\frac{\\exp^{w \\cdot x}}{1+\\exp^{w\\cdot x}}$ ，在 $w \\cdot x$ 的结果在实数范围内的分布结果如下： 1. 逻辑斯谛模型该模型在分类上是通过事件几率（odds，判断事件发生的概率和不发生的概率的比值）来判断事件的分类，其中发生和不发生的概率分别表示为 $P(y=1|X)$ 和 $P(y=0|X)$ 。因此模型相关计算过程如下:$$\\begin{align}P(y=1|X)&amp;=\\frac{\\exp^{(W\\cdot X)}}{1+\\exp^{W\\cdot X}} \\P(y=0|X)&amp;= 1- P(y=1|X)\\&amp;=1-\\frac{\\exp^{(W\\cdot X)}}{1+\\exp^{W\\cdot X}}\\&amp;=\\frac{1}{1+\\exp^{W\\cdot X}} \\\\text{odds}&amp;=\\frac{P(y=1|X)}{P(y=0|X)} \\&amp;=\\frac{P(y=1|X)}{1-P(y=1|X)}\\end{align}$$ 通过对上面的几率表达式进行取对数之后，得到的是一个线性模型 $\\log{\\frac{P(y=1|X)}{P(y=0|X)}}=W\\cdot X$。通过该表达式来判断分类情况，线性函数越趋近于正无穷那么值越趋近于类别为 $1$ 的分类，反之则越趋近于 $0$ 的分类。综合以上信息，可以看出逻辑斯谛模型是一个对数线性模型，也被称为逻辑回归模型的原因。 1.1 逻辑斯谛模型参数估计逻辑回归模型的参数估计方法，是针对给定的训练数据集 $T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N) }$ 通过最大似然估计的方法进行计算参数 $\\displaystyle{\\prod_{i=1}^N}P(\\hat{y}=1|x_i)^{y_i} P(\\hat{y}=0|x_i)^{1-y_i}$。 将似然函数进行对数处理，那么最终的模型参数估计的损失函数为 $L(W)=\\displaystyle{\\sum_{i=1}^N}[y_i(W\\cdot x_i)-\\log{(1+\\exp^{W\\cdot x_i})}]$。当 $L(W)$ 取得极大值时，即得到对参数 $W$ 的估计值。训练过程中的方法时采用了梯度下降法和拟牛顿法，进行训练得到参数 $W$ 即可以用于搭建逻辑回归模型。 2. 最大熵模型最大熵模型（Maximum Entropy Model）是以最大熵原理实现的模型。搭建概率模型时当找到熵达到最大的模型，即在概率模型空间中找到了最佳模型。 2.1 最大熵原理最大熵是以给定数据中，当概率分布的熵最大时能够表达出当前数据状态信息。对一个随机变量 $X$ 的概率质量函数(Probability Mass Function) $p(X)$，计算其熵 $H(X)=-\\displaystyle{\\sum_{x}p(x)\\log p(x)}$（注意在信息论中，通常对数底取 $2$，熵的单位为位(bit)）。 最大熵模型应用的是分类模型中条件概率分布 $P(Y|X), X\\in \\mathcal{X}, Y\\in \\mathcal{Y}$，其中 $\\mathcal{X}\\subseteq R^{n}$ 是表示数据输入，而 $\\mathcal{Y}$ 表示输出分类结果。模型的概率分布使用联合分布和边缘分布，进行搭建 $P(Y|X)=\\frac{P(Y,X)}{P(X)}$。对于联合分布和边缘分布，是根据给定数据集的经验值进行计算。模型中的经验分布以及其他相关的计算概念： 联合分布的经验分布， $\\widetilde{P}(X=x,Y=y)=\\frac{\\nu(X=x,Y=y)}{N}$ 边缘分布的经验分布，$\\widetilde{P}(X=x)=\\frac{\\nu(X=x)}{N}$ 根据数据集搭建的二值特征的函数 $f(x,y)$ ，在 $x$ 和 $y$ 满足条件时取 $1$ 否者取 $0$ $\\nu$ 表示的计数函数， $N$ 表示样本容量 使用特征函数、边缘分布 $\\widetilde{P}(X)$ 以及模型 $P(Y|X)$ 的期望值 $E_P(f)$ 结果为: $E_P(f)=\\displaystyle{\\sum_{x,y; x\\in X, y \\in Y}\\widetilde{P}(X)P(y|x)f(x,y)}$ 。当模型能够获取到数据信息，联合分布的经验分布与特征函数的期望值 $E_{\\widetilde{P}}(f)=\\displaystyle{\\sum_{x,y}\\widetilde{P}(x,y)f(x,y)}$ 与$E_p(f)$ 值相同。 根据 $E_{\\widetilde{P}}(f)=E_P(f)$ 的条件得到所有的模型 $\\mathcal{C}\\equiv {P\\in \\mathcal{P}| E_P(f_i)=E_{\\widetilde{P}}(f_i), i=1,2,\\cdots,n }$，当满足熵 $H(P)=-\\displaystyle{\\sum_{x,y}\\widetilde{P}(x)P(y|x)\\log{P(y|x)}}$ 最大时得到的最佳模型。 2.2 最大熵模型学习根据最大熵条件及其约束条件进行约束最优化学习：$$\\begin{align}&amp;\\max_{P \\in \\mathcal{C}} H(P)=-\\displaystyle{\\sum_{x,y} \\widetilde{P}(x) P(y|x) \\log{P(y|x)}} \\Longleftrightarrow \\min_{P \\in \\mathcal{C}} -H(P)=\\displaystyle{\\sum_{x,y} \\widetilde{P}(x) P(y|x) \\log{P(y|x)}}\\&amp; \\text{s.t.}\\hspace{1cm} E_P(f_i)=E_{\\widetilde{P}}(f_i)\\Longleftrightarrow E_P(f_i)-E_{\\widetilde{P}}(f_i)=0,\\hspace{2mm} i=1,2,\\cdots,n \\&amp;\\text{and}\\hspace{1cm}\\sum_{y}P(y|x)=1 \\Longleftrightarrow 1-\\sum_{y}P(y|x)=0\\end{align}$$通过广义拉格朗日函数（Generalized Lagrange Function）^1对约束学习转化对偶问题：$$\\begin{align}\\text{Lag}(P,w)&amp;=-H(P)+w_0\\big(1-\\sum_{y}P(y|x)\\big)+\\sum_{i=1}^n w_1\\big(E_{\\widetilde{P}}(f_i)-E_P(f_i)\\big) \\&amp;=\\sum_{x,y}\\widetilde{P}(x)P(y|x)\\log P(y|x) + w_0\\big(1-\\sum_{y}P(y|x)\\big)+ \\&amp;\\hspace{.7cm}\\sum_{n=1}^n(\\sum_{x,y}\\widetilde{P}(x,y)f_i(x,y)-\\sum_{x,y}\\widetilde{P}(x)P(y|x)f_i(x,y))\\end{align}$$ 其他逻辑回归模型中损失函数推导过程需要将最大似然函数的乘积转换为求和的方式，因此使用对数是一种转换方法:$$\\begin{align}L(W)&amp;=\\log\\displaystyle{\\prod_{i=1}^N}P(\\hat{y}=1|x_i)^{y_i} (1-P(\\hat{y}=1|x_i))^{1-y_i} \\&amp;=\\displaystyle{\\sum_{i=1}^N y_i \\log{P(\\hat{y}=1|x_i)}+(1-y_i)\\log{(1-P(\\hat{y}=1|x_i))}} \\&amp;=\\displaystyle{\\sum_{i=1}^N y_i \\log{P(\\hat{y}=1|x_i)}+\\log{(1-P(\\hat{y}=1|x_i))}-y_i\\log{(1-P(\\hat{y}=1|x_i))}} \\&amp;=\\displaystyle{\\sum_{i=1}^N} y_i \\log{\\frac{P(\\hat{y}=1|x_i)}{1-P(\\hat{y}=1|x_i)}+\\log{(1-P(\\hat{y}=1|x_i))}} \\&amp;=\\displaystyle{\\sum_{i=1}^N y_i W\\cdot x_i+\\log{\\frac{1}{1+\\exp^{W\\cdot x_1}}}} \\\\end{align}$$ 参考 逻辑回归是一个线性分类器，其判断的依据是分类决策边界是否为线性的，而非算法中使用的模型是线性模型还是非线性模型。 Principle of maximum entropy - Wikipedia 概率质量函数，是离散随机变量在特定取值熵的概率，其本身是一个概率值。和概率密度函数差异，一方面后者是针对连续随机变量的定义，另一方面对于任意的值上不能通过概率密度函数估计概率值（没有意义） 备注​ 是原始的约束最优化问题转换为无约束的对偶问题的方法，带约束条件的原始问题：$$\\begin{align}&amp;\\min_{x\\in R^n} \\hspace{0.2cm} f(x) \\&amp;\\text{s.t.} \\hspace{1cm} c_i(x)\\le 0, i=1,2,\\cdots,n \\&amp;\\hspace{2cm} h_j(x)=0, j=1,2,\\cdots, l\\end{align}$$​ 上式中约束函数 $c_i(x)$ 和 $h_i(x)$ 需要在 $R^n$ 是连续可微函数。转换之后得到对偶问题：$$\\begin{align}\\text{Lag}(x, \\alpha, \\beta)=f(x)+\\sum_{i=1}^k \\alpha_i c_i(x)+\\sum_{j=1}^l\\beta_j h_j(x)\\end{align}$$","link":"/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E5%85%AD%E7%AB%A0%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"},{"title":"Git 使用命令笔记","text":"本篇笔记是对 Git 使用过程中，非常有实践操作意义但平时不会用到的一些命令总结，主要包括了以下几个方面的内容： Git 打包，解决对指定 Branch、HEAD 等的文件打包 Clone 库之后多分支切换 远程仓库管理，包括远程仓库 Branch 查看、远程仓库 Branch 删除 Tag 管理，主要包括添加标签和推送标签 单 Commit 的 Merge 操作 大文件无法上传的处理 1. Git 打包Git 打包指用 git 命令进行打包文件方便分发、存档。作为版本管理工具，Git 有多个和版本管理相关的点，例如：主分支(master)、当前分支上的头指针(HEAD) 、标签等内容。因此在进行打包操作的过程中有多种方式可以选择，详细内容可以参考 Git 开发手册[^1]。 1.1 打包 master使用命令: 1git archive --format=zip --output master.zip master 上面的命令在最后指定了使用 master 分支，此外申明了使用 zip 进行存档（⚠️ git 的打包格式主要有 zip 和 tar ，如果没有申明的情况下可以从输出结果中推荐文件类型，不能推断的情况下默认为 tar——官方文档说明是默认输出 tar 文件，但在 git version 2.17.2 (Apple Git-113) 环境下测试命令 git archive --output masr master 是没有保存为 tar 文件，而是一个文本文件） 1.2 打包 HEAD 文件针对头指针打包，不仅仅可以直接进行对头指针的打包而且可以通过分支比较以及头指针比较，打包存在文件差异的内容。 1.2.1 直接打包 HEAD 文件使用命令: 1git archive --format=zip --output head.zip HEAD 基本操作和 master 类似，需要声明使用 HEAD 1.2.2 打包更新的 HEAD 文件使用命令: 1git archive --format=zip -o update.zip HEAD `git diff --name-only HEAD^` 需要在声明 HEAD 之后使用 git diff --name-only HEAD^ 命令，表示是比较 HEAD 的前一个 commit；如果是使用的 git diff --name-only HEAD~3 ，表示的是最后三个版本修改的文件——即使用 git diff 用于比较筛选出更新的文件 1.2.3 打包标签使用命令: 1git archive --format=zip --output filename.zip &lt;tag&gt; 命令使用上和 master 打包方式一样 2. 克隆仓库后分支切换直接使用 git clone 命令克隆仓库，直接显示只有一个 master 分支，但是实际上 origin 分支上保留了其他分支（如果存在多分枝的情况）。因此需要使用命令或者其他工具（Visual Studio Code 提供了选择分支的按钮，可以直接进行选择操作)。 使用命令的操作流程： 显示所有分支信息，这样可以看到其他分支的 1git branch -a 切换到其他分支，需要申明 origin 1git checkout origin/other_branch 其他分支本地化，以实现 origin 上分支作为工作分支——这样才能实现在相应分支上进行本地操作 1git checkout -b other_branch origin/other_branch 3. 标签管理标签管理涉及到的内容还是非常丰富，具体内容可以查看官方文档[^2]。这里只是对标签添加和推送两个管理，进行简要总结。 3.1 添加标签Tag 有两种形式一种是轻量标签和附注标签，轻量标签的话直接使用 git tag &lt;标签标识&gt; 即可；如果是需要更复杂的信息，例如加上用户邮箱等附注信息，需要 gpg 加密（加密过程可以参考 GitHub 指导文档[^3]，也可以参考 gpg加密操作 ）。在加密完成之后，使用如下命令进行添加签名标签： 1git tag -u &lt;keyid&gt; -s &lt;tag_name&gt; 上面的选项 -s 没有指定参数，它的作用是申明制作一个 gpg 签名标签；-v 也没有指定参数，表示的是验证指定的标签；-u 指定了一个 keyid 参数，它是在创建 gpg 密钥的设置 如果是直接使用 git tag -a &lt;tag_name&gt; 是创建一个附注标签，但是是没有进行签名的。 3.2 签名标签验证使用 gpg 签名的标签，可以使用 git tag -v &lt;tag_name&gt; 验证标签 3.3 推送标签标签推送的命令和推送分支是一样的，直接使用命令 git push origin &lt;tag name&gt;。例如下面是一个得到的验证的结果： 4. 远程仓库管理在本地对远程仓库的管理，除了将本地 commit 推送到远程仓库的操作，还会涉及到在本地查看远程仓库分支和删除远程仓库上的分支 4.1 查看远程分支在进行详细管理之前，需要查看已有分支（包括远程分支），可以使用如下命令： 1git branch -a 4.2 删除远程分支 本地分支和远程分支并没有绝对的关系，删除了远程分支之后如果需要使用相应的命令来删除本地分支，反之亦然。 12345# 删除本地分支，如果需要强制删除，可以使用 -D 来替换 -dgit branch -d &lt;branch_name&gt;# 删除远程分支，需要使用 git push 进行操作git push origin --delete &lt;branch_name&gt; 5. 合并这里合并主要是在遇到只需要合并单个 commit 或者单个文件的情况，而非合并分支 5.1 合并单个 commit1git cherry-pick &lt;commit-hash&gt; 切换到相应的分支之后，用上面的命令进行合并对应的 commit 5.2 合并指定文件该场景主要是解决需要将某个分支上的 HEAD 文件覆盖另一个分支上对应的文件，而不需要完全修改所有的文件。该方式的好处的可以方便测试单个文件调整之后，是否可以良好运行；另一个好处就是可以方便回滚： 12345# 使用下面的命令可以将其他 branch 的 filename 直接覆盖到当前 branch 的 filegit checkout &lt;other_branch&gt; &lt;filename&gt;# 如果测试失败之后，可以直接回滚，直接使用如下命令git checkout HEAD &lt;fielname&gt; 参考[^1]: git archive (Administration) - Git 中文开发手册[^2]: Git - 打标签[^3]: Managing commit signature verification - GitHub Docs [^4]: Git - 签署工作","link":"/%5Bobject%20Object%5D/Git%E4%B8%8D%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{"title":"ETL 和 ELT 差异","text":"ETL 和 ELT 都是数据处理的模式，但是两者在逻辑和应用场景下是具有较大差异的。两者的英文单字都是相同的意思： Extract，数据抽取，从数据源中提取数据 Ttransform，数据转换，包括了数据清理以及规范化处理，以及其他相关的操作 Load，数据加载，可以看作是将当前数据结果交付，应用于下游任务 ETL从数据源中提取数据、转换操作之后，将数据加载到目标数据库或数据仓库的过程。也就是说最终写入数据仓库或者数据库中的数据，是相对来说“清洁的数据”。处理过程可以迭代的数据转换过程得到相应的数据结果——这也容易导致需要投入相应的资源进行维护转换。应用中下游的任务会依赖于 ETL 过程，所以它也不适合应用于长期任务以及大量数据的场景 ELT将转换操作放到各个数据需要应用的引擎中，因此应用的场景更偏向于大数据，可以支持 DataLake，而且可以应用于非结构化数据中。 参考 ETL vs ELT: Must Know Differences","link":"/%5Bobject%20Object%5D/ETL%E5%92%8CELT%E5%B7%AE%E5%88%AB/"},{"title":"Hexo 与 GitHub 搭建静态博客笔记","text":"整理了一下 Hexo 搭建过程中的流程： 环境搭建 Hexo 相关服务使用 定制化主题 1. 环境搭建 在使用 Hexo 和 GitHub 搭建静态博客的方案中，初步阶段需要使用到博客框架 Hexo 、代码托管服务平台 GitHub和分布式版本控制软件 git。环境搭建过程: 安装 Hexo 框架 因为该框架是依赖于 Node.js ，因此在先需要安装 Node 之后，通过 npm 包管理命令进行安装 Hexo（需要注意除了通过 CMD 方式安装 Node 外，可以在 Node.js 链接中选择长期稳定版本进行导航安装）： 12345678910111213141516# mac 通过 homebrew 进行安装$ brew install node# 检查是否安装正确，分别显示 node 和 npm 的版本$ node -vv12.18.2$ npm -v6.14.5# 安装 hexo 框架$ npm install -g hexo-cli# 检查 hexo 安装是否正确$ hexo -v# 为了部署的时候使用 git ，可以在这里安装上 hexo-deployer-git$ npm install --save hexo-deployer-git hexo -v 命令会显示 hexo 版本信息、系统信息以及其他依赖的版本信息 安装 git 软件 安装方式同样可以通过 CMD 和安装程序导航两种方式： 12# homebrew 安装$ brew install git 2. Hexo 搭建博客Hexo 搭建博客，包括的过程： 初始化一个博客目录，该目录最后会生成博客框架需要的基础内容 12345# 创建并进入一个文件目录$ mkdir Blog &amp;&amp; cd Blog# 初始化服务$ hexo init 启动服务 主要是用于预览博客效果，使用的命令为 hexo s 或者 hexo server。其他选项参数使用说明： 12-p &lt;port_num&gt; # 设置端口-i &lt;ip_address&gt; # 设置默认的 IP 地址，不重写的情况下，默认是 0.0.0.0 创建 post 生成一个 &lt;post_name&gt; 的 post 文件，可以通过命令 hexo new “&lt;post_name&gt;” 生成一个 markdown 文件；也可以通过直接在通过一般管理文件的方式，可以直接在 source 目录下的 _posts 目录中管理——即直接在 source/_posts 下创建、编辑以及删除等操作 post 文件 生成静态文件 要以前端的方式展示出博客，可以生成静态文件这样可以部署到相关服务器上，以方便网页访问。生成静态文件命令是 hexo generate ，或者使用 hexo g。推荐在生成之前进行清除数据（设置的 public_dir 目录）和数据库，需要使用命令 hexo clean。默认生成的内容是在一个 public 目录中，如果需要调整到其他目录需要在 _config.yml 文件中 Directory 部分中修改 public_dir 对应的值即可调整 部署 需要注意部署到 GitHub 之前需要设置好仓库，之后才能部署 创建 GitHub 仓库时，其名称一个可以用于访问的地址，例如：&lt;name&gt;.github.io 的仓库是可以直接访问的，其中 &lt;name&gt; 是可以任意取（但是建议是直接使用自己的 GitHub 的用户名称^1），但 github.io 必须加上 修改配置信息 需要在 _config.yml 中修改 deploy 部分信息，更多信息可以参考 官方文档部署说明 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: &lt;远程仓库链接&gt; branch: &lt;git 管理的分支&gt; 使用 hexo d 或者 hexo deployment 进行部署。此外可以直接使用一条命令完成生成静态文件和部署，即 hexo generate --deploy 或者 hexo deploy --generate 3. 定制化主题通常情况下都是依赖于其他相对应的主题，在进行优化调整。这里主要说明怎么样使用主题，以及相关配置修改。一般在页面 Themes | Hexo 中筛选使用的主题之后： 在 Blog 文件夹中使用 git 进行 clone repo： git clone &lt;git_url&gt; themes/&lt;theme_name&gt;，通过该命令对应的主题 repo 会在 themes 目录下创建一个对应的主题[^2] 修改主题配置，在 _config.yml 文件中修改 theme 对应的属性值，例如 theme: icarus 使用的就是 icarus 主题 后续步骤完成生成静态文件和部署过程即可 参考主要是因为后面文件引用会存在某些问题，用户名是被当作根目录来使用，因此在使用内部引用 CSS 和 JS 文件时可以方便使用&lt;name&gt;.github.io/&lt;other_info&gt;。如果需要解决掉因为链接引用是否使用相对链接，可以调整 _config.yml 中 Writing 部分中的 relative_link 设置为 true [^2]: 主题 repo 作为独立的 submodule 管理 如果主题是直接克隆 repo 到本地的情况下，那么需要添加 submodule 的方式让第三方插件作为 submodule 和个人文件保持独立： git submodule add &lt;github_link&gt; themes/&lt;theme_name&gt; [^3]: 部署过程提示需要配置 git 全局用户名 这个提示信息是因为使用的 hexo 部署工具没有识别到用户名和邮箱，只需要修改 .deploy_git/.git/config 文件信息即可，并不需要真正的部署全局信息。将以下信息添加到 config 中即可： 1234567891011[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true# 下面是需要添加上的信息[user] name = &lt;user_name&gt; email = &lt;user_email&gt;","link":"/%5Bobject%20Object%5D/Hexo%E4%B8%8EGitHub%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/"},{"title":"Hexo 博客编写技巧","text":"Hexo 的博客编写是使用 markdown 来实现，但在使用上针对页面优化上可以使用一些技巧。本次整理的内容包括两个方面： 文件配置文件的调整 markdown 编写过程中的技巧 1. 配置文件调整 1.1 代码显示配置调整的配置文件是主题文件夹下的配置文件，例如使用 icarus 主题时，修改的配置文件是 themes/icarus/_config.yml，这里主要针对 markdown 显示的优化。 在配置文件中 article 的小节中可以对 highlight 进行调整，包括从显示的颜色主题，是否显示复制按钮以及是否可以折叠等角度设置: 123456789article: highlight: # 代码高亮主题，颜色高亮主题的名称可以在下面的链接中找 css 文件名 # https://github.com/highlightjs/highlight.js/tree/master/src/styles theme: atom-one-light # 显示复制代码按钮 clipboard: true # 代码块的默认折叠状态。可以是&quot;&quot;, &quot;folded&quot;, &quot;unfolded&quot; fold: unfolded 以上方式是全局的方式，定义代码显示的模式。 1.2 文章目录显示若需要在页面中展示文章目录，一方面需要在 _config.yml 中 widgets 中添加 toc ： “_config.yml\" >unfolded123456widgets: - type: toc position: left 另一方面需要在 markdown 中设置 front-matter “_config.yml\" >unfolded123456---title: 一篇有目录的文章toc: true--- 2. markdown 编写技巧整体上来说，直接在 markdown 中调整显示属性的优先级要高于 page 配置选项（可以在主题下添加一个 _config.page.yml 进行设置），且高于默认配置选择（即 _config.yml 中的配置，前面的 _config.page.yml 配置又高于 _config.yml）。[^1] 2.1 代码高亮对于高亮配置可以在 front-matter 中设置: 12345678---title: 文章date: '2015-01-01 00:00:01'article: highlight: theme: ocean---# title 2.2 代码块显示定制化针对某些特定场景下需要对某些代码块进行定制化，例如设置一个文件名，折叠状态时，可以通过使用 codeblock 的方式完成。例如需要设置代码块的来自文件 test.sh，使用的语言是 bash： 可以使用模版模式定制化显示代码块——显示文件名，语言类型，直接显示是否折叠。下面的例子表示使用的是 bash ，使用文件名为 test.sh，初始状态为 folded（可以使用空字符串或者 unfolded） 123456789101112131415161718{% codeblock &quot;test.sh&quot; lang:bash &gt;folded %} # mac 通过 homebrew 进行安装 $ brew install node # 检查是否安装正确，分别显示 node 和 npm 的版本 $ node -v v12.18.2 $ npm -v 6.14.5 # 安装 hexo 框架 $ npm install -g hexo-cli # 检查 hexo 安装是否正确 $ hexo -v # 为了部署的时候使用 git ，可以在这里安装上 hexo-deployer-git $ npm install --save hexo-deployer-git {% endcodeblock %} 通过以上方式得到的现实结果如下： test.sh >folded1234567891011121314151617# mac 通过 homebrew 进行安装$ brew install node# 检查是否安装正确，分别显示 node 和 npm 的版本$ node -vv12.18.2$ npm -v6.14.5# 安装 hexo 框架$ npm install -g hexo-cli# 检查 hexo 安装是否正确$ hexo -v# 为了部署的时候使用 git ，可以在这里安装上 hexo-deployer-git$ npm install --save hexo-deployer-git 2.2 封面和缩略图使用在某些场景下需要使用添加文章封面或者文章缩略图，可以在 front-matter 中设置 cover 和 thumbnail 值： 12345---title: 封面图和缩略图使用cover: /gallery/covers/cover.jpg # cover 设置封面图thumbnail: /gallery/thumbnails/thumbnail.jpg # thumbnail 设置缩略图--- 参考[^1]: Icarus用户指南 - 主题配置","link":"/%5Bobject%20Object%5D/Hexo%E5%8D%9A%E5%AE%A2%E7%BC%96%E5%86%99%E6%8A%80%E5%B7%A7/"},{"title":"CentOS 安装 NodeJS","text":"NodeJS 目前支持的长期稳定版本已更新到 12.19.0LTS，但考虑到目前大部分开发环境需求都是在 8.X 版本下，因此采用安装 NodeJS 8.X 版本。根据参考文档[^1]。 环境依赖： CentOS 7 gcc / g++ 4.9.4 以后版本，或者使用 clang /clang++ 3.4.2 以后版本 Python 2.6 或者 2.7 GNU make 3.8.1 之后版本 1. 安装依赖在 CentOS 上使用 yum 进行安装相关依赖，yum install gcc-c++ make 2. 下载二进制文件以及配置分别的步骤： 下载二进制文件 wget https://nodejs.org/dist/v8.10.0/node-v8.10.0-linux-x64.tar.xz 解压 tar -xf node-v8.10.0-linux-x64.tar.xz 配置，需要将文件移动到 /usr/local/ 目录中，可以将文件目录名称改为 node： mv node-v8.10.0-linux-x64 /usr/local/node 此外需要将配置文件修改，可以更改 /etc/profile 或者 ~/.bash_profile，添加环境路径并且激活: 12export NODE_HOME=/usr/local/nodeexport PATH=$PATH:$NODE_HOME/bin 验证安装是否正确，使用 node -v 后者 npm -v 验证是否安装好，前者是 node 命令后者是包管理工具 参考[^1]: NodeJS 安装 V8","link":"/%5Bobject%20Object%5D/CentOS%E5%AE%89%E8%A3%85NodeJS/"},{"title":"Python 调用 StanfordNLP 服务","text":"Stanford NLP Group 提供了自然语言软件处理工具，目前（2021年）能够非常非常方便快捷的调用 POS、NER 等功能。本例是以 Server 方式提供服务，让 Python 能够调用相关服务。相关的步骤如下： 在 Download-CoreNLP 页面中下载 CoreNLP 以及其他需要的语言模型依赖包 解压缩 CoreNLP 文件，同时语言模型依赖包放在其中 使用 Java 启动 Server 的两种情况，使用默认的英语模型处理英文数据: java -mx4g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000；第二种情况是需要添加非英语语言模型，处理非英文数据。例如处理中文语言: java -Xmx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port 9000 -timeout 15000。两种模式都申明了最大的内存、端口以及超时时间。 调用服务的方式是网络 API 的方式，因此可以使用 Python 中 requests 进行网络请求方式调用相关服务外，还可以使用其他网络请求的方式获取服务（例如 curl） 12import requestsprint(requests.post('http://[::]:9000/?properties={&quot;annotators&quot;:&quot;tokenize,ssplit,pos&quot;,&quot;outputFormat&quot;:&quot;json&quot;}', data = {'data':'整个世界垮掉，这不仅仅是一个测试，而且还是一个测试'}).text) 参考 Usage - CoreNLP","link":"/%5Bobject%20Object%5D/NLP-StanfordNLP%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/"},{"title":"gpg 加密操作流程","text":"GPG(GNU Privacy Guard) 是一个加密、签名通信内容以及解密的内容的软件，采用的加密方式是非对称加密(即加密和解密使用的密钥不相同，例如使用公钥和私钥)[^1]。它的诞生是因为作为商业软件的 PGP ，不能自由使用，因此 FSF 开发的 GNU 版本。本篇笔记是为了阐释 git 在使用签署过程中，演示加密操作。 1. 安装在 Mac 下安装 GnuPG 可以直接使用 homebrew 工具进行安装，直接使用命令: 1$ brew install gpg 2. 密钥生成gpg 采用非对称加密，因此在进行加密和解密操作之前需要有相应的私钥和公钥，而生成相应的密钥的命令: 123456789101112131415# 因 gpg 版本不同，生成密钥过程的命令存在一些差异&lt;&lt; comment gpg 版本低于 2.1.17 使用如下命令: --default-new-key-algo 选项用于选择使用的加密算法和最小位数， rsa4096 表示 rsa 算法加密至少 4096 bitscomment$ gpg --default-new-key-algo rsa4096 --gen-key&lt;&lt; comment gpg 版本高于 2.1.17 使用如下命令，以交互形式生成密钥comment$ gpg --full-generate-key 以上是 GitHub 文档建议的密钥生成方式，但是可以直接使用 gpg --gen-key 来生成（但生成密钥的流程操作顺序可能不一致，但完整的流程过程是一致的）。输入 gpg --full-generate-key 之后会进行如下流程 选择加密算法选项： gpg (GnuPG) 2.2.22; Copyright (C) 2020 Free Software Foundation, Inc.This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only) (14) Existing key from cardYour selection? 推荐使用 RSA 算法，即使用默认选项或者第四个 RSA 签名选项 设置密钥长度: RSA keys may be between 1024 and 4096 bits long.What keysize do you want? (3072) 一般情况下是长度越高，密码越安全 设置有效期 Please specify how long the key should be valid. 0 = key does not expire &lt;n&gt; = key expires in n days &lt;n&gt;w = key expires in n weeks &lt;n&gt;m = key expires in n months &lt;n&gt;y = key expires in n years Key is valid for? (0) 设置密钥有效期，默认选项为 0 表示永久有效，后面非别表示对应的多少天、周、月以及年的有效期。 在完成确认之后需要补充个人信息，包括姓名、邮箱地址以及备注信息。填充信息之后，会再次要求输入 passphrase 以保护私钥。 3. 密钥管理3.1 密钥导出生成的密钥是一个二进制文件，需要使用公钥和私钥对应的 ASCII 数值需要进行转换，之后才能上传到相应的服务器上（例如 GitHub 上）。 3.1.1 公钥导出使用命令 gpg --armor --output &lt;filename&gt; --export &lt;uid&gt; 可以将对应 uid 的私钥导出到指定的 filename 文件中 对于导出的公钥可以直接复制应用于 GitHub 中，和 SSH 生成的公钥应用方法类似。 3.1.2 私钥导出使用命令 gpg --armor --output &lt;filename&gt; --export-secret-keys &lt;uid&gt; 可以将将私钥导出，私钥导出需要密码 问题1. gpg 签名失败在使用 git tag -s &lt;tag_name&gt; 报错: error: gpg failed to sign the dataerror: unable to sign the tagThe tag message has been left in .git/TAG_EDITMSG 因为环境变量中没有设置 GPG_TTY，在 bash_profile 中添加上 export GPG_TTY=$(tty) 即可 参考[^1]: GnuPG - 维基百科，自由的百科全书[^2]: GPG入门教程 - 阮一峰的网络日志","link":"/%5Bobject%20Object%5D/gpg%E5%8A%A0%E5%AF%86%E6%93%8D%E4%BD%9C/"},{"title":"使用 PySpark 处理数据笔记","text":"调用 Spark 能力处理数据 调用 Master 几种模式： local local[K] local[*] spark://host:port yarn sparkSession >unfolded1234567# 调用 Standalone 模式下的集群能力，同时设置了 worker 的内存使用配置spark = SparkSession \\ .builder \\ .appName(&quot;C3Analysis&quot;) \\ .master(&quot;spark://hn-manager:7077&quot;) \\ .config(&quot;spark.executor.memory&quot;, &quot;13G&quot;) \\ .getOrCreate() 在读取数据时，分为本地数据读取和 HDFS 数据读取。需要本地读取数据那么需要通过 file://file_path 的方式，可以直接添加目录，读取目录中的数据 hdfs 管理，使用 HDFS 管理文件目录时，需要注意使用相关的用户权限 环境变量添加与修改 在使用 PySpark 功能时，存在需要修改环境变量 PPYSPARK_PYTHON 和 PYSPARK_DRIVER_PYTHON 为临时环境值。该问题的解决方案是修改 os package 中的 environ 实例： 123import osos.environ[&quot;PYSPARK_PYTHON&quot;]=&quot;/data/Anaconda3/bin/python&quot;os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;]=&quot;/data/Anaconda3/bin/python&quot; PySpark 退出服务 在运行完 PySpark 程序之后，需要关闭 Spark 的话需要使用 SparkContext 的 stop() 方法关闭或者使用 System.exit(0) 命令或者 sys.exit() 方法退出应用 SparkDataFrame 增加行索引 使用 F.monotonically_increasing_id() 方法可以增加行索引，但是得到的索引值并不是显性排序的——得到的结果的索引和是否为单机、数据是否分为了多个 parlition 等原因相关。命令方式如下: data = data.select(F.monotonically_increasing_id().alias(&quot;index&quot;), &quot;*&quot;) 使用定制化的 Java 方法 思路上使用 Py4J 实现 Python 中数据和 JVM 通信的方式，而在 PySpark 已经支持了 JVM。因此重要的节点上实现 Java 中的类被加载在 PySpark shell中，实现的方法是: 在启动 PySpark 阶段需要添加上包含了类的 JAR 包，eg: pyspark --master local --driver-class-path &lt;jar 包路径&gt; 命令可以在单机模式下将添加 jar 包（在 Spark 2.4.0 版本中使用 --jars 参数方式替换 --driver-class-path 也是可行的） 以 SparkContext 对象的 jvm 调用相关 Java 类方法，例如命令 sc._jvm.com.python.test.TestDem0001.concatStatic('1', 'a', 'b') 是表示调用 com.python.test 的 package 下 TestDem0001 类的 concatStatic 方法 PySpark 中的 SparkDataFrame 字段名称是没有区分大小写的，例如 df.select(&quot;ITV_accounT&quot;).head(3) 和 df.select(&quot;itv_account&quot;).head(3) 都可以得到结果","link":"/%5Bobject%20Object%5D/PySpark-%E4%BD%BF%E7%94%A8PySpark%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/"},{"title":"HBase 以及 Python 使用 HBase","text":"HBase 的支持的文件系统是 HDFS，但其不仅仅支持 HDFS。由于其使用的文件系统是可插拔的架构，只需要提供可以被 Hadoop 接口支持的文件系统那么就可以替换 HBase 底层文件系统。 ZooKeeper 是一个可靠的高可用的、持久化的分布式的协调系统，它在 HBase 的架构中是监控服务器可用性、跟踪机器故障和网络分区。每台 Region 服务器在 ZooKeeper 服务器中会有独立的会话，Region 作为客户端会向 ZooKeeper 服务器定时发送”心跳“，以判断服务器是否故障。 1. HBase 基础1.1 HBase 数据访问HBase 基础的数据访问，通过 get() 和 scan() 方法可以实现指定列族、列、时间戳以及版本号的筛选。这种方式的筛选访问得到的数据是粗粒度数据，不是细粒度方式的筛选，eg: 正则表达式的筛选。在客户端 HBase 的可以通过支持的 Filter 进行行键、列名或者列值过滤，在服务器收到筛选需求需要通过谓词下推的方式实施筛选。虽然在客户端实施筛选，但会增加数据传输压力影响性能。 1.2 版本管理和数据删除在同一个单元格多次写入数据时，HBase 的数据会有不同的版本。在进行数据操作时，可以是隐式的进行版本管理，也可以进行显性的版本管理。在进行删除操作时，是对特定的时间戳添加一个一个墓碑标记，它会屏蔽指定的对应版本的数据（隐式处理的情况下是直接给最大时间戳的版本数据），在刷写缓存或 major 合并动作之前老版本的数据就会出现（刷写或者合并之后，数据会真实删除） 1.3 自定义版本控制自定义的版本控制，可以覆盖服务器时间戳为基础的版本控制。例如由 ZooKeeper 提供的全局数字生成器可以在调用 put 操作时提供以 1 开始增加的顺序数字。使用自定义版本控制，可以通过为每个 put 使用数字生成器，或者在表、列的描述标志中表明使用自定义的时间戳。需要这样如果没有设置生成器值，那么将在服务器被时间戳替换掉，此外某些自定义时间戳的方式需要验证是否可用（例如负值时间戳是否可用，最好验证一次）。 自定义版本控制设置模式： 记录 ID，以时间戳作为存储消息 ID，因为 HBase 的版本隐式排序是降序排序方式，因此可以通过这种 ID 排序方式筛选得到最后 N 个版本的数据 数字生成器 它和时间戳的功能意义上差异不大，都是提供能够区别版本的依据。但是在应用层面上存在较大差异，前者能够提供完全相同时间戳的版本数据，而后者因为 Java 计时器使用的精度是毫秒，难以实现筛选完全相同时间戳版本数据。 HBase 优化HBase 的数据分片以及在 HFile 数据拆分时，都是通过行来进行拆分，因此在表创建时高表方案会更有优势。而在查询方面也会有相应的优化。 数据读写热点优化在某些场景下，例如流式数据处理过程时，HBase 的数据组织方式会让有序的数据会被过份集中到“某个” region 服务器下。这样的存储会导致读写热点，同时因为这种数据过份集中会是系统性能下降。要解决该问题可以通过将数据分散到所有的服务器中，其中一种方法就是采用 Salting 的方法：方法是在键的前端会从一定序列中的某个元素添加到键前端，这样打乱有序 key 的方式可以优化该问题。 [merge_scan_with_prefix.py] >fold12345# 该代码思路是 Mozilla Socrro 思路，在键上从 16 个字符中分别添加到键中去组合成新键[...]for salt in &quot;0123456789abcdef&quot;: salted_prefix = &quot;%s%s&quot; % (salt, prefix)[...] 通过字段交换或者权重提升的方法也是一种优化行键的方案，思路是将时间戳字段一开户添加其他字段作为前缀产生组合行键的思路：对连续递增的时间戳从行键的第一位置变为第二位置来处理。 第三种方案是通过随机化的方式，即利用如 MD5 的随机化函数将行键进行散列。缺点是不能再通过时间范围扫描数据。 2. Python 使用 HBaseHBase 客户端包括 Thrift、REST 等类型，Python 访问需要通过 Thrift 客户端形式访问（Java 可以直接配置 zookeeper 信息，以 zookeeper 连接访问）。 2.1 用 Thrift 访问 HBasehappybase 提供了封装 Thrift 访问的方式，直接以 connection = happybase.Connetion(host='host', port=9090) 形式访问可能会出现异常： TTransportException: TTransportException 和 TTransportException: TSocket read 0 bytes。这个问题需要检查一下 HBase 和 Thrift 的配置信息： hbase-site.xml >unfolded123456&lt;property&gt; &lt;name&gt;hbase.regionserver.thrift.framed&lt;/name&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.thrift.compact&lt;/name&gt;&lt;/property&gt; 因为上面 hbase.regionserver.thrift.framed 和 hbase.regionserver.thrift.compact 配置信息和 happybase 连接中默认配置信息不一致，需要进行调整相关的参数值: hbase_connection.py >unfolded1234567891011import happybaseconnection = happybase.Connection( host='host', port=9090, timeout=None, autoconnect=True, transport='framed', # Default: 'buffered' protocol='compact' # Default: 'binary' )print('tables:', connection.tables()) 3. 其他概念3.1 HBase 解编码器HBase 提供的不同压缩算法对压缩比、CPU 消耗以及解压效率等方面表现不同，因此选择不同的压缩方案是由重要的实践意义： Snappy Google 发布的 BSD 协议压缩压缩算法，在压缩率和解压效率都较高 LZO Lempel-Ziv-Oberhumer ，一种无损压缩算法，在解压效率较高 GZIP 压缩效率较高，能够减少存储空间开销，但是速度偏慢，通过本地操作系统的 GZIP 库可以缓解性能问题——如果本地库不存在，那么会收到 “Got brand-new compresser” 日志信息。但是它是 CPU 密集型算法性能会受到严重影响，因此服务器的负载会加大 启用压缩 >unfolded1234// 表创建时申明启用压缩&gt; create 'table', {NAME =&gt; 'colfam1', COMPRESSION =&gt; 'GZ'}// 使用中更改压缩&gt; &gt; create 'table', {NAME =&gt; 'colfam1', COMPRESSION =&gt; 'GZ'}","link":"/%5Bobject%20Object%5D/HBase-HBase%E4%BB%A5%E5%8F%8APython%E4%BD%BF%E7%94%A8Hbase/"},{"title":"使用Spark进行机器学习","text":"Spark 提供的统一分析引擎中包括了数据整合分析、特征、模型训练和部署等一套完整的生态系统，因此使用 Spark 能够进行实现机器学习的任务。常规的分类和回归问题，都可以通过 spark.mlib 或者 spark.ml 两个包来完成。但是两者在在处理数据类型上存在一些差异，spark.mlib(自 Spark 2.0 版本进入维护模式) 提供的是基于 RDD API 的原生机器学习 API；而 spark.ml 是一个相对较新的包，它是基于 DataFrame API 的机器学习 API","link":"/%5Bobject%20Object%5D/PySpark-%E4%BD%BF%E7%94%A8Spark%E8%BF%9B%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"title":"RFM 模型与使用 RFE 模型分析用户价值","text":"1. RFM 背景RFM 是分析用户价值的一种营销分析方法，其目的是为了找到最佳用户的方式，利用帕累托效应衡量当前用户价值和和客户潜在价值。分析的方法正如其名： Recency 最近消费时间间隔，指距离最近一次消费的时间长度，在评分上可以采取 rank 为 10 的方式，其中 1 为最低[^1] Frequency 指定时间段内的消费次数，针对的是在给定是时间跨度范围内（例如 12 个月），用户消费的消费次数。在分析中同样可以采取 rank 为 10 的方式评分[^1] Monetary 消费总额或者平均消费额 2. RFM 变体用户的第三个 M 属性是非常重要的价值属性，但在应用场景下或者其他特殊的目的下，需要扩展用户其他属性信息用于评估用户价值或者使用其他信息用于解决特殊问题。因此 RFM 产生了其他的不同变体模型： RFD – Recency, Frequency, Duration RFM分析的修改版本，可用于分析以观众/读者/冲浪为导向的商业产品的消费者行为 RFE – Recency, Frequency, Engagement RFD分析的广义版本，其中可以将参与度定义为包括访问持续时间，每次访问的页面数或其他此类指标 RFM-I – Recency, Frequency, Monetary Value – Interactions RFM框架的一个修改版本，以说明与客户的营销互动的新近度和频率（例如，广告参与度影响因素控制） RFMTC – Recency, Frequency, Monetary Value, Time, Churn rate 增强RFM模型，该模型利用概率论中的伯努利序列，并创建公式来计算客户在下一次促销或营销活动中购买的概率。该模型已由Alexandros Ioannidis实施，用于诸如输血和CDNOW数据集的数据集。 3. RFE 模型应用本次分析以用户参与度的角度进行分析，因为数据源包括的是用户播放记录信息——用户的直播播放记录和用户点播播放记录。分析的目的是了解用户付费转化的可能性，而付费的关注依据是用户点播，用户的活跃指标是包括了直播和点播信息。 3.1 R 属性Recency 属性计算，可以通过计算用户最近消费日期到当前日期的间隔天数计算： RFM模型应用.ipynb >folded2021222324252627282930313233343536373839404142import datetimefrom pyspark.sql import SparkSessionfrom pyspark.sql import typesfrom pyspark.sql import functions as Ffrom pyspark.sql import Window# Pyspark 计算通过 UDF 计算@F.udf(returnType=types.IntegerType())def recency(date): &quot;&quot;&quot;计算 RFM 中 R 属性 计算 RFM 中 R 属性，返回的是距离当前日期的时间上的天数 &quot;&quot;&quot; if isinstance(date, datetime.date): return (datetime.date.today() - date).days elif isinstance(date, datetime.datetime): return (datetime.datetime.now() - date).days else: return None# 计算用户 recencydata.groupby(&quot;itv_account&quot;).agg(F.max(F.col(&quot;adjust_start_date&quot;)).alias(&quot;date&quot;)) \\ .withColumn(&quot;recency&quot;, recency(&quot;date&quot;)).head(3) 3.2 F 属性计算Frequency 计算的是在一定周期内活跃度频率，只需要对一定周期内特定类型的数据。因此计算过程针对选定的类型，统计数量即可统计出结果 RFM模型应用.ipynb808182from pyspark.sql import functions as Fdata.groupby(&quot;itv_account&quot;) \\ .agg(F.count(F.col(&quot;adjust_start_date&quot;)).alias(&quot;frequency&quot;)).head(3) 3.3 E 属性计算Engagement 计算用于参与度，本次的分析直接使用使用点播播放时间占总播放时间的比率。处理数据的思路: 通过窗口函数提取出总播放时间和点播播放时间之后，再进行比例求解。 RFM模型应用.ipynb >folded96979899100101102103104105106107108109110111112113114115116from pyspark.sql import SparkSessionfrom pyspark.sql import typesfrom pyspark.sql import functions as Ffrom pyspark.sql import Window# 方法一：先通过窗口函数计算出到组内每行为止的播放时长以及分别通过播放类型判断是否需要将数据转换为 0 值以计算点播播放时长window = Window.partitionBy(&quot;itv_account&quot;) \\ .orderBy(F.col(&quot;watched_type&quot;).desc()) \\ .rowsBetween(Window.unboundedPreceding, Window.currentRow)data.select(&quot;itv_account&quot;, &quot;watched_type&quot;, &quot;total&quot;, F.sum(&quot;total&quot;).over(window).alias(&quot;allEngage&quot;), F.when(F.col(&quot;watched_type&quot;) != &quot;点播&quot;, 0).otherwise(F.col(&quot;total&quot;)).alias(&quot;engage&quot;)) \\ .groupby(&quot;itv_account&quot;) \\ .agg((F.sum(&quot;engage&quot;) / F.max(&quot;allEngage&quot;)).alias(&quot;ratio&quot;)).head(3)# 方法二: 先转换需要的数据，再通过 groupby 聚合计算data.select(&quot;itv_account&quot;, &quot;total&quot;, F.when(F.col(&quot;watched_type&quot;) != &quot;点播&quot;, 0).otherwise(F.col(&quot;total&quot;)).alias(&quot;vod&quot;)) \\ .groupby(&quot;itv_account&quot;) \\ .agg((F.sum(&quot;vod&quot;)/F.sum(&quot;total&quot;)).alias(&quot;ratio&quot;)).head(3) 3.4 模型应用不论是 RFM 还是其变体在用户价值的应用上，都是需要将是根据相应的属性得分组合进行判别用户类型，并且将相应的类型应用到特定的场景和目的下。一种是将每个属性拆分为三级之后，三个属性可以得到 27 种类型的组合，以不同的组合得到的用户类型；另外是将三种属性进行权重调整（权重方案可能根据具体的目的进行调整），筛选出特定的用户[^2]。 参考\u0012 [^1]: RFM (market research)Wiki 中提到的评分方法是使用 Rank 等级为 10，个人建议还是使用奇数 Rank——有助于进行拆分[^2]: 用户画像无头绪？手把手教你RFM模型","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-RFM%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E7%94%A8%E6%88%B7%E4%BB%B7%E5%80%BC/"},{"title":"Mock 技巧","text":"Mock 的方法是解决单元测试等简单测试方法时过于隔离，非真是真实场景。在复杂系统下需要接入外部系统信息进行测试，例如接受外部数据、完整流程的系统测试，但是在开发阶段并接入外部依赖存在较大的复杂性以及增加了开发阶段测试的难度。因此通过 “模拟”的方式来解决以上的问题，在 Python 的 3.x 版本之后提供了 mock 的基础模块以解决 Python 开发过程中的上述问题。 1. Mock 对象Mock 对象常用的方式包括了：1）补丁方法，2）对象方法或者属性调用。Mock 常用对象包括了 Mock 和 MagicMock（此外还有其他 Mock 对象），通常情况下是可以互换使用的[^2]，但后者有更丰富的属性方法而被广泛使用。初始化对象时常用的参数包括： name: 用于申明对象名称，是一个可选项 spec: 用于申明一个对象，可以用于限制可用的对象或者方法。在没有指定该参数时， Mock 对象调用属性或者方法可以直接创建；反之，如果指定没有在 spec 中的会出现属性错误 return_value: 表示调用相关的方法时，返回的值 side_effect: 可以接收可调用对象、可迭代对象、异常以及根据情况的行为变化 1.1 Mock 对象创建及其基本应用创建基本的 Mock 对象: >folded123456789101112131415# load packagefrom unittest import mock# 创建没有 spec 的对象m = mock.Mock(name=&quot;withoutspec&quot;) # 该对象 repr 示例&lt;Mock name='withoutspec' id='139745029751952'&gt;# 调用对象任意属性或者方法，如果不存在时会直接创建相关属性和方法m.attr # 该方式会直接返回 &lt;Mock name='withoutspec.attr' id='139745029763472'&gt; 并且添加 attr 属性m.te() # 该方式会直接返回 &lt;Mock name='withoutspec.te()' id='139745029777488'&gt; 并且添加 te 方法# 修改属性或者方法，直接使用赋值方式m.attr = 12 # 该方式不会返回任何结果，但是属性值会被调整m.te = lambda x: x + 12 # 修改 te 的方法，可以通过 m.te(x=23) 的方式调用 在 Mock 对象的属性，可以调整为方法即需要通过 m.attr() 的方式调用。该方式是直接在属性上配置 return_value 的属性值即可: >folded12345m.attr1.return_value = 12# 要返回 attr1 的值，需要通过方法的方式调用m.attr1() # 返回值是 12m.attr1 # 会返回该属性的信息，&lt;Mock name='withoutspec.att1' id='139745733653712'&gt; 除了配置一个 return_value 的方式创建一个方法，可以通过 side_effect 属性进行配置。side_effect 的可配置功能更丰富，例如需要配置一个可迭代对象 m.attr2.side_effect = range(12)，即可以创建一个 iterator 对象。如果是需要直接 mock 一个函数，可以以 t = mock.Mock(return_value=12) 创建一个 t() 的函数。 >folded12345678910111213141516# 配置 iteratorm.attr2.side_effect = range(12) # 可以被 next() 函数调用# 配置 exceptionm.attr3.side_effect = ValueError(&quot;Wrong Value&quot;) # 以 m.attr3() 调用时会出现异常# 配置对象class obj: def __init__(self): self.attr1 = 12 self.attr2 = None def method(self, arg): passm.attr4.side_effect = obj 使用 side_effect 方式只是相当于将类赋值给类 attr4，但要创建对象还是需要初始化赋值一次。这里存在另一种应用条件，即直接将类指定给 Mock 对象，保留相应对象的属性和方法——可以在初始化时通过 spec 或者 spec_set 参数配置。 >folded123456789101112131415class obj: attr1 = None attr2 = None def __init__(self): self.attr3 = 12 self.attr4 = None def method(self, arg): pass # 等效于 m = mock.Mock(spec_set=obj)，这两种方式下都表示 m 可以调整 attr1, attr2 属性m = mock.Mock(spec=obj)# 等效于 m = mock.Mock(spec_set=obj())，这两种方式下可以调整 attr1, attr2, attr3, attr4m = mock.Mock(spec_set=obj()) 需要注意如果添加非对象的属性时，会提示属性错误。另外如果 spec 和 spec_set 参数，得到的数据值是一个具体的数据，那么该 Mock 对象会将 __class__ 属性值修改为该对象的类。 >folded12345m = mock.Mock()# 等效于 m = mock.Mock(spec={})m.__class__ = dictisinstance(m, dict) # 结果是 True 1.2 断言调用-asserting calls对于外部命令需要检验其是否被执行、类方法是否被调用等，测试过程中将相应的命令或者方法进行 mock 之后在调用 assert_call* 等方法，即可进行测试: >unfolded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 创建类class Elimination: def __init__(self, values, *, rows=None, cols=None, copy=True) -&gt; None: if rows is None and cols is None: rows = math.sqrt(len(values)) cols = rows elif rows is not None and cols is not None: pass else: miss = &quot;rows&quot; if rows is None else &quot;cols&quot; raise Exception(f&quot;Arguments '{miss}' lost&quot;) # 创建环境 rows, cols, grid = int(rows), int(cols), int(math.sqrt(rows)) self.env = Board(rows, cols, grid) if copy: self._copy = self._copy_env(self.env) @classmethod def _copy_env(cls, board): result = Board(board.rows, board.cols) for cell, target in zip(result, board): if isinstance(target.value, int): cell.value = str(target.value) elif isinstance(target.value, str): if target.value.isdigit(): cell.value = target.value elif target.value == &quot;&quot; or target.value == &quot;.&quot;: cell.value = [] else: raise ValueError(f&quot;Cell value is not a digit value,&quot; f&quot; get type {type(target.value)}&quot;) return result# 使用 mock 测试class TestElimination(unittest.TestCase): def setUp(self) -&gt; None: return super().setUp() def test_copy_env_classmethod(self): # 这里进行测试 elimination.Elimination._copy_env = mock.MagicMock(name='copy') elim = elimination.Elimination(&quot;1134223&quot;) elim._copy_env.assert_called_once() Patch 测试Mock 可以拥有模拟外源数据，对于硬编码无法模拟出外源数据点情况可以通过 patching 的方式进行伪造对象——例如模拟外部对象，这样可以实现全局进行访问而并非真实创建一个需要的对象。 参考 参考书籍 Leonardo Giordani, Clean Architectures In Python unittest.mock &lt;a","link":"/%5Bobject%20Object%5D/Python-Mock%E6%8A%80%E5%B7%A7/"},{"title":"Pytest 不常用使用技巧","text":"Pytest 设置模式测试使用 Pytest 进行测试时，如果需要对项目内的脚本进行测试时。可以设置 pytest.ini、pyproject.toml、tox.ini 或者 setup.cfg 等文件配置需要测试的文件相关信息（但是不建议配置 .cfg 文件[^1]）。下例为一个简单的配置文件: pytest.ini >unfolded1234[pytest]minversion = 2.0norecursedirs = .git .tox requirements*python_files = test*.py, __init__.py, test_*.py 其中 python_files 属性确定了可以用于测试的范围为 “test” 开头以及 “test_” 开头以及 “init“ 的脚本文件 Pytest 异常测试通常情况下测试是检验结果是否符合要求，但某些情况下需要测试的是满足某种条件下会发生异常。Pytest 提供了相应的方法实现运行(使用 pytest.raise())结果发生异常的情况才能通过测试: test.py >unfolded1234567import pytest#... 省略代码def test_mul_by_zero_raise_exception(): # 测试乘数中存在 0时，返回值异常 obj = Objec() with pytest.raise(ValueError): obj.mul(3, 0) 测试代码运行时间需要测试代码运行时间消耗可以使用命令 pytest --durations=0 -vv &lt;file_name.py&gt;，其中参数 --durations 表示时间超过该时间——即该方法是表示代码运行时间的下限 限定测试在某些情况下，比一定需要测试项目中所有文件的前提下，可以在测试脚本中添加标识之后使用 -m 参数以选择需要测试的限定脚本。 test_postgresrepo.py >unfolded123456import pytestpytestmark = pytest.mark.integrationdef test_dummpy(): pass 使用 py.test -svv -m integration 可以限定测试有 integration 的脚本。可以参考文档 mark 说明 参考书籍 Leonardo Giordani, Clean Architectures In Python 参考[^1]: API Reference — pytest documentation","link":"/%5Bobject%20Object%5D/Python-Pytest%E6%8A%80%E5%B7%A7/"},{"title":"cookecutter 使用以及整洁架构设计","text":"在设计架构过程是使用 cookiecutter 能够方便快速的搭建统一的框架。该笔记是依据 《整洁架构》搭建日志模版的过程和思路整理。 1. Python 环境依赖拆分Python 在开发环境、测试环境以及生产环境下，可能对相关 package 依赖方面存在差异。因此将三个环境依赖拆分开是有实际意义的，创建一个 requirements 目录之后分别生成 test.txt、prod.txt 以及 dev.txt 分别存放三个环境依赖。 在实践过程中可以使用 virtualenv 创建需要的环境，使用 pip install -r requirements/&lt;env_name&gt;.txt 可以分别创建三个不同的依赖环境 2. 测试方案和 Python 标准检验2.1 Pytest 工具配置使用 pytest 进行测试时，可以在 package 的根目录下存放一个 pytest.ini 文件，添加相关配置之后可以直接在根目录下使用 py.test -sv 进行快速测试。配置信息如下： pytest.ini >unfolded1234[pytest]minversion = 2.0norecursedirs = .git .tox venv* requirements*python_files = test*.py 2.2 覆盖测试命令Coverage 和 pytest-cov 提供了覆盖测试的方式，可以结合 pytest 命令一起进行覆盖测试。使用命令 py.test --cov-report term-missing --cov=&lt;package_name&gt; 可以进行覆盖测试，将返回详细的测试报告 2.3 PEP 标准检查Python 编程过程中主要的代码规范标准是 PEP8，flake8 工具提供了该该规范检查。该规范检查可以在 setup.cfg 中添加配置信息之后，使用 flake8 命令检查。相关的配置信息如下： setup.cfg >unfolded1234[flake8]ignore = D203exclude = .git, venv*, docsmax-complexity = 10 参考 flake8 文档","link":"/%5Bobject%20Object%5D/Python-cookecutter%E4%BD%BF%E7%94%A8/"},{"title":"[统计学习]第一章基本概念","text":"1. 基本概念统计学习（statistical learning），谈论的是统计机器学习（statistical machine learning），解决的方式是利用数据，抽取出相关特征，构建数据的模型以发现数据中的知识，并最终对未知数据进行分析和预测。而围绕数据的角度来有一个基本的假设，即建立知识模型的数据和预测分析使用的数据具有相同的性质。而且对于数据 2. 模型分类围绕统计学习包括的内容分别为模型假设空间、模型选择准则以及模型学习算法，这三个方面对应的统计学习的三个要素为模型、策略以及算法。其中模型从类型角度可以划分为： 概率模型和非概率模型/确定性模型，概率模型是以条件概率 $P(y|X)$ 形式表示（以函数形式来表示即 $y=f(x)$），以监督式学习来看这个结构，它是一个生成式模型。可以理解为需要从数据中学习到相应的参数，以对数据进行预测。非概率模型以函数形式表示 $y=f(X)$ ，从监督式学习的来看它是一个判别式模型。 生成式模型：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型 判别式模型：感知机、支持向量机、$k$ 近邻、AdaBoost、$k$ 均值、潜在语义分析、神经网络 此外还有一个特殊的模型逻辑回归模型，它可以用作生成式模型，也可以是作为判别式模型^1。 概率模型的代表是概率图模型，其联合概率分布可以用有向图和无向图表示，其中的例子包括贝叶斯网络、马尔可夫随机场、条件随机场。 线性模型和非线性模型 是否为线性模型的划分依据是模型是否使用的线性函数。 线性模型：感知机、线性支持向量机、 $k$ 近邻、$k$ 均值以及潜在语义模型 非线性模型：核函数支持向量机、AdaBoost 以及神经网络 参数化模型和非参数化模型 划分的依据是在搭建模型过程中，训练模型的特征维度是否会随着使用的数据量增大而增加。在应用场景中，参数化模型适用于解决简单问题，而解决复杂的问题使用非参数化模型可能会更有效率。 参数化模型：感知机、朴素贝叶斯、逻辑回归、$k$ 均值以及高斯混合模型 非参数模型：决策树、支持向量机、AdaBoost，$k$近邻、潜在语义分析、概率潜在语义分析以及潜在狄利克雷分配 3. 统计学习三要素3.1 策略作为统计学习三要素之一的策略，指的是采用什么样的 criterion 进行学习和选择最优的模型，达到从假设空间中选择“最优”的模型。因此选择 criterion 的过程可以从损失和风险两个角度来度量模型，损失函数用于度量模型单次预测的好坏，风险函数度量模型预测的平均好坏。 3.1.1 损失函数损失函数（Loss Function）或者叫成本函数（Cost Function），计算的是模型对给定输出结果 $f(x)$ 和真实结果 $y$ 之间的差异程度。常用的损失函数类型： 0-1损失函数$$\\begin{align}L(y_i, f(x_i))=\\begin{cases}1,\\ y_i\\ne f(x_i) \\0,\\ y_i = f(x_i)\\end{cases}\\end{align}$$通过比较预测值和实际值是否相等的方式，应用上和分类问题中的评估中有相关性 平方损失函数（quadratic loss function）$$\\begin{align}L(y_i, f(x_i))=(y_i-f(x_i))^2\\end{align}$$计算实际值和预测值之间的距离 绝对损失函数（absolute loss function）$$L(y_i, f(x_i))=|y_i-f(x_i)|$$ 对数损失函数（logarithmic loss function）或对数似然损失函数（log-liklihood loss function）$$L(y_i, P(y_i|x_i))=-\\log{P(y_i|x_i)}$$通常是计算给定输入 $x_i$ 得到 $y_i$ 的概率 ⚠️注意，以上 $x_i$ 和 $y_i$ 都是属于随机分布的变量。 3.1.2 风险函数上面的损失函数是针对的单个输入的角度，但模型的输入和实际输出是随机变量，遵循联合分布 $P(y,X)$ 的期望结果。风险函数即期望损失，是模型 $f(x)$ 在联合分布下 $P(x,y)$ 的平均损失：$$\\begin{align}R_{exp}(f) &amp; = E(L(y, f(X)))\\&amp; =\\int_{x\\times y}L(y,f(x))P(x,y)d_xd_y,\\end{align}$$ 学习的目标是期望能够明确期望损失最小的模型，但实际情况下，联合分布的概率是未知的，不能直接得到期望风险，而模型利用训练数据集得到的平均损失是一种经验风险或称为经验损失。因此为了通过经验风险来估计期望风险的方法之一，是通过大量数据训练的方式是经验风险趋于期望风险。在假设空间、损失函数以及训练数据集一定的情况下，经验风险是确定的，而其优化的方案是通过最小化经验风险的方式来求解最优模型：$$\\min_{f\\in \\mathcal{F}}\\frac{1}{N}\\displaystyle{\\sum_{i=1}^N L(y_i, f(x_i))}, \\mathcal{F}\\text{ 是假设空间}$$在小样本的情况下，通过可能并不能通过经验风险来良好估计期望风险，因为会出现过拟合的情况。因此结构风险最小化是另一个监督式学习优化策略，通过在经验风险的基础上添加结构风险（例如添加正则项^2）。在整个模型的最终优化方案变为:$$\\min_{f\\in \\mathcal{F}}\\frac{1}{N}\\displaystyle{\\sum_{i=1}^N L(y_i, f(x_i)) +\\lambda \\mathit{J(f)}}$$$\\mathit{J}$ 是用于评估模型复杂度，是定义在 $\\mathcal{F}$ 下的泛函数， $\\lambda$ 一个非负数，它是用于权衡经验风险和模型复杂度。 3.2 模型监督学习场景下，模型就是指的在数据集下的所有可能的条件概率或者决策函数，而这些所有可能的集合就是模型的假设空间。所谓的数据决定了模型的上限的意思，就是说数据的确定那么模型假设空间也即确定了。 3.3 算法指学习模型的具体计算方法，主要思考的问题用什么样的计算方法求解最优模型。例如是通过解析解来得到最优化问题解，还是通过数值计算的方式计算得到全局最优解 参考监督式学习方法分为生成方法和判别方法，前者由根据数据的联合概率分布 $P(X,y)$ ，得到条件概率 $P(y|X)$ 作为数据模型：$P(y|X)=\\frac{P(X,y)}{P(X)}$，模型预测是给定输入得到其和输出之间的关系。而判别方法是数据直接学习决策函数 $f(X)$ 或者条件概率 $P(y|X)$ 作为数据模型的方法，模型的预测是给定输入，预测得到相应的输出（没有关心输入和输出之间的关系）。 两者的特点： 生成模型：可以还原出联合概率，学习收敛的速度快，存在隐变量时也能得到模型用于预测 判别模型：不能还原出联合概率的分布，得到条件概率或者决策函数直接用于预测，通常情况下准确率较高；学习过程需要对数据抽象、以及对数据特征处理，从而能够简化学习问题 监督式学习下生成式模型和判别式模型其他角度认识： 生成模型预测的展开式 $P(y,X)=P(y|X)\\times P(X)$ ，可以被看作是判别模型和关于特征的先验概率两个部分，不仅需要解决判别模型的问题还需要解决关于特征的先验模型（通常来看对于分类问题，判别模型优于生成模型；对于小样本时生成模型可能会优于判别模型，相当于加入正则化处理是对 $P(X)$ 的处理） 正则化是用于调整模型复杂度的，最终的目标是降低模型结构风险。经验风险的降低的一个原因是模型复杂度增加，因此添加正则化之后模型最终选择的经验风险和模型复杂度同时较小的模型。 从奥卡姆剃刀原理（Occam’s Razor）的角度来说，能够以简单有效的模型解释数据，这类模型才是最佳模型。从贝叶斯估计的角度来说， 不同概率分布： 数据分布假设： 假设的依据是计算的便利性，因此能够提供更好的计算便利性的假设就是可用的 数据分布常使用高斯分布的原因，数据抽样过程会服从高斯分布 说明： 封面图来自 Charts On Black Wooden Table 缩略图来自 Flickr janeqjames","link":"/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"},{"title":"[统计学习]第三章 K 邻近法","text":"K-Nearest Neighbour 即 K 邻近算法，是可用于解决分类和回归问题的算法。在用于解决分类问题的思路是在已知的数据实例上，对于新的实例根据 $k$ 个最邻近的已知训练实例通过多数表决的方案进行预测，因此 $k$ 邻近算法不是一个显式学习过程。$k$ 邻近算法模型要素是通过 $k$ 选择，距离度量以及分类决策规则确认。 1. k 邻近算法$k$ 邻近算法是通过多数表决的方式来确定新实例的类别，其整个训练流程: 对于输入的实例 $T={(x_1, y_1), (x_2,y_2),\\cdots,(x_N, y_N)}$ 满足 $x_i\\in\\mathcal{X} \\subseteq \\mathrm{R}^m$ 和 $y_i \\in \\mathcal{y}={c_1, c_2,\\cdots,c_k}$，需要预测的实例 $x_i^\\prime$ 输出的 结果是 $y$ 中某个类别 学习的过程是实现 $y=\\displaystyle{\\text{argmax}_{c_j} \\sum_{x_i\\in N_{k}(x)}} I(y_i=c_j)$，当 $y_i=c_j$ 时 $I(y_i=c_j)$ 为 $1$，否则为 $0$ 因为 $k$ 的不同选择，会有不同的结果，例如 $k=1$ 时表示最邻近的数据决定其类别。 1.1 模型在完成建立邻近算法模型时，邻近的度量指标、$k$ 值选择以及相关的决策规则已经确立，对应于需要预测的新数据时通过模型确立时使用的特征子空间来判断所属的分类。换句话说也就是 $k$ 邻近算法的模型，是解决了从特征空间中选择合适的子空间来确认数据点分类。 下图的二维特征空间的例子，构建了每个实例点与其邻近的所有点组成的区域（这样的区域就是单元）： 1.2 度量指标$k$ 邻近算法是非显性的学习，而判断学习效果需要一个重要的指标。而距离是一个比较通用的指标，包括了各种范数指标。不论是 $L_2$ 的欧式距离、$L_1$ 曼哈顿距离，还是 $L_p$ 的 Minkowski 距离，都可以通过以下的方式进行评估:$$\\begin{align}L_p(x_i, x_j)=\\big(\\displaystyle{\\sum_{m=i}^m |x_i^m-x_j^m|^p} \\big)^{\\frac{1}{p}} \\tag{1}\\label{1}\\end{align}$$等式 $\\ref{1}$ 中 $x_i,x_j \\in \\mathcal{X}$ 且 $m$ 为维度特征数量，Minkowski 距离 $p$ 范数是更泛化的距离，其取值范围为 $[1, \\infin]$。其中当取值为 $\\infin$ 时，计算的方式为 $\\displaystyle{\\max_{m}|x_i^m-x_j^m}|$，即取得各个坐标距离的最大值 1.3 k 值选择$k$ 值差异会影响投票决策的数据点，如果 $k$ 越小表明训练实例可作用的领域会较小，很容易受到噪声的影响，容易发生过拟合，模型结构复杂度更高。反之如果 $k$ 越大，可以减小学习的估计误差，模型的复杂度相对较小。例如当 $k=N$ 时预测的结果由实例中最多的类决定。 2. KD 树KD 树（即 K 维树，K Dimension Tree），是解决数据点在 K 维中划分问题的数据结构，其应用场景包括多维空间数据的搜索。而在 KNN 算法中，可以减少线性扫描计算输入实例和每个实例之间的距离所需要的时间消耗。 2.1 KD 树构造KD 树实际还是是一个二叉树，其构建 k 维空间的划分可以看作在垂直于坐标轴的 k 维空间切分数据，最终产生一系列的 K 维超矩形区域。实例在被拆分到子区域内后（拆分为两个子区域），子区域会继续拆分直到子区域内没有实例时终止。在进行寻找子区域拆分点时，通过查找实例点在坐标轴上的中位数作为切分点——这样的好处是可以得到平衡树，但是在搜索上却未必是最有效率的一种方式。 2.1.1 构造算法实施 KD 树的算法（构造平衡 KD 树）的过程如下： 输入：具有 $k$ 个维度的数据集 $T={x_1,x_2,\\cdots,x_N }$，用列向量表示 $x_i=(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(k)})$ 其中 $i\\in[1,N]$ 过程: 构造根节点 构建深度为 1 的二叉树（即根节点，其节点深度为 $0$），因此需要从 $k$ 维中选择一个特征（选择第一个特征 $x^{(1)}$）作为第一个切分点。切分点的查找是通过中位数点来确认，通过切分点可以在选择的维度上讲数据分为大于切分点和小于切分点两个子区域 构造深度为 $j$ 的节点，需要注意⚠️深度是选择特征的方式，其计算方式（计算第 $l$ 维特征选择）：$l=j (\\text{mod}\\ {k}) + 1$。通过选定的特征确认切分点，获取 $j$ 节点的数据划分。 终止条件，需要满足两个子区域没有实例 输出: KD 树 2.1.2 示例[^2]使用二维数据 $T={(70, 721), (207, 313),(479,449), (615,40),(751,177),(888,555),(343,858) }$ 进行模拟构建 KD 树: 选择第一维数据 ${70,207,479, 615, 751, 888,343 }$ 进行排序，通过中位数（$(479,449)$）进行数据拆分 $T_{\\text{left}}={(70,721),(207, 313), (343, 858)}$ 和 $T_{\\text{right}}={(615,40),(751,177),(888,555) }$ 利用下面的式子计算下一个节点使用的维度：$$l=j(\\text{mod }k)+1$$例如划分第二个字节点时，数据 $T$ 的维度 $k$ 为 $2$，节点 $j$ 的值为 $1$，那么选用的维度 $l= 1 (\\text{mod } 2)+1=2$，得到的结果是使用第二个维度划分第二个节点为。 在 $T_{\\text{left}}$ 和 $T_{\\text{right}}$ 中分别使用 ${721, 313, 858 }$ 和 ${40, 177, 555 }$ 获取中位数切分点，得到节点的左右叶节点 重复步骤 2 和步骤 3，直到平衡二叉树不可以在分隔出叶节点 2.2 KD 树搜索KD 树进行 $k$ 近邻搜索时，可以利用其平衡树的特点减少搜索的计算量，其搜索的步骤为： 通过对目标点进行计算，搜索其最近邻点 通过包括目标点的叶节点（指的是包括目标点的最小超矩形区域）进行回退，一直回退到根节点。在过程中需要不断确定与目标点最近邻的节点。最近邻的搜索过程是以目标点为中心并通过当前最近点的超球体内部，之后返回当前叶节点的父节点。如果在父节点的另一子节点中超球体区域与当前超球体相交，那么以相交区域寻找更近实例点——如果存在这样点，那么其为最新的当前最近点。如果在过程中父节点的另一区域不与超球体相交，或者不存在比当前最近点更近的点，那么直接回退 在根节点时结束，确定的最近邻节点即为最终结果 2.2.2 KD 树搜索算法下面的搜索过程是最近邻搜索，不是 K 近邻搜索： 输入：构造完成的 KD 树以及需要查找的点 $x$过程： 在 KD 树中找出包含目标点 $x$ 的叶节点，需要从根节点出现递归向下访问——因为是平衡树，$x$ 对比的维度上数据小于节点同维度值，向左节点移动反之向右。最终需要在叶节点上停止，以此叶节点为最近点 向上回退搜索——存在两种方式搜素 回退后的节点，如果该节点比当前最近点距离目标最近，那么该店为新的最近点 当前最近点的节点，需要检查同级的另一子节点对应区域是否存在更近点。检查的方式是以另一子节点对应的区域是否存在以目标点为球心的超球体，与目标点和当前最近邻点为半径的超球体是否相交——直接通过比较两者的半径是否差值是否小于 0 。相交那么可以跳到另一个子节点递归搜索，反之直接递归回退 到达根节点时停止 输出：目标点的最近邻节点 参考[^1]: kd 树算法之思路篇 - JoinQuant量化课堂[^2]: K-D Tree: build and search for nearest neighbor[^3]: 平衡二叉搜索树[^4]: knn算法-kd树原理(搜索) 说明: 封面图 “File:K-nearest Neighbors.png” by Pkuwangyan06 is licensed under CC BY-SA 4.0 缩略图来自 Flickr janeqjames","link":"/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%89%E7%AB%A0K%E9%82%BB%E8%BF%91%E6%B3%95/"},{"title":"[统计学习]第二章感知机","text":"感知机（Perceptron），是线性分类模型，利用一个线性超平面对数据进行二分类。 1. 感知机模型感知机的模型的假设是对于输入空间中的变量，经模型$$f(x)=\\text{sign}(w\\cdot x+b) \\tag{1} \\label{1}$$ 得到输入变量 $y\\in \\lbrace -1, +1 \\rbrace$ 。对于模型中的 $\\text{sign}$ 它是一个指示函数，用于筛选在某种条件下属于正例，反之属于负例。该模型是属于 $y=f(x)$ 的模型，即是一个判别模型。 2. 感知机学习策略对于给定的数据集是线性可分的：即满足 $y_i=+1$ 的实例 $i$ 满足 $w\\cdot x_i+b&gt;0$，反之实例 $i$ 满足 $w\\cdot x_i+b&lt;0$ 的条件的情况下数据集才是线性可分的，否则数据集是线性不可分的。 确定感知机模型参数 $w$ 和 $b$ 是通过明确损失函数最小化的来明确。这个经验损失函数误分类数据到超平面的距离确认的：$$\\begin{align}L(w) &amp; = -\\frac{1}{\\left| w\\right|}\\displaystyle{ \\sum_{x_i\\in X}y_i(w\\cdot x_i+b)} \\\\end{align}$$上面的公式中误分类点的确认是通过 $y_i \\left(w\\cdot x_i+b \\right)$ 来判断，因为是误分类的情况 $y_i$ 和 $\\left(w\\cdot x_i +b \\right)$ ^1的正负号不一致情况，结果一定是小于零；另外当 $x_i$ 在 $w\\cdot x_1+b$ 的线上时 $w\\cdot x_1+b=0$，损失函数结果为零 。其中 $\\left |w \\right|$ 是参数 $w$ 的二范数，数据值肯定是大于零。整个函数的最优化问题变为求解误分类点的。 在进行计算优化策略：没有误分类的点，损失函数的值为 $0$；误分类点的损失函数值为 $\\left( w\\cdot x+b\\right)$。最终是从假设空间 $\\eqref{1}$ 中选择损失函数最小的模型参数 $w$ 和 $b$。 3. 感知机算法3.1 算法流程感知机算法是一个误分类驱动的解决的问题，在计算过程中使用的随机梯度下降方法（Stochastic Gradient Descent）是其中一种方法，计算的方式就是就是利用下式计算出更新数据值：$$\\begin{align}\\frac{\\partial{L(w,b)}}{\\partial w}&amp;=-\\displaystyle{\\sum_{x_i\\in X}y_i\\times x_i} \\\\frac{\\partial{L(w,b)}}{\\partial b}&amp;=-\\displaystyle{\\sum_{x_i\\in X}y_i} \\\\end{align}$$参数^2每步更新的具体数据值需要通过学习率 $\\eta$ 来确定“步长”，对于任意一个发生误分类(即满足 $y_i(w_i\\cdot x_i+b)\\le 0$)的数据点 $(x_i, y_i)$ 的权重更新如下：$$\\begin{align}w &amp;\\leftarrow w+\\eta \\times y_i\\times x_i \\b &amp; \\leftarrow b + \\eta \\times y_i\\end{align}$$ 3.2 算法收敛性证明感知机算法的收敛需要证明的是在数据集 $T=\\lbrace (x_1,y_1),(x_2, y_2),\\cdots, (x_N, y_N)\\rbrace$ 满足线性可分的条件下，感知机随着参数更新，其误分类次数 $k$ 是有上界的。对 Novikoff 定理解读： 存在满足条件的 $\\vert \\hat{w}{\\text{opt}} \\vert=1$ 的超平面 $\\hat{w}{\\text{opt}}\\cdot \\hat{x}=w_{\\text{opt}}\\cdot x+b_{\\text{opt}}$ 经数据集正确分开，同时存在$\\gamma \\gt 0$ 对所有 $i=1,2,\\cdots, N$ 满足 $y_i\\times(\\hat{w}{\\text{opt}}\\cdot \\hat{x_i})=y_i\\times (w{\\text{opt}}\\cdot x_i+b_{\\text{opt}})\\ge \\gamma$ 解读： $|\\hat{w}{\\text{opt}}|=1$ 的意思是参数 $w{\\text{opt}}$ 和 $b_{\\text{opt}}$ 构造的超平面的法向量的长度为一，且这个超平面能够将数据正确分类——那么 $y_i \\times f(x_i)$ 的结果肯定是非负数的 假定 $R=\\displaystyle{\\max_{1\\le i \\le N}|\\hat{x}_i|}$，那么感知机算法在数据集中误分类次数 $k$ 满足不等式 $k\\le \\left(\\frac{R}{\\gamma} \\right)^2$ 解读： 这个条件是算法收敛的最终说明，理解该结果需要有一个完整的证明过程手写证明过程 3.3 感知机算法的对偶形式对偶形式的思路是，以 $w$ 和 $b$ 作为数据实例 $x_i$ 和对应的标签 $y_i$ 的线性组合形式，求解系数就可以得到对应的参数。如果参数初始化的值为 $\\vec{0}$，那么通过 $W\\leftarrow W+\\eta\\times y_i\\times x_i$ 更新参数的方式，最终参数更新的累计值是 $\\displaystyle{\\sum_{i=1}^N \\alpha_i\\times y_i\\times x_i}$ ^3。 在步骤三中 $x_j\\cdot x_i$ 是构建的 Gram 矩阵^4。迭代计算的步骤中，感知机算法的对偶形式和原始形式是对应的，同时结果也是存在多个解。 参考Dot products and duality 中可以了解到点积 $\\left( w\\cdot x_i\\right)$对应的几何变换是 $x_i$ 和 $w$ 之间的有“方向”的距离计算，添加上 $b$ 可以看作是在延着超平面截距移动。从下面的图中可以理解任意点 $x_i$ 到超平面的距离可以通过 $\\frac{\\left(w\\cdot x_i \\right)}{\\left | w\\right|}$ 方式得到（在不考虑截距移动的移动情况下）： 在使用 SGD 的过程中，首先会有一步初始化参数 $w$ 和 $b$ 的过程，因此在得到的结果上受初始化的值和误分类点不同，得到的结果也不同。梯度下降和 SGD 的差异在于，SGD 每一步计算的损失是不需要将所有的数据点都计算完，主要是存在大量数据的情况下 SGD 得到的损失是梯度下降损失的无偏估计。因此 SGD 能够保证计算结果，同时能够提高计算的效率 在算法中更新的参数是通过错误计数点来计算的，所以 $N$ 表示的是错误分类点，也就是 $\\alpha_i=n_i\\times \\eta$ 中 $n_i$ 是在第 $i$ 次更新参数过程中有 $n_i$ 个错误分类点 Gram 矩阵是通过数据 $X\\in \\mathbb R^{N\\times N}$ 的内积形式构建的：$\\text{Gram Matrix}=[X_i\\cdot X_j]_{N\\times N}$，其重要的作用是确认数据特征之间的相关程度，所以其中的 $i$ 和 $j$ 都是表示的列索引。在应用上可以参见 A Neural Algorithm of Artistic Style 中应用 Gram 矩阵来分析图片的风格损失，这里的应用是通过 Gram 矩阵计算出图片的”风格“特征。 说明: 封面图 “Perceptron” by fdecomite is licensed under CC BY 2.0 缩略图来自 Flickr janeqjames","link":"/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E7%AB%A0%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"title":"需求和价格分析","text":"对于商品来说不同的定价策略会影响到收益和利润，理解不同的需求的消费者的消费意愿对价格的响应具有重要的实践意义。从分析的角度利用需求曲线，以及价格对市场环境拟合可以用于评估利润的最大值。在基础信息包括： 成本产量，生产单位产品的成本 产品的需求曲线，反映用户对产品不同价格的需求数量。本次的讨论不扩大影响产品需求变化的其他因素——例如经济环境以及竞争者价格，单纯从价格方面讨论 价格弹性，反映给定的需求曲线下，价格每增加一个百分点致使需求减少的百分比 $\\text{price elasticity} =-\\frac{(\\text{demand}_2 - \\text{demand}_1)*100}{1% \\text{price}_1\\text{demand}_1}$ 。具有价格弹性的表现是价格弹性大于 1，反之当数据值小于 1 时，则是不具有价格弹性 需求曲线利用价格变化，需求响应变化得到曲线即为需求曲线，常用需求曲线包括线性需求曲线和乘幂需求曲线。 线性需求曲线需求数量和价格存在 $\\text{demand}=a*\\text{price}+b$ 的线性关系，线性需求曲线上价格弹性是变化而非一个常数: $$\\begin{align}&amp;\\text{pe} = -\\frac{\\Delta{demand}100}{1% \\text{price}_1\\text{demand}_1} \\&amp;\\frac{\\text{pe}(\\text{demand}_1^21% - b)}{100a} + \\text{demand}_1= \\text{demand}_2\\end{align}$$ 因为价格弹性 $\\text{pe}$，需求价格线性关系系数 $a$、$b$ 是常量，所以需求的变化不是线性的。 如果存在价格和需求数据，可以使用 statsmodel 进行线性拟合: linear_fit.py >folded1234567891011121314151617181920212223242526import ioimport scipyimport pandas as pdimport statsmodels.api as sm# 模拟数据data = io.StringIO(&quot;&quot;&quot;price,demand100,500101,490&quot;&quot;&quot;)df = pd.read_csv(data)# 使用 scipy 进行线性拟合endog = df[[&quot;demand&quot;]].to_numpy()exog = df[[&quot;price&quot;]].to_numpy()# 添加常数项exog = sm.add_constant(exog)# 使用最小二乘法线性拟合model = sm.OLS(endog, exog)result = model.fit()# 得到模型系数result.params # 得到的结果是 array([1500., -10.]) 乘幂需求曲线乘幂需求曲线的关系是 $\\text{demand}=a*\\text{price}^b$，此时 $-b$ 为需求的价格弹性。如果存在价格和需求数据，需要拟合出系数可以通过 scipy 的包进行: linear_fit.py >folded12345678910111213141516171819202122232425262728293031import ioimport scipyimport pandas as pdimport statsmodels.api as sm# 模拟数据data = io.StringIO(&quot;&quot;&quot;price,demand50,200060,1388.88970,1020.40880,781.2590,617.284100,500110,413.2231120,347.2222130,295.858140,255.102150,222.222&quot;&quot;&quot;)df = pd.read_csv(data)# 使用 scipy 的 curve_fit 拟合 y=a*x**bydata = df[&quot;demand&quot;].to_numpy()xdata = df[&quot;price&quot;].to_numpy()# curve_fit 需要申明幂函数表达式:ydata = f(xdata, *params) + epspopt, pcov = scipy.optimize.curve_fit(lambda x, a, b: x**b * a, xdata, ydata,)# 前者是结果参数，后者是参数的协方差popt # array([ 5.00000270e+06, -2.00000013e+00]) 规划求解最优价格需求和价格达到最佳的匹配才能获取到最大化利润，应用中通过规划求解的方式得到最优价格是有实际意义的。规划求解的模型条件包括三个部分： 需要达到的最优目标值 可变条件，是可以调整的变量，在当前条件下即是价格 约束条件，对于可变条件的限制条件 规划求解根据求解的问题类型不同，需要使用不同的 solver 解决问题（可以参考 Post not found: 数据分析-规划求解 规划求解）。本例子是针对剃须刀的定价策略，给定的条件是需求的价格弹性为 2，单个的可变成本为 2.00$，价格在2.00$ 时，需求的数量为 600 万个。 针对需求解决定价问题目的是达到利润最大化，而 $\\text{profite}=\\text{price} \\times \\text{demand}$ 中价格和需求都是变化的，因此在使用 solver 时不能使用线性规划求解器。 #TODO: 参考 Get Started with OR-Tools for Python | Google Developers","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E9%9C%80%E6%B1%82%E5%92%8C%E4%BB%B7%E6%A0%BC%E5%88%86%E6%9E%90/"},{"title":"Attention Is All You Need 笔记","text":"1. 背景在解决翻译问题时，如果句子长度过长 Encoder-Decoder 的模型也不能得到良好的结果。这样的问题就是长序列会遇到的问题，其主要原因是序列过长时，向后处理序列时前面的序列会出现“遗忘”问题。通过 Attention 的方式可以缓解这种问题，增加长记忆的能力。 RNN 存在的问题之一，不能并行化——即 $t$ 时刻结果序列需要依赖于 $t-1$ 时刻前的所有输入序列。虽然 CNN 可以进行并行化计算，但 CNN 也存在缺点：1）是在序列模型中，只能考虑到 $kernel-size$ 大小的序列，而不能进行全局考虑；2）当然可以通过 CNN 叠加的方式，实现在上层网络输出一定程度上考虑更长的信息。需要的网络复杂性较大，才能进行全局信息。 2. 模型机制self-attention 解决了全局信息需求的同时，而且可以进行并行化运行。结构上还是参考了 Encoder-Decoder 结构，其中 Encoder 使用叠加六个相同结构的 layer，其中使用了多头注意力机制: 三个重要的变量 $Q$ 查询，表示需要匹配其他的对象；$K$ 键，需要用于被匹配的对象；$V$ 提取出的信息。 2.1 Positional Encoding 细节self-attention 天然并未解决输入的位置关系问题，其计算过程是并行化计算不依赖于上一个 token 的输出（而 RNN 是具有一定前后序列关系）。为了加入 Token 的位置顺序，论文中加入了 positional encoding 的方法来解决位置因素——添加的信息考虑到绝对位置。 模型中的位置信息是在输入和输出的 Embedding 中添加上 Positional Encoding，根据奇偶位置采取正弦和余弦函数得到编码的数据值:$$PE_{(pos, 2i)}=\\sin(\\frac{pos}{10000^{2i/d_{model}}}) \\PE_{(pos, 2i+1)}=\\cos(\\frac{pos}{10000^{2i/d_{model}}})$$其中 $pos$ 表示为位置，而 $i$ 是表示维度（representation dimension，对应的值是在 $d_{model}$ 中的位置）。以该方式得到相关的位置信息，进行可视化展现出位置以及维度影响： 从第二张图可以看出这种序列顺序，通过这种差异表现出来，而且从第一张图可以看出这种关系具有一定的周期性（论文中阐述到这种周期性长度从 $2\\pi$ 到 $1000\\cdot 2\\pi$）。Positional Encoding 的应用是将其值和 Embedding 值相加即引入了顺序值。 2.2 多头注意力机制2.2.1 scale dot product attention通用的 attention 结构包括包括 additive 模式和 multiplicative 模式，而在论文中采用了 scale dot product attention，数学表达式为 $\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$，其中 scale 的步骤是通过 $\\frac{1}{\\sqrt{d_k}}$ 完成——它的作用也是避免高维度点积处理 softmax 之后梯度过小的问题。 将整个 sequence 作为输入，self-attention 的流程如下： 在生成 $Q$、$K$ 和 $V$ 的等式中各个 $W$ 对应的输入 token 都是共享的。上图在论文中的模型结构如下: 2.2.2 Multi-Head Attention论文中引入了 $h$ 个对 $Q、K、V$ 的并行化计算的映射取得了较好的效果，这样的好处类似于能够得到更多的注意力信息。拆开来看多头类似于下图中（下图只是使用了两个 head），将使用原有的一套 $Q、K、V$ 变为了两套——需要注意每一套内部相关联计算，而不是交叉计算。 多头生成的结果使用了 concat 拼接的方式得到结果，之后将拼接的结果进行了线性变换以实现降维度目的。最终的数学表达式为 $\\text{MultiHead(Q,K,V)}=\\text{Concat}(\\text{head}_1,\\cdots,\\text{head}i,\\cdots,\\text{head}_h)W^O$，其中 $\\text{head}_i=\\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)$，$W^O$ 是一个 $\\mathbb{R}^{hd_v \\times d{model}}$ 的矩阵。Multi-Head Attention 的结构如下： 2.2.3 Encoder 和 Decoder 其他细节在两个 Block 中均存在残差网络和 Layer Normalization，此外在 Decoder 的 Block 中有两次多头处理，而且在第一层多头中添加了 Mask 处理。 残差网络可以在增加网路深度时，能够维持梯度存在以避免深度过深而梯度消失 Layer Normalization 可以类比于 CNN 中对 Channel 的维度方向上进行 Normalization，而在这里使用是为了保持 Tokens 在多个序列上缩放 使用 Mask 是因为在训练过程中，添加了后续数据的位置信息（在 Positional Encoding 是添加上的），需要避免在在预测时结果序列数据完全未知，因此需要在训练阶段将未参与的位置信息屏蔽掉 备注 The Annotated Transformer Transformer 实现细节 Transformer: A Novel Neural Network Architecture for Language Understanding Google 博客对 Transformer 解释","link":"/%5Bobject%20Object%5D/%E8%AE%BA%E6%96%87-AttentionIsAllYouNeed/"},{"title":"[统计学习]第四章朴素贝叶斯法","text":"朴素贝叶斯法是以特征独立性假设为基础，利用贝叶斯定理进行分类方法。因为在朴素贝叶斯方法过程中，需要在学习过程中学习到生成数据的模式——该模式是能够进行预测前 $P(\\hat{y}|X)$ 需要通过 $P(X|y)\\times P(y)$ 方式能够构建数据生成机制；此外依赖独立性假设，使预测的方式转换为求解最大化后验概率来预测结果。 1. 朴素贝叶斯的推导朴素贝叶斯首先需要通过先验概率（即实际数据中标签在不同的类别 $k$ 的比例）和条件概率的分布（即满足标签条件下，特征集的比例）学习联合概率分布： $$P(y=\\text{label}), \\text{label}\\in[1,2,…,k] \\P(X=x|y=\\text{label}) = P(x^{(1)},x^{(2)},\\cdots,x^{(n)}|y=\\text{label})$$ 实际计算过程中，如果以贝叶斯规则直接以条件概率 $P(X=x|y=\\text{label})$计算分布的参数会存在较大的计算量——$\\text{label}$ 存在 $k$ 个可能性，同时 $x^{(j)}$ 的可能取值为$S_j$，那么计算的参数个数为 $k\\displaystyle{\\prod_{j=1}^n}S_j$ 。因此在贝叶斯计算基础上提出了一个强假设条件——各个条件之间独立（即明确的 $\\text{label}$ 中各个变量$x^{(j)}$ 的条件概率 $P(x^{(j)}|y=\\text{label}$ 没有相关性），这也是朴素贝叶斯的由来。根据以上的假设，需要解决的问题转变为: $$\\begin{align}P(X=x|y=\\text{label})&amp;=P(x^{(1)},\\cdots,x^{(n)}|y=\\text{label}) \\&amp;=\\prod_{j=1}^nP(x^{(j)}|y=\\text{label})\\end{align}$$ 最终需要计算的是后验概率，其数学表达式是: $$\\begin{align}P(y=\\text{label}|X=x)&amp;=\\frac{P(y, X)}{P(X)} \\&amp; =\\frac{P(X=x|y=\\text{label})P(y=\\text{label})}{\\displaystyle{\\sum_k P(X=x|y=\\text{label})P(y=\\text{label})}} \\&amp;=\\frac{P(y=\\text{label})\\prod_{j=1}^nP(x^{(j)}|y=\\text{label})}{\\displaystyle{\\sum_k P(y=\\text{label})\\prod_{j=1}^nP(x^{(j)}|y=\\text{label})}}\\end{align}$$ 演化为需要将在 $\\text{label}=[\\text{label}_1,\\cdots, \\text{label}_k]$ 分类中，确认最大化 $P(y=\\text{label}|X=x)$ 概率，而且分母作为其中的 normalization 项，所以目标转换为之需要最大化分子项 1.1 朴素贝叶斯损失函数朴素贝叶斯解决的是分类问题，针对二分类问题构造出的损失函数为:$$L(y, f(X))=\\begin{cases}1, y \\ne f(X) \\0, y = f(X)\\end{cases}$$ 期望风险 $R_{exp}(f)=E{\\displaystyle{\\sum_{k=1}^K}(1-y)P(y=\\text{label}|X)}$ 最小化，即是最大化后验概率。 1.2 朴素贝叶斯算法输入： 训练数据集 $T={(x_1, y_1),\\cdots,(x_n, y_n) }$ 其中第 $i$ 个样本点 $x_i=(x_i^{(1)}, \\cdots,x_i^{(j)})$ 中 $j$ 表示第 $j$ 个特征 $x_i^{(j)} \\in {a_{j1},\\cdots, a_{js}}$ ，表示第 $i$ 个样本的 $j$ 维特征，可以以取得的值集合 $y_i\\in {c_1, \\cdots, c_k }$ 表示第 $i$ 个数据点上可以取得的标签维度 训练过程： 计算先验概率和在每个特征不同取值条件概率 计算每个标签的先验概率，$P(y=\\text{label})=\\frac{\\displaystyle{\\sum_{i=1}^N}I(y_i=c_k)}{N}, \\text{label}={1,\\cdots,k}$ 需要分别计算在标签为 \\text{label} 的条件下，每个特征可能取值的条件概率，$P(X^{j}=a_{js}|\\text{label})=\\frac{\\displaystyle{\\sum_{i=1}^N I(x_i^{j}=a_{js}, y_i=\\text{label})}}{\\dislpalystle{\\sum_{i=1}^N I(y_i=\\text{label})}}$ 给定的实例 $x=(x^{(1)},\\cdots,x^{(n)})^\\rm T$ 计算 $P(y=\\text{label})\\displaystyle{\\prod_{j=1}^n}P(X^{(j)}=x^{(j)}|y=\\text{label})$ 计算实例的类 $y=\\displaystyle{\\text{argmax} P(y=\\text{label})\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|y=\\text{label})}$ 输出: 实例的分类 说明: 封面图 “File:K-nearest Neighbors.png” by Pkuwangyan06 is licensed under CC BY-SA 4.0 缩略图来自 Flickr janeqjames","link":"/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E5%9B%9B%E7%AB%A0%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"},{"title":"Curl 命令使用","text":"curl命令 是一个利用URL规则在命令行下工作的文件传输工具。它支持文件的上传和下载，所以是综合传输工具，但按传统，习惯称curl为下载工具。作为一款强力工具，curl支持包括HTTP、HTTPS、ftp等众多协议，还支持POST、cookies、认证、从指定偏移处下载部分文件、用户代理字符串、限速、文件大小、进度条等特征。做网页处理流程和数据检索自动化，curl可以助一臂之力。 语法1curl(选项)(参数) 选项 -a/--append上传文件时，附加到目标文件 -A/--user-agent 设置用户代理发送给服务器 -anyauth可以使用“任何”身份验证方法 -b/--cookie cookie字符串或文件读取位置 --basic使用HTTP基本验证 -B/--use-ascii使用ASCII /文本传输 -c/--cookie-jar 操作结束后把cookie写入到这个文件中 -C/--continue-at 断点续传 -d/--data HTTP POST方式传送数据 --data-ascii 以ascii的方式post数据 --data-binary 以二进制的方式post数据 --negotiate使用HTTP身份验证 --digest使用数字身份验证 --disable-eprt禁止使用EPRT或LPRT --disable-epsv禁止使用EPSV -D/--dump-header 把header信息写入到该文件中 --egd-file 为随机数据(SSL)设置EGD socket路径 --tcp-nodelay使用TCP_NODELAY选项 -e/--referer来源网址 -E/--cert cert:[passwd]客户端证书文件和密码 (SSL) --cert-type 证书文件类型 (DER/PEM/ENG) (SSL) --key 私钥文件名 (SSL) --key-type 私钥文件类型 (DER/PEM/ENG) (SSL) --pass 私钥密码 (SSL) --engine 加密引擎使用 (SSL). \"--engine list\" for list --cacert CA证书 (SSL) --capath CA目录 (made using c_rehash) to verify peer against (SSL) --ciphers SSL密码 --compressed要求返回是压缩的形势 (using deflate or gzip) --connect-timeout 设置最大请求时间 --create-dirs建立本地目录的目录层次结构 --crlf上传是把LF转变成CRLF -f/--fail连接失败时不显示http错误 --ftp-create-dirs如果远程目录不存在，创建远程目录 --ftp-method [multicwd/nocwd/singlecwd]控制CWD的使用 --ftp-pasv使用 PASV/EPSV 代替端口 --ftp-skip-pasv-ip使用PASV的时候,忽略该IP地址 --ftp-ssl尝试用 SSL/TLS 来进行ftp数据传输 --ftp-ssl-reqd要求用 SSL/TLS 来进行ftp数据传输 -F/--form 模拟http表单提交数据 --form-string 模拟http表单提交数据 -g/--globoff禁用网址序列和范围使用{}和[] -G/--get以get的方式来发送数据 -H/--header 自定义头信息传递给服务器 --ignore-content-length忽略的HTTP头信息的长度 -i/--include输出时包括protocol头信息 -I/--head只显示请求头信息 -j/--junk-session-cookies读取文件进忽略session cookie --interface 使用指定网络接口/地址 --krb4 使用指定安全级别的krb4 -k/--insecure允许不使用证书到SSL站点 -K/--config指定的配置文件读取 -l/--list-only列出ftp目录下的文件名称 --limit-rate 设置传输速度 --local-port强制使用本地端口号 -m/--max-time 设置最大传输时间 --max-redirs 设置最大读取的目录数 --max-filesize 设置最大下载的文件总量 -M/--manual显示全手动 -n/--netrc从netrc文件中读取用户名和密码 --netrc-optional使用 .netrc 或者 URL来覆盖-n --ntlm使用 HTTP NTLM 身份验证 -N/--no-buffer禁用缓冲输出 -o/--output把输出写到该文件中 -O/--remote-name把输出写到该文件中，保留远程文件的文件名 -p/--proxytunnel使用HTTP代理 --proxy-anyauth选择任一代理身份验证方法 --proxy-basic在代理上使用基本身份验证 --proxy-digest在代理上使用数字身份验证 --proxy-ntlm在代理上使用ntlm身份验证 -P/--ftp-port 使用端口地址，而不是使用PASV -q作为第一个参数，关闭 .curlrc -Q/--quote 文件传输前，发送命令到服务器 -r/--range 检索来自HTTP/1.1或FTP服务器字节范围 --range-file读取（SSL）的随机文件 -R/--remote-time在本地生成文件时，保留远程文件时间 --retry 传输出现问题时，重试的次数 --retry-delay 传输出现问题时，设置重试间隔时间 --retry-max-time 传输出现问题时，设置最大重试时间 -s/--silent静默模式。不输出任何东西 -S/--show-error显示错误 --socks4 host[:port]用socks4代理给定主机和端口 --socks5 host[:port]用socks5代理给定主机和端口 --stderr -t/--telnet-option Telnet选项设置 --trace 对指定文件进行debug --trace-ascii Like --跟踪但没有hex输出 --trace-time跟踪/详细输出时，添加时间戳 -T/--upload-file 上传文件 --url Spet URL to work with -u/--user user[:password]设置服务器的用户和密码 -U/--proxy-user user[:password]设置代理用户名和密码 -w/--write-out [format]什么输出完成后 -x/--proxy host[:port]在给定的端口上使用HTTP代理 -X/--request 指定什么命令 -y/--speed-time放弃限速所要的时间，默认为30 -Y/--speed-limit停止传输速度的限制，速度时间 实例文件下载 curl命令可以用来执行下载、发送各种HTTP请求，指定HTTP头部等操作。如果系统没有curl可以使用yum install curl安装，也可以下载安装。curl是将下载文件输出到stdout，将进度信息输出到stderr，不显示进度信息使用--silent选项。 1curl URL --silent 这条命令是将下载文件输出到终端，所有下载的数据都被写入到stdout。 使用选项-O将下载的数据写入到文件，必须使用文件的绝对地址： 1curl http://example.com/text.iso --silent -O 选项-o将下载数据写入到指定名称的文件中，并使用--progress显示进度条： 12curl http://example.com/test.iso -o filename.iso --progress######################################### 100.0% 不输出错误和进度信息 -s 参数将不输出错误和进度信息。 12curl -s https://www.example.com# 上面命令一旦发生错误，不会显示错误信息。不发生错误的话，会正常显示运行结果。 如果想让 curl 不产生任何输出，可以使用下面的命令。 1curl -s -o /dev/null https://google.com 断点续传 curl能够从特定的文件偏移处继续下载，它可以通过指定一个便宜量来下载部分文件： 1234curl URL/File -C 偏移量#偏移量是以字节为单位的整数，如果让curl自动推断出正确的续传位置使用-C -：curl -C -URL 使用curl设置参照页字符串 参照页是位于HTTP头部中的一个字符串，用来表示用户是从哪个页面到达当前页面的，如果用户点击网页A中的某个连接，那么用户就会跳转到B网页，网页B头部的参照页字符串就包含网页A的URL。 使用--referer选项指定参照页字符串： 1curl --referer http://www.google.com http://wangchujiang.com 用curl设置用户代理字符串 有些网站访问会提示只能使用IE浏览器来访问，这是因为这些网站设置了检查用户代理，可以使用curl把用户代理设置为IE，这样就可以访问了。使用--user-agent或者-A选项： 12curl URL --user-agent &quot;Mozilla/5.0&quot;curl URL -A &quot;Mozilla/5.0&quot; 其他HTTP头部信息也可以使用curl来发送，使用-H“头部信息” 传递多个头部信息，例如： 1curl -H &quot;Host:wangchujiang.com&quot; -H &quot;accept-language:zh-cn&quot; URL curl的带宽控制和下载配额 使用--limit-rate限制curl的下载速度： 1curl URL --limit-rate 50k 命令中用k（千字节）和m（兆字节）指定下载速度限制。 使用--max-filesize指定可下载的最大文件大小： 1curl URL --max-filesize bytes 如果文件大小超出限制，命令则返回一个非0退出码，如果命令正常则返回0。 12curl --limit-rate 200k https://example.com# 上面命令将带宽限制在每秒 200K 字节。 用curl进行认证 使用curl选项 -u 可以完成HTTP或者FTP的认证，可以指定密码，也可以不指定密码在后续操作中输入密码： 12curl -u user:pwd http://wangchujiang.comcurl -u user http://wangchujiang.com 只打印响应头部信息 通过-I或者-head可以只打印出HTTP头部信息： 12345678[root@localhost text]# curl -I http://wangchujiang.comHTTP/1.1 200 OKServer: nginx/1.2.5date: Mon, 10 Dec 2012 09:24:34 GMTContent-Type: text/html; charset=UTF-8Connection: keep-aliveVary: Accept-EncodingX-Pingback: http://wangchujiang.com/xmlrpc.php get请求 1234curl &quot;http://www.wangchujiang.com&quot; # 如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地curl -i &quot;http://www.wangchujiang.com&quot; # 显示全部信息curl -l &quot;http://www.wangchujiang.com&quot; # 只显示头部信息curl -v &quot;http://www.wangchujiang.com&quot; # 显示get请求全过程解析 post请求 12345$ curl -d &quot;param1=value1&amp;param2=value2&quot; &quot;http://www.wangchujiang.com/login&quot;curl -d'login=emma＆password=123' -X POST https://wangchujiang.com/login# 或者$ curl -d 'login=emma' -d 'password=123' -X POST https://wangchujiang.com/login --data-urlencode 参数等同于 -d，发送 POST 请求的数据体，区别在于会自动将发送的数据进行 URL 编码。 12curl --data-urlencode 'comment=hello world' https://wangchujiang.com/login# 上面代码中，发送的数据hello world之间有一个空格，需要进行 URL 编码。 读取本地文本文件的数据，向服务器发送 12curl -d '@data.txt' https://wangchujiang.com/upload# 读取data.txt文件的内容，作为数据体向服务器发送。 json格式的post请求 1curl -l -H &quot;Content-type: application/json&quot; -X POST -d '{&quot;phone&quot;:&quot;13521389587&quot;,&quot;password&quot;:&quot;test&quot;}' http://wangchujiang.com/apis/users.json 向服务器发送 Cookie 使用--cookie &quot;COKKIES&quot;选项来指定cookie，多个cookie使用分号分隔： 1curl http://wangchujiang.com --cookie &quot;user=root;pass=123456&quot; 将cookie另存为一个文件，使用--cookie-jar选项： 1curl URL --cookie-jar cookie_file -b 参数用来向服务器发送 Cookie。 12curl -b 'foo=bar' https://taobao.com# 上面命令会生成一个标头Cookie: foo=bar，向服务器发送一个名为foo、值为bar的 Cookie。 123456curl -b 'foo1=bar' -b 'foo2=baz' https://taobao.com# 上面命令发送两个 Cookie。​```shellcurl -b cookies.txt https://www.taobao.com# 上面命令读取本地文件 cookies.txt，里面是服务器设置的 Cookie（参见-c参数），将其发送到服务器。 Cookie 写入一个文件 12curl -c cookies.txt https://www.taobao.com# 上面命令将服务器的 HTTP 回应所设置 Cookie 写入文本文件cookies.txt。 请求的来源 -e 参数用来设置 HTTP 的标头 Referer，表示请求的来源。 12curl -e 'https://taobao.com?q=example' https://www.example.com# 上面命令将Referer标头设为 https://taobao.com?q=example。 -H 参数可以通过直接添加标头 Referer，达到同样效果。 1curl -H 'Referer: https://taobao.com?q=example' https://www.example.com 上传二进制文件 -F 参数用来向服务器上传二进制文件。 12curl -F 'file=@photo.png' https://taobao.com/profile# 上面命令会给 HTTP 请求加上标头 Content-Type: multipart/form-data ，然后将文件photo.png作为file字段上传。 -F 参数可以指定 MIME 类型。 12curl -F 'file=@photo.png;type=image/png' https://taobao.com/profile# 上面命令指定 MIME 类型为image/png，否则 curl 会把 MIME 类型设为 application/octet-stream。 -F 参数也可以指定文件名。 12curl -F 'file=@photo.png;filename=me.png' https://taobao.com/profile# 上面命令中，原始文件名为photo.png，但是服务器接收到的文件名为me.png。 设置请求头 -H 参数添加 HTTP 请求的标头。 12curl -H 'Accept-Language: en-US' https://google.com# 上面命令添加 HTTP 标头 Accept-Language: en-US。 12curl -H 'Accept-Language: en-US' -H 'Secret-Message: xyzzy' https://google.com# 上面命令添加两个 HTTP 标头。 12curl -d '{&quot;login&quot;: &quot;emma&quot;, &quot;pass&quot;: &quot;123&quot;}' -H 'Content-Type: application/json' https://google.com/login# 上面命令添加 HTTP 请求的标头是 Content-Type: application/json，然后用 -d 参数发送 JSON 数据。 跳过 SSL 检测 12curl -k https://www.example.com# 上面命令不会检查服务器的 SSL 证书是否正确。 请求跟随服务器的重定向 -L 参数会让 HTTP 请求跟随服务器的重定向。curl 默认不跟随重定向。 1curl -L -d 'tweet=hi' https://api.example.com/tweet 调试参数 -v 参数输出通信的整个过程，用于调试。 12curl -v https://www.example.com# --trace参数也可以用于调试，还会输出原始的二进制数据。 1$ curl --trace - https://www.example.com 获取本机外网ip 1curl ipecho.net/plain 说明: 内容摘自 Linux命令大全搜索工具","link":"/%5Bobject%20Object%5D/Curl%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/"},{"title":"数据分析之留存分析报告搭建","text":"对用户进行分析过程中，留存是一个重要的指标，它体现了新用户/会员在经过一定时间之后，仍然具有访问、登陆、使用或者转化等属性或行为。用户的留存才有可能产生收入，同时作为监控产品的指标也是需要进行周期性分析。 1. 留存率指标留存率的分析需要依赖于分析的周期，因此在常用的周期上主要分为了三类：日留存、周留存以及月留存。 1.1 日留存日留存在具体使用上会根据不同的业务需要使用不同的时间间隔，因此在详细级别上分为： 次日留存率，第一天新增用户中，在第二天继续访问用户除以第一天新增用户数 第三日留存率，第一天新增用户中，在第三天继续访问用户除以第一天新增用户数 第七日留存率，第一天新增用户中，在第七天继续访问用户除以第一天新增用户数 第十四日留存率，第一天新增用户中，在第十四天继续访问用户除以第一天新增用户数 第三十日留存率，第一天新增用户中，在第三十天继续访问用户除以第一天新增用户数 1.2 周留存周留存统计，也依赖于分析的分析的时间间隔，详细级别上分为： 一周后留存率，第一周新增用户，在第二周继续访问用户除以第一周新增用户数 两周后留存率，第一周新增用户，在第三周继续访问用户除以第一周新增用户数 三周后留存率，第一周新增用户，在第四周继续访问用户除以第一周新增用户数 1.3 月留存月留存，分析当前月份相对于统计月份上人数的比例。针对于月留存率分析，使用矩阵式表现是一种比较不错的方式： 2. 应用分析三类留存的分析应用上，日留存分析短期效果，周留存体现中期效果，月留存用于分析长期效果。留存分析需要特别注意衰减比率，一般情况下留存会随着时间变化而递减，可能表现出线性、指数递减。通过递减数据搭建衰减模型可以用于异常检测，例如发现衰减异常发生的时间。 3. 应用实践——月留存报告3.1 背景短视频产品新上线，需要了解用户在新产品上留存率的情况。 3.2 数据数据使用用户播放记录数据。目前业务上短视频是一个独立的栏目专区，在流程上没有独立的登陆验证机制，因此和运营人员沟通确认之后采取了以最低播放时长作为判断该用户是否有使用短视频产品。 判断依据：播放记录的播放时长达到 5s 以上 日期类型解析：需要对表中播放开始、结束时间（begin_time, end_time）以及创建时间（create_time）解析为时间戳 起始时间戳缺失，确认是初期上线之后出现 bug。排除缺失数据之前的数据 时间解析之后，起始时间戳和结束时间戳存在错误，沟通确认该问题是前端数据收集时出现的问题，对解析播放时长(结束时间减去起始时间没有影响)。对于用户的交互时间，可以直接以 create_time 作为参考 3.3 数据清理因此整个数据清理的内容包括： 起始时间缺失的情况下，删除最大 create_time 以前的数据 以时间起讫建立用户观看时长 以 create_time 字段建立用户交互日期信息 删除用户播放时长低于 5s 的记录 数据处理的代码如下： 12345678910111213# 筛选出缺失的起始时间戳下的最大时间time_threshold = data.loc[data.begin_time.isnull(), &quot;create_time&quot;].max()data = data.loc[data.create_time &gt; time_threshold, :].reset_index(drop=True)# 时间处理data['duration']= data.apply(lambda x: (x['end_time'] - x['begin_time']).seconds, axis=1).astype(&quot;int32&quot;)data['date'] = pd.to_datetime(data.create_time.dt.date)# 转换日期数据为月份data[&quot;month&quot;] = data.date.apply(lambda x: x.strftime(&quot;%Y-%m&quot;))# 删除不符合要求的播放时长记录data = data.loc[data.duration &gt;= 5].reset_index(drop=True) 3.3 月留存分析报告分析各个月份的留存情况，需要拆分出各个月份的数据。拆分计算之后在聚合报告 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def date_pair(dates): &quot;&quot;&quot;日期配对生成器 将日期序列进行配对生成当前日期及其之后的日期配对信息，同时保留最大长度的日期长度，不能配 对的信息以 None 补齐 Args: ------- dates: list，日期序列数据 Examples: ------- &gt;&gt;&gt; dates = ['2020-12', '2021-01', '2021-02'] &gt;&gt;&gt; for date1, date2 in date_pair(dates): &gt;&gt;&gt; print(date1, date2) 2020-12 2020-12 2020-12 2021-01 2020-12 2021-02 2021-01 2021-01 2021-01 2021-02 None None 2021-02 2021-02 None None None None &quot;&quot;&quot; dates = sorted(dates) for index in range(len(dates)): for elem in dates[index:]: yield dates[index], elem # 优选补齐当前日期下的信息 for _ in range(index): yield None, Nonedef extract_monthly_rention(data, dates, groups, padding=&quot;-%&quot;, full=&quot;100%&quot;, ucol=&quot;user_id&quot;): &quot;&quot;&quot;计算月度用户留存率 计算各月用户的留存率 Args: ------ data: pd.DataFrame, 用户数据，包括发生交互的时间戳信息 dates: list, 需要统计的时间列表，各个元素为需要统计的时间信息 groups: dict，对需要统计的时间信息进行 GROUP BY 之后，生成的字典信息 padding: str，用于填充需要补齐的时间留存率，默认为 '-%' full: str, 用于填充当前月份下的留存率，默认为 '100%' ucol: str, 表示的是 data 数据中的用户字段名称 &quot;&quot;&quot; result = [] login_user = set() # 登陆用户 date = None element = [] for index, (current, query) in enumerate(date_pair(dates)): # 更新在新日期下的新用户，以及合并已登陆用户 if date is None or (current is not None and date != current): new_user = set(data.loc[groups[current], ucol].unique()) - login_user login_user |= set(data.loc[groups[current], ucol].unique()) date = current # 如果 element 有元素时，更新结果数据 if element: result.append(element) element = [] # 如果当前日期和查询日期相同，是当前月份留存 if query is not None and current == query: element.append(full) # 如果是补齐的日期对信息，直接填充数据值 elif query is None: element.append(padding) # 其他情况时，直接查询数据计算留存 else: active = new_user &amp; set(data.loc[groups[query], ucol].unique()) element.append(&quot;{0:.2f}%&quot;.format(len(active) / len(new_user) * 100)) else: result.append(element) # 生成报告 columns = [f&quot;留存率_{i}&quot; for i in range(len(dates))] report = pd.DataFrame(data=result, columns=columns, index=dates) return report# 处理需要使用到的参数padding = &quot;-%&quot; # 填充缺失的数据full = &quot;100%&quot; # 首月留存ucol = &quot;user_id&quot; # 用户字段名称dates = data.month.unique().tolist()groups = data.groupby(&quot;month&quot;).groups# 运行结果extract_monthly_rention(data, dates, groups)","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%95%99%E5%AD%98%E5%88%86%E6%9E%90%E6%90%AD%E5%BB%BA/"},{"title":"Flask 应用之搭建 API 服务的笔记","text":"Flask 是基于 Werkzeug 和 Jinja2 的 Python 轻量级 Web 应用框架，适用于轻量级的服务，例如: 简单网页开发以及搭建 RESTful API 服务。主要整理了搭建 API 过程中遇到的一些问题，以及相关流程中的代码。 通用 API 请求直接通过链接不带任何请求数据以及查询参数的请求，这类请求下可以直接搭建相关相应的网页返回数据即可： example.py >unfolded12345678910111213141516171819202122232425262728import flaskfrom flask import request, jsonifyapp = flask.Flask(__name__)app.config[&quot;DEBUG&quot;] = True# Create some test data for our catalog in the form of a list of dictionaries.books = [ {'id': 0, 'title': 'A Fire Upon the Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'}, {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'}, {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}]@app.route('/api/v1/resources/books/all', methods=['GET'])def api_all(): return jsonify(books) 上面提供的 API 提供了 GET 方法，直接通过 http://&lt;host&gt;/api/v1/resources/books/all 链接可以得到所有数据 JSON 结果。 带参数查询的 API 请求带参数的请求包括通过在链接中添加查询参数访问或者通过表单方式请求，前者的参数解析需要通过 request 的 args 属性得到相关的，查询的单一参数在链接中一般是以 ?&lt;key&gt;=&lt;value&gt; 形式跟在链接后。如果需要查询多个参数时，是以 ?&lt;key1&gt;=&lt;value1&gt;&amp;&lt;key2&gt;=&lt;value2&gt; 形式；后者的参数解析是通过解析表单数据，键值是以请求中 body 参数传入。下面的示例是需要带访问参数的请求查询: example1.py12345678910111213141516171819202122232425# 省略了部分初始化 app 代码from flask import request, jsonify@app.route('/api/v1/resources/books', methods=['GET'])def api_id(): # Check if an ID was provided as part of the URL. # If ID is provided, assign it to a variable. # If no ID is provided, display an error in the browser. if 'id' in request.args: id = int(request.args['id']) else: return &quot;Error: No id field provided. Please specify an id.&quot; # Create an empty list for our results results = [] # Loop through the data and match results that fit the requested ID. # IDs are unique, but other fields might return many results for book in books: if book['id'] == id: results.append(book) # Use the jsonify function from Flask to convert our list of # Python dictionaries to the JSON format. return jsonify(results) 以上的 API 可以通过链接 http://&lt;host&gt;/api/v1/resources/books?id=1 查询 id 为 1 的书籍。 定制化响应异常在 API 服务中异常处理包括了本身代码异常处理，以及提供服务的异常处理。服务的异常包括请求格式不符合要求、连接异常等，某些异常有专门的响应代码表达响应的结果。为了规范响应，使用 errorhandler 可以通过定制化响应的响应结果： example3.py >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 省略了部分初始化 app 代码from flask import request, jsonifyimport sqlite3def dict_factory(cursor, row): d = {} for idx, col in enumerate(cursor.description): d[col[0]] = row[idx] return d@app.route('/', methods=['GET'])def home(): return '''&lt;h1&gt;Distant Reading Archive&lt;/h1&gt;&lt;p&gt;A prototype API for distant reading of science fiction novels.&lt;/p&gt;'''# 404 异常@app.errorhandler(404)def page_not_found(e): return &quot;&lt;h1&gt;404&lt;/h1&gt;&lt;p&gt;The resource could not be found.&lt;/p&gt;&quot;, 404@app.route('/api/v1/resources/books', methods=['GET'])def api_filter(): query_parameters = request.args id = query_parameters.get('id') published = query_parameters.get('published') author = query_parameters.get('author') query = &quot;SELECT * FROM books WHERE&quot; to_filter = [] if id: query += ' id=? AND' to_filter.append(id) if published: query += ' published=? AND' to_filter.append(published) if author: query += ' author=? AND' to_filter.append(author) if not (id or published or author): # 未找到信息时，返回指定的 404 信息 return page_not_found(404) query = query[:-4] + ';' conn = sqlite3.connect('books.db') conn.row_factory = dict_factory cur = conn.cursor() results = cur.execute(query, to_filter).fetchall() return jsonify(results) 上面代码定制化处理了 404 响应结果 开发与测试 SQLAlchemySQLAlchemy 是作为 Flask 扩展工具，其主要主要作用是处理数据库数据。在开发或者测试阶段进行交互环境下操作时，需要调用应用上下文的 push 方法才能进行进行后续的数据操作: app.py >unfolded123456789from flask import Flaskfrom flask_sqlalchemy import SQLAlchemydb = SQLAlchemy()def create_app(): app = Flask(__name__) db.init_app(app) return app 上面的代码，包括了初始化一个应用实例以及初始化应用数据库（通过 db.init_app 方法可以初始化应用的需要的数据库）。在交互时需要以下面的方式使用 push 方法: >unfolded123&gt;&gt;&gt; from app import create_app&gt;&gt;&gt; app = create_app()&gt;&gt;&gt; app.app_context().push() 外链图片显示Flask 中直接使用 URL 的外链，可能会存在无法展示的问题。解决这个问题，需要将链接转换为具体的数据值，例如将数据直接数据读取保存为 base64 编码数据即可以解决： 需要将数据转换为 base64 编码数据 example3.py >unfolded1234567 # 代码部分省略了数据保存部分import requestsimport base64# 如果是需要保存在数据库中，可以不需要使用解码，即保留字节型数据（例如 MySQL 中保存为 BLOB 数据，保存的数据格式应该是子节型数据，不应该使用解码）url = “https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2196652087.jpg”content = base64.b64encode(requests.get(url).content) 在渲染阶段需要将图片的值解码后，赋值给 src 属性。下面的代码包括了两个部分，一个是从数据库查询数据并解码，另一个是利用 Jinja2 的模版格式化图片 app.py >unfolded12345678# 代码部分省略了 app 初始化等过程@app.route(&quot;/hello&quot;)def index(): series = Series.query.limit(10) for item in series: item.cover = item.cover.decode(&quot;ascii&quot;) return render_template(&quot;index.html&quot;, series=series) 模版格式化图片 index.html >unfolded12# 代码部分省略了页面其他部分 参考 Creating Web APIs with Python and Flask Flask 文档","link":"/%5Bobject%20Object%5D/Python-Flask%E5%BA%94%E7%94%A8%E4%B9%8B%E6%90%AD%E5%BB%BAAPI%E6%9C%8D%E5%8A%A1%E7%9A%84%E7%AE%80%E5%8D%95%E7%AC%94%E8%AE%B0/"},{"title":"PySpark 处理数据异常笔记","text":"整理的使用 PySpark 处理数据过程中遇到的一些异常问题，以及可能的解决方案。 1. 数据类型相关异常1.1 数据计算过程中 PickleException出现该问题是在计算相似度过程中出现相关的问题，具体情况是直接使用 PySpark 下的 DenseVector 计算余弦相似度，没有将结果转换为返回要求的 float 的情况下，会出现 net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype) 这样的异常信息。 解决方案：出现是该问题是在结算结果之后的数据类型不符合 API 设计，因此思路上检查数据的数据类型，尝试使用原子数据类型，例如将最终计算结果保存为 float 类型数据。","link":"/%5Bobject%20Object%5D/PySpark-PySpark%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/"},{"title":"PySpark 搭建ALS协同过滤模型","text":"协同过滤是推荐系统中常用的一种机制，利用用户对于内容的评分搭建交叉的用户偏好以推荐相关内容。其中评分具有多样性，需要根据具体的业务和数据来确认可用的评分信息。这种评分信息可能是用户直接**评分，也可能是间接*从其他数据中提取。在本次落实该项目，基于仅有的少量数据搭建一种推荐机制。 协同过滤在 IPTV 下应用包括使用矩阵分解方法解析用户-内容矩阵，一种方式显示的用户对内容的评分，第二种方式隐式的方式分析用户的行为动作强度来分析(eg: 利用用户点击次数或者用户观看电影的累积时长)。 在应用中通过显性反馈数据信息进行推荐是一种广泛且直观的方法，因为很容易对显性反馈数据进行解析。非显性的反馈数据信息是对缺少显式反馈数据搭建推荐的良好补充。TV 端推荐引擎反馈数据特点： 用户非负反馈，用户的非负行为反馈较难直接推断用户的不喜欢行为，而且在推荐应用上因为数据分析处理过程中的“缺失化处理”（对用户没有交互的内容采取缺失处理）。因此关注于这类缺失数据中负反馈是有重要意义的 显式反馈的天然噪声，显示的反馈很容易被关联上用户偏好和真实动机，但是这种方式存在潜在的问题——例如用户购买后，对产品的态度并不存在天然关联。而在 TV 上用户长时间观看记录，并不一定真实在观看内容 显性反馈数值指示偏好信息，隐形反馈反馈数值指示置信度。单次事件发送可能是多种原因，但对于重复发生事件能够在一定层度上反映用户“意见” 隐式反馈推荐评估方法的转变，传统的显式反馈可以通过数值直接反映预测效果，但隐式模型推荐需要考虑到待推荐内容的可用性和其他内容的竞争 模型搭建本次的应用过程中基于 ALS 算法进行矩阵分解的方式搭建协同过滤推荐系统，落实过程和一般的模型过程一致，而重点在于搭建模型前的评分确认。经过业务沟通、现有数据采集方案和其他内容参考敲定了相关的评分计算模式：$评分= 时长权重观看时长得分 + 次数权重观看次数得分$。 评分计算计算用户对内容评分过程包括了两个部分，观看时长和观看次数两个部分。两者的得分上分别是单一影片的统计数据占所有影片的内容统计数据，最终的详细计算方案为： $评分=时长权重 * \\frac{该影片观看时长}{该用户总观看时长}+次数权重*\\frac{该影片观看次数}{该用户总观看次数}$。 >unfold1234567891011121314151617181920# 评分计算方法一：根据用户和内容的交互数据统计出播放时长和点播次数user2item = vod.groupby(&quot;itv_account&quot;, &quot;节目名称&quot;) \\ .agg(F.sum(F.col(&quot;duration&quot;)).alias(&quot;span&quot;), F.count(F.column(&quot;duration&quot;)).alias(&quot;frequency&quot;))# 评分计算方法二：下面注释的方法是使用 collect_list 去将时间聚合# user2item = vod.groupby(&quot;itv_account&quot;, &quot;节目名称&quot;) \\# .agg(F.collect_list(F.col(&quot;duration&quot;)).alias(&quot;durations&quot;))# 根据用户数据，统计出各用户交互信息users = vod.groupby(&quot;itv_account&quot;) \\ .agg(F.sum(F.col(&quot;duration&quot;)).alias(&quot;span_&quot;), F.count(F.column(&quot;duration&quot;)).alias(&quot;frequency_&quot;))# 计算评分，这里权重scores = user2item.join(users, on=&quot;itv_account&quot;) \\ .select(&quot;itv_account&quot;, &quot;节目名称&quot;, (F.col(&quot;span&quot;) / F.col(&quot;span_&quot;) + F.col(&quot;frequency&quot;) / F.col(&quot;frequency_&quot;)).alias(&quot;rate&quot;))# 构建评分矩阵——因 IPTV 用户观看内容存在重复观看，所以评分统计最后采取累加的方案data = scores.groupby(&quot;itv_account&quot;).pivot(&quot;节目名称&quot;).sum(&quot;rate&quot;) 模型训练数据拆分同其他监督学习模型一样，需要拆分为训练数据和测试数据。 >unfold1234567891011from pyspark.ml import evaluation, recommendationfrom pyspark.ml import feature, pipeline# 为方便选择相关字段，在后续处理之前先将特征进行处理，包括名称userStringIndexer = feature.StringIndexer(inputCol=&quot;itv_account&quot; , outputCol=&quot;uid&quot;)contentStringIndexer = feature.StringIndexer(inputCol=&quot;节目名称&quot; , outputCol=&quot;cid&quot;)pipe = pipeline.Pipeline(stages=[userStringIndexer, contentStringIndexer])transformed_scores = pipe.fit(scores).transform(scores)# 拆分数据集(train, test) = transformed_scores.randomSplit([.8, .2], 42) 模型训练使用 recommendation 模块中的 ALS 实现模型搭建，对于冷启动策略使用 drop 方案： >unfold1234567# 搭建模型超参数als = recommendation.ALS( rank=20, maxIter=10, userCol=&quot;uid&quot;, itemCol=&quot;cid&quot;, ratingCol=&quot;rate&quot;, numUserBlocks=4, numItemBlocks=4, alpha=1., implicitPrefs=False, coldStartStrategy=&quot;drop&quot;)model = als.fit(train) 模型评估因该模型解决的是回归问题，搭建过程中使用了 RMSE 来验证模型： >unfold123456789# 利用测试数据验证结果predictions = model.transform(test)# 评估器rmse_evaluator = evaluation.RegressionEvaluator(predictionCol='prediction', labelCol=&quot;rate&quot;)print(&quot;测试数据集结果 RMSE: %s&quot; % rmse_evaluator.evaluate(predictions))# 显示结果predictions.head(3) 优化与部署部署方面，该模型可以得到对于基于用户历史行为信息的协同过滤模型，结果可以得到不同用户的推荐内容，以此作为推荐 candidates。上述基于 ALS 算法的方式得到了一个模型，但并没有进行详细的优化——可以调整rank，numIterations，lambda，alpha 这些参数，与基准结果比较判断结果。所以优化方面，对参数优化达到结果较优模型和推荐解雇。另一方面该模型是矩阵分解的算法，其副产品是用户和内容的矩阵。可以在后续的模型中应用用户矩阵作为用户特征。","link":"/%5Bobject%20Object%5D/PySpark-PySpark%E6%90%AD%E5%BB%BAALS%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%A8%A1%E5%9E%8B/"},{"title":"Numba 包使用简要总结","text":"Numba 是一个开源 JIT 编译工具，功能是将 Python 和 Numpy 代码快速转换为机器码。通过llvmlite Python包，使用LLVM将一部分Python和NumPy (需要注意 Pandas 不能被 Numba 理解，直接对 Pandas 处理导致的结果是计算成本会增加) 转换为快速机器代码。它提供了一系列选项，用于并行化 CPU 和 GPU 的 Python 代码，通常只需进行少量代码更改。 Numpa 的装饰器类型除了 @jit 外还有其他类型的装饰器： @njit 这是@jit（nopython = True）的别名，可用于替换 @vectorize 生成NumPy ufunc（支持所有ufunc方法 Universal functions(ufunc)）。文件在这里。Numba 能将 Python 函数转换为 ufunc，这样可以实现 Numpy array 的 C 语言方式执行 @guvectorize 生成 NumPy广义ufunc。和 @vectorize 差异在于能够处理任意数量的数组 @stencil 将函数声明为类似模板的操作的内核。是一种通用的计算模式，以每个元素均可以被固定的模式更新数据 @jitclass 用于处理类的编译 @cfunc 申明调用本地的 C/C++ 库。它的使用和 @jit 相似，但是需要强制性给定 signature（可能是需要强制说明数据类型？！需要确认) @overload-注册自己的函数实现以在nopython模式下使用，例如@overload（scipy.special.j0） @jit 装饰器@jit 编译有两种方式：Lazy Compilation 和 Eager Compilation。前者的编译，是在首次执行时才会编译；后者的使用是通过函数 signature 来实现，如果是需要进行详细的精度控制这是一种良好的方式——特别需要注意在某些情况下是否申明了返回值类型会得到不同的结构： 123456789101112131415161718192021222324# Eager 编译，没有申明返回值类型In [2]: @nb.jit((nb.int32, nb.int32)) ...: def f(x, y): ...: return x + y ...:In [3]: f(1, 2)Out[3]: 3In [4]: f(2 **31, 2**31+1)Out[4]: -4294967295# Eager 编译，申明了返回值类型，超过精度之后会被抛弃In [5]: @nb.jit(nb.int32(nb.int32, nb.int32)) ...: def f(x, y): ...: return x + y ...:In [6]: f(2**31, 2**31+1)Out[6]: 1# 直接用 Python 进行计算In [7]: 2**31 + 2**32+1Out[7]: 6442450945 行内调用其他函数需要调用其他编译的函数时，建议在调用的函数/类中使用 Numba 编译，否则可能会导致结果很慢： 12345678@jitdef square(x): return x ** 2@jitdef hypot(x, y): # 调用了编译的 square 函数 return math.sqrt(square(x) + square(y)) nopython 参数Numpa 的使用一般是通过装饰器的方式，主要有两种模式：nopython 模式和 object 模式。 12345678910111213from numba import jitimport numpy as npx = np.arange(100).reshape(10, 10)@jit(nopython=True) # Set &quot;nopython&quot; mode for best performance, equivalent to @njitdef go_fast(a): # Function is compiled to machine code when called the first time trace = 0.0 for i in range(a.shape[0]): # Numba likes loops trace += np.tanh(a[i, i]) # Numba likes NumPy functions return a + trace # Numba likes NumPy broadcastingprint(go_fast(x)) nopython 的模式是进行完全编译，而不调用 Python 解释器的方式。这种模式是推荐的最佳实践方式。如果是没有显性的说明 nopython 那么会变为 object 模式，这种模式下是会检查代码中哪些部分可以编译为机器码，其他不能编译的会使用 python 解释器来处理。例如: 123456789101112from numba import jitimport pandas as pdx = {'a': [1, 2, 3], 'b': [20, 30, 40]}@jitdef use_pandas(a): # Function will not benefit from Numba jit df = pd.DataFrame.from_dict(a) # Numba doesn't know about pd.DataFrame df += 1 # Numba doesn't understand what this is return df.cov() # or this!print(use_pandas(x)) 上面的代码中因为 Numba 对 pandas 处理效果不好，因此可以通过关闭 nopython 模式让可以通过 python 解释器执行的，通过 python 解释器执行。 此外需要注意，使用 numa 会需要将代码进行编译，这在首次执行时会消耗一定时间用于编译。因此如果是需要评估代码的时间消耗，可以在 IPython 中使用 %timeit 方式进行评估 nogil 参数nogil 参数是用于控制是否还需要维持使用 Python 的 GIL。释放 GIL 可以充分利用多核处理器计算以及多线程计算。如果是在对象模式下（即代码处理的数据全是以 Python 对象或者使用 Python C 的 API 处理这些对象的模式），也不可能释放 GIL。虽然释放了 GIL 带来了效率，但是注意会带来多线程编程中的隐患。 cache 参数cache 参数用于控制函数编译结果是否需要写入缓存。为了避免每次调用Python程序时都要进行编译，可以指示Numba将函数编译的结果写入基于文件的缓存中 ￼parallel 参数parallel = True 是用于允许自动化执行并行函数运算，fastmath = True 作用是可以进行不安全的浮点运算，相对来说准确率要低一些。 Signature 申明显性的 signature 类型，包括： void is the return type of functions returning nothing (which actually return None when called from Python) intp and uintp are pointer-sized integers (signed and unsigned, respectively) intc and uintc are equivalent to C int and unsigned int integer types int8, uint8, int16, uint16, int32, uint32, int64, uint64 are fixed-width integers of the corresponding bit width (signed and unsigned) float32 and float64 are single- and double-precision floating-point numbers, respectively complex64 and complex128 are single- and double-precision complex numbers, respectively array types can be specified by indexing any numeric type, e.g. float32[:] for a one-dimensional single-precision array or int8[:,:] for a two-dimensional array of 8-bit integers. @genrated_jit 装饰器该方式的编译是用于解决一个函数执行不同类型输入，实现保留 JIT 函数执行效率的同时，支持选择编译时(compile-time) 的不同类型选择。例如需要根据不同的值返回是否缺失的情况： 输入数据是浮点数类型的却是 NaN Numpy 的时间日期类型缺失 以及不满足缺失条件的数据 1234567891011121314151617import numpy as npfrom numba import genrate_jit, types@generated_jit(nopython=True)def is_missing(x): &quot;&quot;&quot; Return True if the value is missing, False otherwise. &quot;&quot;&quot; if isinstance(x, types.Float): return lambda x: np.isnan(x) elif isinstance(x, (types.NPDatetime, types.NPTimedelta)): # The corresponding Not-a-Time value missing = x('NaT') return lambda x: x == missing else: return lambda x: False 在使用该方式编译的过程中，需要注意： 装饰器函数需要调用的是 numba 的 types 参数 这类型的装饰器函数，并没有直接返回计算结果，而是返回的一个可调用对象 预先计算的数据在编译执行时，是可以被重用的 函数定义使用与修饰后的函数中的参数相同的名称，这是确保通过名称传递参数按预期工作所必需的 可使用参数该编译模式，使用参数包括 nopython 和 cache 选项 @vectorize 和 @guvectorize装饰器这两种装饰器分别处理的是两种类型的数据： 标量数据计算，这类型是使用 universal functions (或 ufuncs)，是直接使用 @vectorize 装饰器 高维度数据计算，这类型需要使用 generalized universal functions(或 gufuncs)，是使用 @guvectorize 装饰器 @vectorize 应用需要注意在文档中描述的标量数据计算，实际上是解决的是一维 array 数据： 123456# 主要可以看作是解决了类似下面的直接用 python 代码写出可以逐个处理 array 的元素import numpy as npimport mathdef test(a, b): return math.sin(a) * math.exp(b) 上面的例子中，如果传入一个 array 数据就会报类型错误。numba 的函数则可以用于解决该问题。 该装饰器也包括了两种运算模式——Eager 编译和 Lazy 编译。在使用 Eager 方式时，可以申明使用多种数据类型，但是要注意 signature 顺序——需要将最不确定的类型放在最后，例如: 1234567891011121314151617181920from numba import vectorize, float64, float32, int32, int64@vectorize([int32(int32, int32), int64(int64, int64), float32(float32, float32), float64(float64, float64)])def test(x, y): return x + y# 计算的结果会根据相应的输入类型匹配条件返回相应的类型a = np.arange(12) test(a, a) # 返回的元素类型是 inta = np.linspace(0, 1, 4)test(a, a) # 返回的元素类型是 float# 但是在处理其他类型数据时回报类型错误a = np.linspace(0, 1+1j, 12)test(a, a) # 回报类型错误 在实际应用中，可以模仿出 Numpy 的广播方式，例如: 1234567891011121314151617 &gt;&gt;&gt; a = np.arange(12).reshape(3, 4)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; test.reduce(a, axis=0)array([12, 15, 18, 21])&gt;&gt;&gt; test.reduce(a, axis=1)array([ 6, 22, 38])&gt;&gt;&gt; test.accumulate(a)array([[ 0, 1, 2, 3], [ 4, 6, 8, 10], [12, 15, 18, 21]])&gt;&gt;&gt; test.accumulate(a, axis=1)array([[ 0, 1, 3, 6], [ 4, 9, 15, 22], [ 8, 17, 27, 38]]) target 参数是用于选择函数以什么样形式计算，cpu 是以单核 cpu，parallel 是以多核 cpu 的方式计算，cuda 是以 CUDA GPU 方式计算。选择什么样的方式计算，可以参考数据量： 小数据量数据，小于 1KB 的数据和低计算要求的算法，可以使用 cpu 中等数据量数据，近似 1MB 的数据可以使用 parallel 超过 1MB 的数据和高频计算的算法，推荐使用 gpu 计算 cache 参数也是用于选择是否需要将函数缓存 @gpuvectorize 应用主要是解决任意维度的输入数据计算， @vectorize 和 @guvectorize 尚未弄清楚http://numba.pydata.org/numba-doc/latest/user/vectorize.html#guvectorizehttp://numba.pydata.org/numba-doc/latest/user/vectorize.html#vectorize 注意 使用 numba 的函数，不建议传入 list 会报错 ——“Reflected list” is being deprecated when there is no reflection?至于相关的原因，在 核心开发者有解释 。虽然可以通过其他调整属性 numba 的 List，但是没有传入 tuple 的效率高 1234567891011121314151617import numba as nbfrom numba.typed import List@nb.njitdef euclidean(x:tuple, y:tuple)-&gt;float: &quot;&quot;&quot; 直接计算欧式距离 $\\sqrt{\\displaystyle{\\sum_{i=1}^n(x_i-y_i)^2}}$ &quot;&quot;&quot; if len(x) != len(y): raise ValueError(&quot;Differente Length&quot;) result = 0 for x_i, y_i in zip(x, y): result += (x_i - y_i) ** 2 return result Numba 多线程数量设置 numba的多线程的数量通过全局变量来设置： 12import numbanumba.config.NUMBA_NUM_THREADS=8 使用 GPU 的注意事项，使用 GPU 并不一定会提高运算效率，其原因是 输入数据量太小，GPU通过并行性实现性能，同时处理数千个值。需要更大的阵列才能使GPU繁忙 计算太简单了，与在CPU上调用函数相比，将计算发送到GPU涉及大量开销。如果计算没有涉及足够的数学运算（通常称为“算术强度(arithmetic intensity)”），则GPU将花费大部分时间等待数据移动 数据复制，到GPU或从GPU复制数据，虽然对于单个函数运行包括复制数据到时间。但通常希望依次运行多个GPU操作时，将数据发送到 GPU 并保留在那里直到完成所有处理才有意义 数据类型长度一定超过数据需求，使用32位和64位数据类型的标量运行速度在CPU上基本相同，但是64位数据类型在 GPU 上的性能成本却很高。 64位浮点数的基本算术运算速度比32位浮点数慢2倍（Pascal架构Tesla）到24倍（Maxwell架构 GeForce）。 创建数组时，NumPy 默认为64位数据类型，因此设置 dtype属性或在需要时使用 ndarray.astype() 方法选择 32 位类型非常重要 允许直接在 GPU 中运算的 python 表达式 if/elif/else while for 循环 基本数学运算 math 和 cmath 模块中的某些函数 元组，Tuple 详情参考 the Numba manual","link":"/%5Bobject%20Object%5D/Python-Numba%E5%8C%85%E4%BD%BF%E7%94%A8/"},{"title":"数据化运营指标体系之会员运营指标体系","text":"数据化运营的体系分类包括：会员数据化运营、商品数据化运营、流量数据化以及内容数据化运营。运营指标体系是量化分析的 metrics，是说明命题的参考依据。本文从相关分类上的指标体系进行收集和归纳. 1. 会员数据化运营会员数据化运营是从企业的客户/会员，企业发展周期阶段，企业规模以及企业性质等因素进行相关分析，是辅助于 CRM 解答以下问题： 会员/客户生命周期状态 会员/客户核心诉求 会员/客户转化习惯和路径 会员/客户价值 会员/客户增长与维护 针对会员/客户的运营活动时间、措施以及需要差异化措施 1.1 会员数据化运营的关键指标会员数据化运营的关键指标体系包括：会员整体指标、营销指标、活跃度指标、价值度指标、终生价值指标以及异常指标。 会员整体指标 从注册数量、激活数量以及购买数量等角度搭建相关指标。在落地角度来看需要根据具体的业务和应用场景区分，例如针对激活会员的定义，需要确认什么行为是确定为激活状态的用户——点击链接、手机验证还是身份证验证；应用中是需要以单纯激活的数量来量化还是以激活率来量化。 会员营销指标 针对的可以通过一定会员营销方式——手机号码、邮箱、微信以及其他可以触达信息的方式的会员数量（可营销会员数量）。费用方面指标，可以从营销媒介费用、优惠券费用以及积分兑换费用——积分兑换费用可以用直接兑换货币或者兑换量化的物品，这类费用都会算作营销费用。收入方面指标，主要是营销收入，其中需要特别注意对相应的动作、渠道等有一定的埋点用以区分活动类型、渠道以及促销码促成交易等。在应用中会用到的比值型指标： 用券会员比例 用券金额比例 用券订单比例 用券用户平均订单金额 用券用户复购率 营销费率，针对的是会员营销费用占营销收入的比例 每注册收入，每个注册用户带来收入 每订单收入，每笔订单带来收入 每会员收入，每个会员带来收入 每注册成本，每获得一个注册用户需要成本 每订单成本，每获得一个订单需要成本 每会员成本，每获得一个会员需要成本 每挽回损失用户成本 每单位线索成本，eg:获得一个联系方式成本 会员活跃度指标 整体会员活跃度的评价是以会员的动作或者关键指标作为会员是否活跃的标识。该指标是反应累积的用户活跃度情况。统计信息包括所有会员的行为，包括账户行为、互动行为、订单行为以及分享行为——从业务角度出发将相关行为拆分到需要跟踪的行为，可取值范围以及行为编码、具体的权重值构成可以搭建出相应的活跃度定义矩阵。 活跃用户数，通用的指标包括 DAU（每日活跃用户）、WAU（每周活跃用户）以及 MAU（每月活跃用户数量），三者在同一个统计周期中相同用户计数一次。 会员价值指标 会员价值分群 通过模型或者方法将会员分为几个群体和层级，其是用户标签值，用于体现用户状态、层次以及价值区分 复购率 针对一定周期内购买两次以及以上的会员比例。但在统计上可能存在差异——一类是自然月内的统计，另一类是间隔之间的统计，以及两者混合的统计的方式 消费频次 统计的是一定周期内的消费次数。应用上可以反映出用户对于企业的用户消费黏性指标 最近一次购买时间 统计的是用户距离上次购买或者消费的时间，是用于评估会员消费价值黏性指标 最近一次购买金额 统计的是用户最近一次订单的消费或者购买金额。该指标是用户反应消费的能力 会员终生价值指标 会员生命周期是从用户称为企业会员开始到离开的数据统计值，需要注意相关指标不是限定在一定的时间内，而是从整体了解会员的状态，同时也是衡量整体价值的指标 会员生命周期价值（CLV，Customer Lifetime Value），会员生命周期内的订单金额综总和 会员生命周期订单量，会员整个生命周期内的下单量总和 会员生命周期平均订单价值，会员整个生命周期内下单金额除以下单量 会员生命周期转化率，针对的是会员在完整生命周期内完成订单数和触达企业产品的次数比例。eg：会员访问 100 次网站，其中消费次数为 2 次，其会员生命周期转化率为 2% 会员生命周期剩余价值，预测会员在生命周期内内还能产生多少价值。该指标是一系列的预测性指标： 预期未来 20 天的会员转化率 预期生命周期剩余订单价值 预期 7 天内下单数量 预计下 1 个订单的订单金额 下次购买的商品名称 会员异动指标 针对不再购买、消费相关服务、产品的用户相关指标 会员流失率 统计的是流失的会员数量占全部会员数量的比例。会员流失是需要重点关注的指标，一方面需要注意流失现状和行业标准差异，另一方面是需要关注流失率的趋势。前者是用于监控实际现状，另一方面是需要关注波动情况以及时响应 会员异动比 统计新增会员和流失会员之间的比例关系，主要反应的是会员增长和流失之间关系，能体现用户发展状态 1.2 会员数据化运营应用场景数据化运营的应用主要包括会员会员营销和会员关怀。会员营销包括建立基于会员的客户关系管理系统，用户的类型转换，挖掘用户潜在需求和消费热点，基于历史数据为营销活动提供策略和建议以达到精准营销，会员营销中异常转化识别出有效 VIP 客户，以及挖掘会员传播关系，找到口碑传播效应的关键节点。 会员关怀包括对预警事件监控与策略设置，会员行为分析，喜好分析，群体行为分析应用，用户流失预测，以及对会员生命周期的关怀管理 1.3 会员数据化运营分析模型1.3.1 会员细分模型将整体会员划分为不同的细分群体，以方便应用于营销和关怀。搭建相关模型会使用到的方法，包括基于用户标签进行划分、ABC 分类方法以及聚类方法等： 基于用户标签分类 这类属性是直接利用用户相关的标签，eg:会员地域、类别、消费等级以及其他偏好 ABC 分类 利用主要特征进行累积排序的方式，将 A、B、C 三类根据分别占比的不同，确认主要因素（发生频率占 0%80%)、次要因素（发生频率占 80%90%）和一般因素 聚类法 利用聚类模型对会员进行分类 1.3.2 会员活跃度模型和会员价值度模型这里主要说明应用 RFE 或者 RFM 模型结果： 基于模型结果下用户的得分，对会员活跃度进行解析以及不同得分的会员进行差异化运营 将得到\b的会员得分作为得分，以作为后续分析和模型输入特征需要注意模型的得分解析，会因为行业、产品类型等出现不同的解释，应用上也需要根据具体情况判断 1.3.3 会员流失预测模型会员的流失预测模型，是对会员生命周期管理的重要预防性应用。在应用中流失预测模型中重要的关键是明确流失的因素，例如什么条件、什么特征属于流失会员，同时是永久性流失还是临时性流失。从流失的因素可以从两个角度分析，会员主动流失（表现是用户明确表达，不再希望收到相关信息、服务等），另一类没有从会员角度得到关于主要业务领域的有效反馈。在搭建该模型中需要注意以下几个问题： 流失会员的样本数量是少数类，需要注意处理样本不均衡问题 模型特征的选择需要注意实际情况 模型在进行特征工程时，需要注意结合时间维度搭建相关特征，同时访问、登陆、浏览/搜索以及咨询等行为数据也应当应用于模型中应用上模型搭建是周期性的而非一次性的，需要从天、周以及月为周期进行监控；对用户的流失标签管理，可以结合营运需要进行关怀和营销。 1.3.4 会员特征分析模型会员特征模型的作用是对会员特征做分析，包括了两类可能的结果——一类是对业务提供行动的细节要素，能够强实施性的数据分析特征；另一类是通过数据分析结构，提供详细的动作因素，指明下一步的行动方向和目标。 有明确特征的分析模型 通过明确的业务方向，找到达到目标时间的会员特征。搭建的模型方法中，包括利用决策树的方法找到影响目标变量的关键因素，利用关联规则的方式明确会员特征，利用异常检测的方式明确异常特点以作为运营支撑。 针对没有前期经验或者目标下，通过对整体特征解析会员的全貌以及差异。例如使用聚类方法明确会员差异特征，另一种是利用统计分析来了解会员情况。 1.3.5 营销响应模型是在营销活动之前，针对会员对于营销活动可能的响应搭建的相关模型，以了解响应的会员特征、占据的响应用户比例以及销售情况分析。在数据收集方面，需要特别注意数据采样范围（搭建模型的用户数据，应该是包括了会员用户的整体用户数据），数据字段采集（包括各个媒介和渠道下的数据，手机短信、电邮等，发送时间、频率、信息等运营关键要素以及实施后结果），数据收集周期（对于营销活动电邮的有效期一般在 1～7 天，手机短信一般在 1 天） 通过该模型\b可以从两个方向下得到应用： 基于模型找到最可能产生购买转化行为的规则。eg:最近一次购买时间在 3 个月以内、会员等级为三级以上、总订单额大于 3000、订单量大于 10 的客户，以这种方式得到相关条件之后可以直接进行用户筛选以及营销推广 基于模型预测可能的定量转化数量、转化率，转化客户的客单价，以此计算可能的营销收入作为营销活动计划提报的数据量化指标和支持依据","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%BC%9A%E5%91%98%E8%BF%90%E8%90%A5%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/"},{"title":"关联分析及其应用","text":"关联分析是通过寻找大型数据中的隐藏关系，确认可用的关系规则，这种确认的关系即是关联规则。其中典型的应用例子为购物篮分析，沃尔玛的关联分析得出的“尿布-啤酒”规则。实际应用的场景，不仅限于购物篮分析，还有可能应用于网页挖掘等。 1. 关联分析在进行关联分析时，会涉及到以下相关概念： 支持度计数 是对项集(Itemset)的事务个数统计。$k$ 项集表示存在 $k$ 个项，例如 ${\\text{啤酒},\\text{牛奶}}$ 是一个 2 项集；如果是空集，那么 $K$ 表示为零 关联规则 表示存在 $X \\rightarrow Y$ 的关系表达式，其关系的强度需要通过支持度(Support)和置信度(Confidence)来度量。但仅依赖支持度和置信度，可能存在虚假模式的可能性，因此引入了另一个参数提升度(Lift)进行分析。 关联规则支持度 用于统计数据集中频繁程度，表达式 $s(X\\rightarrow Y)=\\frac{\\sigma(X\\cup Y)}{N}$，其中分子表示 $X\\cup Y$ 的计数，分母表示样本容量。其意义在于判断规程重要程度，应用上是用于删除掉低重要性规则 关联规则置信度 确定的是 $Y$ 在包含 $X$ 的事务中出现的频繁程度，从贝叶斯规则来看它表示的给定 $X$ 条件下 $Y$ 发生的概率，其表达式是 $s(X\\rightarrow Y)=\\frac{\\sigma(X \\cup Y)}{\\sigma(X)}$，其是用于推断规则的可靠性 关联规则提升度 应用关联规则和不应用规则产生结果的比例，其表达式为 $l(X\\rightarrow Y)=\\frac{s(X\\rightarrow Y)}{s(X\\rightarrow X)}$。当提升度等于 $1$ 时，关联规则应用与否结果相同，大于 $1$ 表示关联规则能产生更好的效果，小于 $1$ 表示关联规则存在负相关作用 1.2 关联分析应用在经典的“尿布-啤酒”的商品销售关联分析之外，关联分析还可以应用于更多的其他场景中。这种相同项的分析模式，还可以应用于内容-内容分析：1）分析站点内页面浏览的相关性，2）用户关键字搜索关联分析，3）不同场景的关联分析，例如一定的会话 session 中分析用户浏览产品和购买产品分析，浏览价格和购买价格的关联分析等不同方式，其中可应用于促销活动的价格策略制定。4）相同场景关联分析，例如对页面功能、页面应用选择等进行关联分析，这种分析模式可以应用于功能组合、开发和身体优化的方向确认；针对内容查看、点击的关联分析可以查找打包、组合以及其他营销策略以及提升用户体验等方向。 除了通过单次“购物篮”进行关联分析寻找可用规则外，还可以在此基础上添加上“时间”序列的维度进行分析。这种添加时间维度分析的模式，和“完成某个事务之后在特定的时间周期内完成其他的事务”类似。例如通过用户在上次对产品/服务行为和时间，推测用户下次可能的有行为的产品/服务以及可能的时间。 2. 关联规则产生多数关联规则挖掘算法采用的方法是分为以下两个步骤： 频繁项集产生，目的是挖掘出满足最小支持度的项集，以生成这些频繁项集 产生规则，从频繁项集中提取高置信度的规则，这类规则即是强规则 在以上两个步骤中，频繁项集的产生需要消耗大量的计算资源，因此第一步是需要有效的算法来完成。 2.1 Apriori 算法Apriori 算法开创性的使用来支持度剪枝计数，控制候选项集的指数增长。假定支持度的阈值为 $60%$，那么在后续项集的最小支持度计数约为 $3$（项集的计数少于 $3$ 时，会被抛弃）。 3. Apriori 应用示例算法依赖于第三方库 apriori，具体的代码示例如下: 12345678910111213141516# 导入库import apriori# 转换为关联所用的记录模式ids = df.order_id.unique()records = [data[data['order_id']==each_id]['product_name'].tolist() for each_id in ids]# 通过调用自定义的apriori做关联分析，分别确认支持度和置信度的阈值，搭建相应的规则minS = 0.01minC = 0.05L, suppData = apriori.apriori(records, minSupport=minS) rules = apriori.generateRules(records, L, suppData, minConf=minC)# 创建频繁规则结果result = pd.DataFrame(rules, columns=['item1', 'item2', 'instance', 'support', 'confidence', 'lift'])","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%88%86%E6%9E%90%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"},{"title":"数据化运营指标体系之内容运营指标体系","text":"内容运营是媒体信息化运营的核心，指的是对内容策划、编辑、发布、优化以及营销等一系列工作。其中内容生产的方式上包括 UGC、PGC 以及 OGC(职业化生产内容，和 PGC 存在一些差异——内容生产者更多样，不仅仅是一类专业化内容生产)。在内容运营的数据指标类型方面，包括了内容类指标、SEO 指标、内容流量指标、内容互动指标以及目标转化指标。 内容质量指标衡量内容的质量包括但不限于内容原创性、关键词等相关指标。原创性的判断是通过查询内容和其他已有内容进行对比（可能为自有内容或者外源内容），通过计算非重复关键字的比例作为原创可能性（即原创度）判断方式: $1-\\frac{\\text{重复关键字数量}}{\\text{总关键字数量}}$ 对于关键字的获取，可以使用分词工具来完成 SEO 指标SEO(Search Engine Optimization，即搜索引擎优化)，是体现网站在自然搜索结果中的用户体现效果。要评估内容的 SEO，可以从内容收录、关键字以及点击行为等方面分析。 内容收录 收录数量/比例，收录数量是所有内容中能够被搜索引擎搜索而加入到内容索引中的数量。而收录占比，是加入到索引的数量占总内容数量的比例 收录速度，它是衡量收录能力的指标，速度越快那么越能在获取到早期关注 内容收录到影响因素包括多个方面因素，网站结构、爬虫规则、站内互链设置、URL 设置等[^1]。 关键字排名反应搜索引擎搜索内容相关关键字时，内容在结果中的排名。它是一个结果指标，反应的是 SEO 的结果。而影响关键字排名的因素包括外链数量、关键字密度、子域名设置以及关键字在内容中的分布(例如关键字在网页中的位置信息) 点击行为是用户对搜索结果响应结果，也是对于内容运营的其中一个目的——吸引用户流量。 点击量，反应用户点击次数的信息 点击率，是一个比率指标，计算方式 $\\frac{\\text{点击量}}{\\text{总展示次数}}$，可以反应用户在内容上得到的信息匹配程度 内容流量指标内容运营也是需要用户流量，所以一般的流量指标也可用于内容流量中: 到达率 UV PV 访问深度 包括但不限于以上内容 内容互动指标是直观评价用户对内容的响应结果，所以是一类互动相关的指标： 收藏量，它是衡量用户参与到相关内容的指标 点赞量/踩量，是用户对内容的评分方式的指标，它可能有多种体现形式——等级评价或者评分。这里衍生出了其他可用的指标，包括点赞率、平均评分、最高/最低评分 评论量，它是体现互动的方式，但并不是评价内容质量的最佳方式——一方面内容质量和评论数量，很难说有直接因果关系（例如以争议性内容），另一方面评论自身的内容也是需要在内容监控中的角度。针对评论本身，还有包括文本信息、情感信息、语义信息以及主题信息等还需要进行分析和监控 传播量/传播率，包括转发、分享等形式传播 二次传播率，二次传播表示的是内容能够产生两次及以上的传播，它是扩大传播效应的基础。但另一方面因产品/服务定位的不同，可能在内容传播上不一定回去追求内容传播的长期性——“阅后即焚”的产品/服务即不是追求的长期性 目标转化指标目标转化的目的反应了企业对内容生产的目的，常见的目标和对应的指标： 希望用户停留在内容中，阅读更多的内容。对于这类目标，可以量化访问的深度、平均停留时间等用户体验类指标 希望能够长期用户运营，获取到用户联系方式。可以直接量化用户完成相关目标的数量为指标 希望完成特定的目标事件，例如:内容下载、广告点击。通过量化完成特定目标事件的次数或者比率来评估 希望提升用户对内容付费转化。可以以相关打赏/捐赠等数值进行评估，同时可以衍生其他相关指标，打赏量/率，平均打赏金额以及重复打赏率等 参考[^1]: SEO 新手指南：基础知识 Google Developers","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%86%85%E5%AE%B9%E8%BF%90%E8%90%A5%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/"},{"title":"数据不平衡处理","text":"数据不均衡问题指样本数据中不同类别的容量差异过大，这类数据集在特定的领域会更容易遇见，例如: 信用欺诈、垃圾邮件检测、客户流失以及广告点击等。在搭建这类数据集的分类模型，容易出现过拟合情况，因此需要对数据集进行一定的处理。 过采样处理过采样，即上采样(Over Sampling，或者 Up-sample Minority)，是通过对少数类别的样本进行随机重复抽样的方式，达到增强该类别数据的目的。实施的方法： 最简单快速的方式是又放回的重复抽样，Sklearn 提供的 resample 方法可以实现方式Bootstrap >unfold12345678910111213141516171819202122232425262728from sklearn.utils import resampleimport numpy as npimport pandas as pd# 生成模拟数据label = np.random.choice([0,1], p=[.2, .8], size=(1000, 1))features = np.stack( [np.random.normal(i, np.random.random() * 10, size=1000) for i in range(5)], axis=1)data = pd.DataFrame(np.hstack((features, label)), columns=[f&quot;col{i}&quot; for i in range(5)] + [&quot;label&quot;])# 拆分出少数数据minority = data[data[&quot;label&quot;]==0]majority = data[data[&quot;label&quot;]==1]# 有放回重复抽样sample = resample( minority, replace=True, # 有放回 n_samples=majority.shape[0], random_state=42)# 合并数据new = pd.concat([majority, sample], ignore_index=True) 采用该方法得到的样本，大类别样本容量和小类别样本容量相同。此外可以通过 imblearn 也可以实现简单随机抽样: Bootstrap >unfold123456789from imblearn.over_sampling import RandomOverSampler# 实例化对象ros = RandomOversampler( random_state=42, sampling_strategy=&quot;not majority&quot;, # 仅对小类别样本抽样)nfeatures, nlabel = ros.fit_resample(data.drop(&quot;label&quot;), data[&quot;label&quot;]) 小类别样本生成方法，包括 SMOTE 方法，它是通过添加噪声、干扰数据进行生成小类别数据的方法；ADASYN 方法，对不同类别学习过程的难度水平（数据合成的量越大，那么其学习难度越大）调整权重分布Generate >unfold123456from imblearn.over_sampling import SMOTE, ADASYN# SMOTE 方法生成数据-默认使用的策略是 not majoritynfeatures, nlabel = SMOTE().fit_resample(data.drop(&quot;label&quot;), data[&quot;label&quot;])# 使用 ADASYN 方法生成数据-默认使用策略是 not majoritynfeatures, nlabel = ADASYN().fit_resample(X, y) 欠采样处理欠采样，即下采样（Under Sampling，Down Sample Majority），方法上是通过减少类别中的多数类别样本数量来实现样本均衡，这种方法主要是降低大类别信息。最简单的方式是对大类别进行不放回的抽样： Under Sampling >unfold12345678910111213141516171819202122232425262728rom sklearn.utils import resampleimport numpy as npimport pandas as pd# 生成模拟数据label = np.random.choice([0,1], p=[.2, .8], size=(1000, 1))features = np.stack( [np.random.normal(i, np.random.random() * 10, size=1000) for i in range(5)], axis=1)data = pd.DataFrame(np.hstack((features, label)), columns=[f&quot;col{i}&quot; for i in range(5)] + [&quot;label&quot;])# 拆分出少数数据minority = data[data[&quot;label&quot;]==0]majority = data[data[&quot;label&quot;]==1]# 有放回重复抽样sample = resample( majority, replace=False, # 不放回 n_samples=minority.shape[0], random_state=42)# 合并数据new = pd.concat([minority, sample], ignore_index=True) 除了这种简单抽样的方式外，imblearn 提供了其他欠采样的方法，例如压缩最邻近数据。 样本组合组合样本的方式是将数据重新分配，再进行训练的过程。整体的思路类似于模型集成的方式：从大样本中随机抽取数据和小样本数据进行训练，这样不同的数据集合生成不同模型；在预测阶段，通过投票的方法进行预测结果。 选择合适的模型具错误类别惩罚模型对于某些模型是成本敏感性的，例如支持向量机（SVM）的可以通过增加分类错误的类别，来提升模型鲁棒性。 SVM >unfold123456789from sklearn.svm import SVCmodel = SVC( kernal=&quot;linear&quot;, class_weight=&quot;balanced&quot;, # 该参数用于调整模型类别权重)# 拟合模型model.fit(feature, label) 基于决策树的模型决策树模型在不平衡的数据上也会有相对较好的表现，这是因为该算法天然的具有从不同类别的信号中生成层次性结构的特点。特别是在决策树集成模型中（例如随机森林、梯度提升树等）能够通过单一算法模型，得到良好的表现 参考 Over-sampling, Imblearn SMOTE for Imbalanced Classification with Python Haibo He, Yang Bai, ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning 宋天龙, Python 数据分析与数据化运营","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86/"},{"title":"数据化运营指标体系之流量运营指标体系","text":"流量是企业运营的基础，泛化的流量概念针对用户对内容关注、转化等过程方式。流量的运营包括流量采集、流量数据与其他数据整合、流量指标选择以及流量数据化运营等，主要内容集中在对流量指标的整理。流量数据化运营指标，包括了站外营销推广指标、网站流量数量指标以及网站流量指标。 营销推广指标 曝光量 内容对外用户展示的次数，通常是统计特定追踪代码加载的次数。该指标并不是一个突出的分析指标，它只是反映出广告被加载了多少次，而影响用户查看内容包括网络、内容素材、内容位置等 点击量 内容被用户点击的次数。在记录点击过程中，因网络、监测规则、规范定义等原因，存在点击了内容而未被统计的情况。 点击率 是计算用户点击内容的数量与内容曝光量的比例。应用上可以反映内容投放与投放渠道的匹配，另外可以用于说明用于对内容的喜好 费用相关指标：CPM(Cost Per Mile，每千人成本)，CPD(Cost Per Day，每天展示成本)，CPC(Cost Per Click，每次点击成本)，CPA(Cost Per Action，每次转化成本)，每 UV 成本(广告费用和 UV 数量比例)，每访问成本(广告费用和访问数量的比例，访问数量的计算存在单用户多次访问)，ROI(投资回报率，对于负利润的情况，可以直接使用销售额与费用的比例) 收益相关指标：每次点击收益(一般和 CPC 对应，分析的是转化的订单金额)，每 UV 收益(分析每个用户转化收益)，每访问收益(每个访问次数下的收益)，每次目标转化收益 网站流量数量指标 到达率 在不同的应用场景下有不同的定义^1，在广告流量的统计下指站外广告点击后达到网站的情况，到达量统计的是带有站外标记的链接进入网站后触发站内跟踪代码的次数，计算上是到达量和点击量之间的比例。该指标可以用于体现出网站页面加载结果，过低的情况下可能是因为页面加载过慢 UV 与 Visit 两者都是独立访客的数量，但是通常情况下访客数会有较短的刷新时间——例如定义 30 分钟间隔的情况下，用户超过 30 分钟刷新时即计算为新的 Visit。 PV 即页面浏览量，是衡量站内页面的曝光量 新访问占比 之前没有任何访问记录的用户占比。对于新访问用户的定义，包括之前未访问过的用户，也包括定义的新用户访问信息被删除修改之后的访问（例如删除或者修改 cookie 后访问）。对于访问后的用户再次访问时，即被作为老用户计算。 实例数 是对站内自定义对象触发次数，这类自定义对象可以是某个按钮、下拉项或者功能区。 网站流量质量指标 访问深度 即人均网页浏览量、每次访问页面平均浏览量，计算方式是 $\\frac{\\text{PV}}{\\text{访问量}}$ 其中访问量使用 Visit 统计表示计算每次访问的平均浏览量；如果使用 $UV$ 表示访问量，那么表示的是计算人均页面浏览量。访问深度的指标的意义解读，需要注意指标过高可能是因为用户不能确认需要的内容，指标数据值过低可能意味着用户对网页内容不感兴趣 停留时间 分为用户在站点停留时间和页面停留时间，前者计算的是最后一次请求与第一次请求之间的时间差，而后者针对的是当前页面发出下一个页面请求的时间与跳转到当前页面的时间差。在实际情况下存在无结束时间戳的情况，例如发生退出或者跳出页面，可以通过其他数据来辅助计算——当心跳监测的情况下可以以最后一次心跳为准，方法二是使用固定的时间填充（假设页面停留时间为 30s 时，使用当前页面请求时间戳和这个固定时间相加），方法三是直接将这类实例数据丢弃 跳出/跳出率 跳出指用户到达落地页之后，未发生点击跳转即离开页面。计算跳出率，即计算跳出访问量和落地页访问量的比例。关于跳出需要明确未发生点击的实际意义，例如重复刷新落地页面、特定的单页面是否发生了某些事件或点击操作。因跳出是针对特定的落地页面，其可以作为分析站外流量的质量或者落地页面设计质量的指标 退出/退出率 退出指用户从站点离开，不会发生进一步动作的行为。退出率和跳出率计算方式相似，但两者有显著的差异前者针对站点全页面和总访问量的分析，后者是针对落地页及其访问量。可以参考 Google Analytics（分析）示例分析 产品页转化率 该指标是分析网络购物流程中的一个指标，表示的是用户从其他页面跳转到产品详情页的转化率。计算方式上为产品页访问量和页面总访问量的比率，注意访问量所使用的统计量 加入购物车转化率 加入购物车是用户进入购物车环节第一步，计算方式是加入购物车访问量与总访问量的比例。因该指标显示用户的购物导向性，因此该指标是衡量站外营销和站内运营的效果 结算转化率 结算是用户购物环节第二步，用户需要该在阶段完成确认订单信息——包括联系人、收获地址、优惠信息等。计算方式为结算访问量和总访问量的比例率 购物车内转化率 衡量用户将产品加入购物车后完成订单的比例，计算的方式为订单提交的访问量与加入购物车访问量比率。该指标常用于销售类电子商务网站监控购物车流程设计和流量作弊。但监控购物车内转化率不适用于某些购买决策周期较长的产品。 参考","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%B5%81%E9%87%8F%E8%BF%90%E8%90%A5%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/"},{"title":"过拟合和欠拟合问题","text":"在搭建模型的过程来看，可以看作是优化与泛化过程。优化(Optimization) 是解决模型训练过程中在训练数据上表现出的较佳性能，而泛化(Generalization) 是体现模型用于解决实际的问题的性能。两者较难都达到最佳性能，两者的差异化表现可以即是过拟合和欠拟合。 1. 过拟合和欠拟合欠拟合，指模型在训练数据上性能表现不好，同时在新数据上表现不佳。过拟合，指模型在训练数据上性能表现良好，但是对新数据上表现不佳。对于过拟合和欠拟合的认知，可以从一般和特例的关系来思考——将特例情况当作样本的一般情况时，就是过拟合的情况；如果完全没有学到数据的一般特点，出现“一叶障目”的情况时就是欠拟合。将过拟合和欠拟合，在机器学习中是非常常见的问题，是训练模型过程中特别需要注意的。一般的解决该问题的思路是通过增加数据量，调节模型等方式进行优化。 2. 模型优化模型优化的角度，除了增加使用数据量对方法外还有对结构风险调整的方案，训练策略的调整等方式。 2.1 正则化正则化是应用于降低过拟合的方法，是用于调整模型结构风险的方式。例如在线性回归算法中岭回归、LASSO 以及弹性网络的算法，就是在模型使用了 L1 和 L2 范数正则。而在深度学习中，对模型权重添加权重正则的方式也是通过范数调整；此外模型中添加 Dropout 的方式，以一定概率屏蔽某些节点等也是正则化的一种。 2.2 交叉验证交叉验证是模型选择的一种策略，将模型训练数据和验证数据拆分，以提前验证模型泛化能力。交叉验证的方法包括： 简单交叉验证，单次使用数据拆分为训练数据和测试数据的方式，用测试数据评价模型效果 K-Fold 验证，将数据随机分为 K 份数据，使用 K-1 组数据进行训练，使用其中一组数据进行测试的方法。这样模型就有 K 中可能的训练方式，将验证结果取平均值用于筛选模型 留一法，是 K Fold 的特例，直接使用 1 份数据进行验证，其他作为训练。这种方式优点是不受数据随机影响，更能够学习到数据的一般特点，相应的增加了模型训练的时间消耗——因为有多少个数据就需要进行多少次训练。因此该方法可以用于小样本数据中","link":"/%5Bobject%20Object%5D/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/"},{"title":"[统计学]统计学","text":"关于统计学的笔记 1. 渐近分布抽样分布、参数估计以及假设检验构成推荐统计学核心，通过精确的抽样分布能够进行有效推导出统计量，这样可以以有限样本容量进行统计推断。在实际的抽样分布理论中，确认的精确抽样分布是有限的。因此需要借助极限的方法来作为抽样分布，其思路是当样本量无限增大时，可以进行有效的统计推断。以这种极限抽样分布的方式就是渐近分布，它也是其他推断统计的基础——例如中心极限定理。除了精确抽样分布和上面的渐近分布，此外通过计算机进行随机模拟获得近似分布的方式是一种随机模拟方法（相关内容属于统计模拟计算）。 2. 二项分布的抽样分布对于计量值的变量的抽样分布研究问题的一个例子，假定总体中对某产品的喜好比例为 $\\pi$ 中进行抽样 $n$ 个体调查，对产品喜好的概率。这种问题就是从二项分布进行抽样估计，根据二项分布原理和渐近分布原理可以得到喜好产品的数量为 $\\hat{m}\\sim \\mathcal{N}(n \\pi, n \\pi(1-\\pi))$。对于结果需要表示为喜好比例的情况，需要根据以下计算公式推导: $$\\begin{align}\\exists&amp; \\hspace{.5cm} \\text{变量 }x\\text{ 满足 } E(x)=\\mu, D(x)=\\sigma^2 \\&amp;\\forall \\text{ 常数 } \\lambda \\text{ 使 } \\lambda\\cdot x \\text{ 的期望与方差变为 } E(\\lambda \\cdot x)=\\lambda \\cdot \\mu, D(\\lambda \\cdot x)=(\\lambda\\cdot \\sigma)^2 \\\\end{align}$$ 所以存在一个比例 $\\hat{p}=\\frac{\\hat{m}}{n}$，所以 $\\hat{p}\\sim \\mathcal{N}(\\pi, \\frac{\\pi(1-\\pi)}{n})$","link":"/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6-%E7%BB%9F%E8%AE%A1%E5%AD%A6/"},{"title":"自监督学习之AutoEncoder","text":"自监督学习是监督学习的特例(严格来说，它并不是监督式学习)，其学习数据中并没有相应的人工标注标签，但作为监督学习是存在标签的——一般标签是来自于输入数据中。比较显著的例子是 AutoEncoder，是直接使用输入数据作为输出的标签。","link":"/%5Bobject%20Object%5D/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8BAutoEncoder/"},{"title":"[书籍]乡土中国","text":"&emsp;&emsp;这本书是第一次看关于中国社会研究的书籍，是经由空哥推荐阅读。也是第一次认识到，我好像是根本就对我们所处的中国社会一无所知（虽然一直在自我催眠告诫自己需要了解这个社会）。这种无知包括中国社会在历史演变之下，那些真实的人是怎么面对这个社会的，又是因为一种什么样的力量让他们去以这样的方面面对。 乡土和差序格局&emsp;&emsp;曾探讨过一次了解一个社会，从制度、技术和文化三个基本方向是一种可行的方法。这本书中从中国乡土社会格局解释了这个制度之下，对乡民的影响，在这种格局之下产生对文化特点。&emsp;&emsp;受土地束缚的环境，一方面带来的积极影响是在相对稳定的环境下，乡民间的熟悉程度高，交流方式可以以简单方式达到更高效的交流（其中一个例子就是两个“熟悉”的部落之间，可以通过硬币来传达各种不同的信息）。另一方面，这种方式下确是“现代社会”下的弊端，这种冲突体现在“现代社会”需要的是明确的条文规范去明确规范。而在乡土社会下，是由另一种权力的形式保障这个这个社会运行。从历史不同的阶段，可以看到中国社会变革的失败和成功都受到这种形式的大力影响。这个是乡土社会制度特色，需要了解的是在这个环境特色下人的特色是什么样的。 &emsp;&emsp;“修身齐家治国平天下”，是受儒家文化影响下的人生理想，但是在本书中确从另一个角度探讨这个理想。在乡土社会下，君君臣臣父父子子所表达的意思是一定范围下的由“己”及人构成的社会网络，这种网络结构下体现了随着网络长度变化对于道德和“法律”判断下也是具有一定的伸缩性的。","link":"/%5Bobject%20Object%5D/%E4%B9%A6%E7%B1%8D-%E4%B9%A1%E5%9C%9F%E4%B8%AD%E5%9B%BD/"},{"title":"[书籍]摄影的哲学思考","text":"&emsp;&emsp;作者（威廉 弗卢塞尔）关于装置工具、人和社会关系的变化，不得不让人会想到工业化前和工业化后，以及当前面临这的 AI 带来影响三者关系变化。在工业化前工具是人的器官延伸，它是做人类实现某种目的的凭借。但是随着工业化的带来的工具装置功能变化，作为了一个“大机器”下的生产力，直接侵入了人的工作，人反而变化为工具的工具了。而当前技术变化的点是作为智能的工具更加侵入人的工作，导致人与工具的关系会发生重要的变化。 &emsp;&emsp;这三种阶段下人与社会的关系也呈现出不一样的点，前两个阶段是已经在历史记录的。而正在进行的变革，将带来什么样的刺激还需要慢慢观察。是让人彻底从工具中解放出来，而让人更有创造性还是被这种工具给淹没，需要时间来证明。 关于摄影是从观察者的角度去了解信息，而非简单对现实世界的复制","link":"/%5Bobject%20Object%5D/%E4%B9%A6%E7%B1%8D-%E6%91%84%E5%BD%B1%E7%9A%84%E5%93%B2%E5%AD%A6%E6%80%9D%E8%80%83/"},{"title":"GitHub 配置及其应用笔记","text":"本篇笔记记录使用 GitHub 过程中的配置及其使用笔记： GitHub 设置 SSH keys 单一环境下的多用户管理 GitHub 显示数学表达式 1. GitHub 添加 SSH Keys此处笔记内容不涉及注册 GitHub 的账户，针对的是注册好用户名之后配置步骤： 使用 ssh-keygen 生成 ssh key 将 private key 添加到本地 ssh-agent GitHub 配置 public key 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;&lt;'COMMENT'最好是在 .ssh 文件夹（MacOS 下的默认位置是 ~/.ssh，Linux 是在 /home/&lt;username&gt;/.ssh，Windows 是在 /c/Users/&lt;username&gt;/.ssh）下进行操作。需要完成的操作如下：1. 如有必要，需要生成一个 SSH key2. 确认 SSH 服务已经启动3. 将 SSH private key 添加到 ssh-agent以上过程需要在本地完成操作COMMENT# 使用 ssh-keygen 来生成 ssh key，命令格式如下：# -t 需要添加 key 类型，例如 rsa, ed25519, ecdsa# -f 申明将密码存储在特定的文件下，确认文件名称# -b 申明 key 的位数大小# -C 添加一个注释# ssh-kegen -t filetype -f ssh_key_name -b number_bits -C comment$ ssh-keygen -t rsa -f rsa_test -b 4096 -C &quot;test@test_mail.com&quot;Generating public/private rsa key pair.Enter passphrase (empty for no passphrase): # 可选择是否需要输入Enter same passphrase again:Your identification has been saved in rsa_test.Your public key has been saved in rsa_test.pub.The key fingerprint is:SHA256:2bcxd5TDwI01C3Iujk080XLdAlwSAberMVLv+3BkZxQ test@test_mail.comThe key's randomart image is:+---[RSA 4096]----+| B*O*++.|| .oX ++oo|| .o*o+ +E.|| . o+* .. .|| S .+o .|| +*+o || o*=o.|| ..+o.|| .... |+----[SHA256]-----+# 接下来第二步，测试 ssh-agent 是否已经启动。如果提示了 pid 值那么表示已经启动$ eval &quot;$(ssh-agent -s)&quot;Agent pid 49547# 添加 private key 到 ssh-agent$ ssh-add -K ~/.ssh/rsa_testIdentity added: /Users/new_user/.ssh/rsa_test (/Users/new_user/.ssh/rsa_test) 通过以上步骤生成了 ssh-key [^1][^2]，同时将 private key 添加到了 ssh-agent 中。接下来需要在 GitHub 中添加 public key[^3]。 12345678910&lt;&lt;'COMMENT'接下来完成配置 GitHub 中的 public key1. 复制 public key2. 在 GitHub 中个人界面下 Setting 中去添加 public keyCOMMENT# 复制 private key 对应的 public key（扩展名为 .pub）$ pbcopy &lt; ~/.ssh/rsa_test.pub# 将数据黏贴至 GitHub 个人页面下的 Setting 中进行配置 在页面 https://github.com/settings/keys 完成配置： 在设置的过程，需要添加标题和黏贴 public key 注意事项: 此外，需要注意⚠️上面命令主要的 Mac OS 系统中，部分命令存在差异——例如，pbcopy 无法在 Linux 和 Windows 中使用。具体的系统命令，可以在 Generating a new SSH key and adding it to the ssh-agent 页面查看。 2. 管理 GitHub 多用户前述内容能够解决单一用户的设置问题，当需要在单一平台上多用户切换使用时（这种情况可能包括两个 GitHub 账户或者 GitHub 与其他托管平台账户的之间），上面的情况就不能解决这些场景问题。 2.1 配置 Config在多用户的情况下，不能使用全局的用户信息和邮箱信息，需要取消相关信息： 12$ git config --global --unset user.name$ git config --global --unset user.email 可以不使用 --global 参数进行当前“局部”配置用户信息： 12$ git config user.name &quot;testuser&quot;$ git config user.email &quot;test@test_mail.com&quot; 这里假设我们已经生成了两个 ssh key，其中 private key 的文件是 ~/.ssh/id_rsa 和 ~/.ssh/rsa_test；此外对应的 public key 已经在相应的托管平台配置完成。接下来需要在 ~/.ssh 文件夹下创建一个 config 文件（文件名称就是 config），并且进行相应的配置，配置内容如下： 123456789101112# Default GitHub UserHost default # 可以任意取一个名称 # User 和 HostName 会生成一个 git@github.com 形式的文件——默认是 git@github.com 可以直连 # 为了避免错误这里将其拆分为 git 和 github.com User git HostName github.com IdentityFile ~/.ssh/id_rsa # 申明 default 的 private key 位置# Second GitHub UserHost second User git HostName github.com IdentityFile ~/.ssh/rsa_test # 第二个 private key 在完成以上配置之后，可以对 Host 进行测试链接情况： 123$ ssh -T default$ ssh -T second 当出现 Hi &lt;username&gt;! You've successfully authenticated, but GitHub does not provide shell access. ，表示可以链接。其中 &lt;username&gt; 是对应 private key 的用户名。 2.2 仓库配置一般情况下，通过 GitHub 创建仓库之后会出现相应的提示信息，直接输入相应的命令即可完成本地仓库配置，同时也能够进行远程管理。相应的提示信息如下 123456$ echo &quot;# test&quot; &gt;&gt; README.md$ git init # 初始化本地 repo$ git add README.md$ git commit -m &quot;first commit&quot;$ git remote add origin https://github.com/username/test.git # 添加 remote$ git push -u origin master # 第一次 push 上面是一般的情况下，现在需要将部分内容调整，主要是远程 repo 的地址信息需要调整： 1$ git remote add origin second:&lt;username&gt;/test.git 3. 显示数学表达式因为 GitHub 本身的 markdown 解析是一个不支持 Latex 的渲染，需要使用第三方工具进行解析数学表达式进行渲染。在 codecogs 进行输入一个对象等方式，生成一个 img 的缘来渲染数学表达式。例如需要添加 y=ax^2+bx+c 的等式，那么可以通过添加 img 对象来实现或者 img 这个 html 标签实现： 123![](https://latex.codecogs.com/gif.latex?y=ax^2&amp;plus;bx+c)// HTML 方式&lt;img src=&quot;https://latex.codecogs.com/gif.latex?y=ax^2&amp;plus;bx+c&quot; title=&quot;y=ax^2+bx+c&quot; /&gt; 结果如下： 参考 Generating a new SSH key and adding it to the ssh-agent ssh-keygen(1) - Linux manual page Adding a new SSH key to your GitHub account - User Documentation","link":"/%5Bobject%20Object%5D/GitHub%E9%85%8D%E7%BD%AE%E7%AC%94%E8%AE%B0/"},{"title":"Markdown 语法以及数学表达式总结笔记","text":"markdown 是一个轻量级标记语言[^1]，它的名称是为了对应 HTML 中的 Markup 对应的语言，提供了简介快速的语法来实现 Markup 的功能。因此 markdown 兼容了 HTML 的标签，此外还可以直接用任何常规的文本编辑器编辑。而本文档的内容主要是描述一些常用 markdown 语法，包括两个方面的内容：1）基本格式语法；2）常用的数学表达式。 1. 基本格式语法在进行常规书写的时候，常用的格式包括 Heading——可以用描述标题内容， 有序和无序列表，斜体，加粗，表格，代码块等格式。 1.1 HeadingHeading 的标签模式和 HTML 中 &lt;h&gt; 标签具有同样功能。HTML 中通过添加数字来表示 heading 等级，在 markdown 中通过 # 的个数表示同样的功能——例如上面的 # 基本格式语法 就是 H1 等级的 heading，而上面的 ## Heading 就是 H2 等级的 heading。常见的语法如下： 123456# h1 的 heading## h2 的 heading### h3 的 heading#### h4 的 heading##### h5 的 heading###### h6 的 heading 示例效果图如下 1.2 常见文字格式常见的文字格式就是分别为斜体、加粗，这两个可以通过添加 * 或者 _ 来进行调整文字格式。其中一个 * 或者 _ 是将字体变为斜体，添加两个 * 或者 __ 可以将字体调整为粗体，下面是语法演示： 12345*字体变为斜体***字体加粗**_字体变为斜体___字体加粗__ 以上内容得到的效果如下： 字体变为斜体字体加粗 1.3 列表列表分为有序列表和无序列表两种形式，其主要的书写方式存在差异。有序列表需要使用数字加点号及空格来表示——1. ；而无序列表可以通过星号或者减号加空格来表示——* 或者 - 。下面是语法演示： 1234567891011121. 第一个有序列表 list2. 第二个有序列表 list3. 第三个有序列表 list* 第一个无序列表 list* 第二个无序列表 list* 第三个无序列表 list- 第一个无序列表 list- 第二个无序列表 list- 第三个无序列表 list 得到的效果如下： 第一个有序列表 list 第二个有序列表 list 第三个有序列表 list 第一个无序列表 list 第二个无序列表 list 第三个无序列表 list 1.4 表格在 markdown 中同样可以插入表格内容，插入的方式如下： 123| 第一列 | 第二列 || ------ | ------ || 内容一 | 内容二 | 得到的小如过下： 第一列 第二列 内容一 内容二 上面的效果是没有进行格式调整，如果需要对内容的对齐方式调整可以通过在第二行中添加冒号（:）来说明对齐方式： 123| 第一列左对齐 | 第二列居中齐 | 第三列有对齐 || :--------- | :--------: | ---------: || 内容一 | 内容二 | 内容三 | 下面是得到的效果，分别对第一列进行做对齐，第二列居中对齐和第三列右对齐处理： 第一列左对齐 第二列居中齐 第三列有对齐 内容一 内容二 内容三 1.5 代码代码可以分为行内代码和行间代码，行内的代码可以表示是一个简单的表达式或者语句。如果是需要在行内插入代码可以通过 `` （即反引号，一般是 Tab 键上面的那个键或者说是在数字 1 之前那个键），在反引号内书写内容；当需要添加大量的代码块内容时，那么就需要使用三个反引号换行书写内容之后换行添加另三个反引号——注意可以在第一行中添加 {} 以及语言类型来说明是哪种语言的代码，⚠️这点在 Rstudio 中使用 RMD 文件，即 R 的 markdown 文件中尤其重要。演示如下： 123456这个是一个行内的代码 `python [option] filename.py`，下面是一个行间代码块：​```{R}# 加载 test.csv 文件为 data.framedf &lt;- read.csv(&quot;./test.csv&quot;)​``` 这个是一个行内的代码 python [option] filename.py，下面是一个行间代码块： 12# 加载 test.csv 文件为 data.framedf &lt;- read.csv(&quot;./test.csv&quot;) 2. 数学表达式数学表达式 markdown 的数学表达式的书写，是遵循了 Latex 的常用语法格式。但是因为某些网站支持了 markdown 但是并不支持 Latex ，所以看起来文档出现很怪异的现象（例如在 GitHub 上，可能为看见 $$ 等标识符）。 2.1 单行与多行表达式数学表达式分为两类：单行表达式 和 多行表达式。两者的差异主要在于使用了几个 $ 标识符，此外多行表达式每行 \\\\结尾，每个元素 &amp;分隔。下面演示单行表达式和多行表示： 12345678这个是单行的质能方程 $E=mc^2$，下面是一个多行的事件概率表示：$$\\begin{cases}p=0, &amp; x=不可能事件 \\\\0 \\lt p \\lt 1, &amp; x=随机事件 \\\\p=1, &amp; x=必然事件\\\\\\end{cases}$$ 这个是单行的质能方程 $E=mc^2$，下面是一个多行的事件概率表示：$$\\begin{cases}p=0, &amp; X=不可能事件 \\0 \\lt p \\lt 1, &amp; X=随机事件 \\p=1, &amp; X=必然事件\\\\end{cases}$$ 2.2 上标和下标在书写过程中可能会遇见上标和小标的可能行，上标需要使用 ^ ，下标需要 _ 。如果遇到使用过多内容的上下标处理，那么就需要使用 {} 将所有内容进行包裹，格式如下： 1$f(x)=1/(1+e^{-(b_0+b_1*x)})$ 这个是一个 Sigmoid 函数和一元一次函数构成的复合函数 $f(x)=1/(1+e^{-(b_0+b_1*x)})$。 Note： 当然 Latex 有更优雅的语法可以使用，例如上面的表达式可以使用\\frac{}{} 来表示 $f(x)=\\frac{1}{1+e^{-(b_0+b_1*x)}}$。更多的方式，可以查看一下 Latex 语法[^3] 2.3 其他常见标识符 代码 效果 意义 $\\sum$ $\\sum$ 求和 $\\leq$ $\\leq$ 小于等于 $\\geq$ $\\geq$ 大于等于 $\\neq$ $\\ne$ 不等于 $\\gt$ $\\gt$ 大于 $\\lt$ $\\lt$ 小于 $\\mu$ $\\mu$ $\\sigma$ $\\sigma$ 2.4 详细其他参考内容2.4.1 希腊字符 代码 效果 $\\alpha$ $\\alpha$ $\\Alpha$ $\\Alpha$ $\\beta$ $\\beta$ $\\Beta$ $\\Beta$ $\\gamma$ $\\gamma$ $\\Gamma$ $\\Gamma$ $\\delta$ $\\delta$ $\\Delta$ $\\Delta$ $\\epsilon$ $\\epsilon$ $\\Epsilon$ $\\Epsilon$ $\\zeta$ $\\zeta$ $\\Zeta$ $\\Zeta$ $\\eta$ $\\eta$ $\\Eta$ $\\Eta$ $\\theta$ $\\theta$ $\\Theta$ $\\Theta$ $\\iota$ $\\iota$ $\\Iota$ $\\Iota$ $\\kappa$ $\\kappa$ $\\Kappa$ $\\Kappa$ $\\lambda$ $\\lambda$ $\\Lambda$ $\\Lambda$ $\\mu$ $\\mu$ $\\Mu$ $\\Mu$ $\\nu$ $\\nu$ $\\Nu$ $\\Nu$ $\\xi$ $\\xi$ $\\Xi$ $\\Xi$ $\\omicron$ $\\omicron$ $\\Omicron$ $\\Omicron$ $\\pi$ $\\pi$ $\\Pi$ $\\Pi$ $\\Rho$ $\\Rho$ $\\Rho$ $\\Rho$ $\\sigma$ $\\sigma$ $\\Sigma$ $\\Sigma$ $\\tau$ $\\tau$ $\\Tau$ $\\Tau$ $\\upsilon$ $\\upsilon$ $\\Upsilon$ $\\Upsilon$ $\\phi$ $\\phi$ $\\Phi$ $\\Phi$ $\\chi$ $\\chi$ $\\Chi$ $\\Chi$ $\\psi$ $\\psi$ $\\Psi$ $\\Psi$ $\\omega$ $\\omega$ $\\Omega$ $\\Omega$ 2.4.2 集合与比较 代码 效果 意义 $\\not\\subset$ $\\not\\subset$ 不包含于 $\\subset$ $\\subset$ 子集，包含于 $\\supset$ $\\supset$ 超集 $\\subseteq$ $\\subseteq$ 真子集 $\\subseteq$ $\\subseteq$ $\\emptyset$ $\\emptyset$ 空集 $\\in$ $\\in$ 属于 $\\notin$ $\\notin$ 不属于 $\\bigcup$ $\\bigcup$ 并集 $\\bigcap$ $\\bigcap$ 交集 $\\mathbb{R}$ $\\mathbb{R}$ 实数集 $\\not$ $\\not$ 非 $\\bigvee$ $\\bigvee$ 逻辑或 $\\bigwedge$ $\\bigwedge$ 逻辑与 $\\nleq$ $\\nleq$ 不小于等于 $\\ngeq$ $\\ngeq$ 不大于等于 $\\approx$ $\\approx$ 约等于 $\\equiv$ $\\equiv$ 恒等于 $\\infty$ $\\infty$ 无穷 $\\sim$ $\\sim$ 相关符号，例如正太分布 $X \\sim N(\\mu,\\sigma^2)$ 2.4.3 矩阵使用 \\begin{matrix}开头及\\end{matrix}结尾，每行 \\\\结尾，每个元素 &amp;分隔。此外向量的表示是 $\\vec{}$，例如 $\\vec{a}$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647$$\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{matrix}$$$$\\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{pmatrix}$$$$\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{bmatrix}$$$$\\begin{Bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{Bmatrix}$$$$\\begin{vmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{vmatrix}$$$$\\begin{Vmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{Vmatrix}$$ $$\\begin{matrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\\\end{matrix}$$$$\\begin{pmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\\\end{pmatrix}$$$$\\begin{bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\\\end{bmatrix}$$$$\\begin{Bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\\\end{Bmatrix}$$$$\\begin{vmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\\\end{vmatrix}$$$$\\begin{Vmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\\\end{Vmatrix}$$ 2.4.4 数学运算符号 代码 效果 意义 $\\times$ $\\times$ 乘法 $\\div$ $\\div$ 除法 $\\pm$ $\\pm$ 正负号 $\\mp$ $\\mp$ 负正号 $\\cdot$ $\\cdot$ 点乘 $\\ast$ $\\ast$ 星乘 $\\frac{}{}$ $\\frac{1}{2}$ 分数表示法 $\\log$ $\\log$ 对数运算 $\\ln$ $\\ln$ 自然对数运算 $\\sin$ $\\sin$ 正弦函数 $\\sum$ $\\sum$ 累加 $\\prod$ $\\prod$ 累乘 $\\coprod$ $\\coprod$ 余积 $\\int$ $\\int$ 积分 $\\iint$ $\\iint$ 二次积分 $\\oint$ $\\oint$ 曲线积分 $\\lim$ $\\lim$ 极限 $\\partial$ $\\partial$ 导数 $\\nabla$ $\\nabla$ 梯度 $\\sqrt$ $\\sqrt{ax+b}$ 求平方根 $\\bigotimes$ $\\bigotimes$ 克罗内克积 $\\bigoplus$ $\\bigoplus$ 异或 $\\bigodot$ $\\bigodot$ 加运算 $\\because$ $\\because$ 因为 $\\therefore$ $\\therefore$ 所以 $\\forall$ $\\forall$ 任意 $\\exists$ $\\exists$ 存在 2.4.5 其他类型 代码 效果 意义 $\\ldots$ $\\ldots$ 底部省略号 $\\cdots$ $\\cdots$ 中部省略号 $\\hat$ $\\hat{y}$ $\\check$ $\\check{y}$ $\\breve$ $\\breve{y}$ $\\circ$ $90^\\circ$ 可以用于表示度数 $\\overline$ $\\overline{a+b+c}$ 用于表示平均值 $\\underline$ $\\underline{a+b}$ $\\uparrow$ $\\uparrow$ $\\Uparrow$ $\\Uparrow$ $\\backslash$ $\\backslash$ $\\Updownarrow$ $\\Updownarrow$ $\\Rightarrow$ $\\Rightarrow$ $\\Longleftarrow$ $\\Longleftarrow$ $\\overbrace$ $\\overbrace{a + b}$ 上方括号，见下方示例 $\\mid$ $\\mid$ 竖线 $\\lbrace \\rbrace$ $\\lbrace \\rbrace$ 花括号 示例： $\\overbrace{a+\\underbrace{b+c}_{1.0}+d}^{2.0}$ 结果为 $\\overbrace{a+\\underbrace{b+c}_{1.0}+d}^{2.0}$ 上花括弧命令：\\overbrace{公式}{说明} 下花括弧命令：\\underbrace{公式}_{说明} 例如：$\\underbrace{a+\\overbrace{b+\\dots+b}^{m\\mbox{ 个}}}_{20\\mbox{ 个}}$显示为：$\\underbrace{a+\\overbrace{b+\\dots+b}^{m\\mbox{ 个}}}_{20\\mbox{ 个}}$ 2.4.6 特殊格式 堆积符号 \\stacrel{上位符号}{基位符号} 基位符号大，上位符号小 {上位公式\\atop 下位公式} 上下符号一样大 {上位公式\\choose 下位公式} 上下符号一样大；上下符号被包括在圆弧内 示例 $\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}\\\\ {n+1 \\choose k}={n \\choose k}+{n \\choose k-1}\\\\ \\sum_{k_0,k_1,\\ldots&gt;0 \\atop k_0+k_1+\\cdots=n}A_{k_0}A_{k_1}\\cdots $ 结果为 $\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}\\ {n+1 \\choose k}={n \\choose k}+{n \\choose k-1}\\ \\sum_{k_0,k_1,\\ldots&gt;0 \\atop k_0+k_1+\\cdots=n}A_{k_0}A_{k_1}\\cdots$ 定界符 通过 \\big , \\Big, \\bigg, \\left, \\right 等进行控制， 例如 $()\\big(\\big)\\Big(\\Big)\\bigg(\\bigg)\\Bigg(\\Bigg)$，结果为$()\\big(\\big)\\Big(\\Big)\\bigg(\\bigg)\\Bigg(\\Bigg)$。此外示例 $\\left(x\\right) \\left(x^{y^{\\scriptstyle z}}\\right)$ 结果为 $\\left(x\\right) \\left(x^{y^{\\scriptstyle z}}\\right)$ 占位宽度 \\qquad 表示两个 quad 空格 $a \\qquad b $ 结果显示为：$a \\qquad b $ \\quad 表示一个 quad 空格 $a \\quad b$ 结果显示为 $a \\quad b$ 一个空格 $a\\ b$ 表示 1/3m宽度，结果显示为：$a\\ b$ ; 表示 $a\\;b$ 占 2/7m宽度，结果显示为：$a;b$ , 表示 1/6m宽度$ a\\,b$, 显示为：$ a,b$ 没有占位控制，$ab$, 显示为：$ab$ ! 表示紧贴缩进1/6m宽度$a\\!b$, 显示为：$a!b$ Note: 其他格式控制可以参考 \\display, \\textstyle, \\scripstyle 3. 参考 Markdown 维基百科 教程MarkDown markdown 中文语法详细内容，对应的英文版内容可以参考 Daring Fireball: Markdown Syntax Documentation Markdown 数学公式 Display style in math mode - Overleaf, Online LaTeX Editor How To Write Mathematical Equations, Expressions, and Symbols with LaTeX: A cheatsheet. 罗列了怎么书写数据表达式，并且有相关的分类","link":"/%5Bobject%20Object%5D/Markdown%E7%AC%94%E8%AE%B0/"},{"title":"Python 数据处理技巧","text":"本篇笔记记录使用 Python 进行数据处理的相关技巧: Pandas 缺失数据处理 数据类型转换 1. Pandas 处理缺失数据Pandas 常用的处理缺失数据处理为特定数据值、特定字段指定数据值填充。当处理到两个表数据更新缺失值时，就需要使用其他方法。 1.1 combine_first 填充不同数据源中均有一些缺失值，那么如何让他们相互“弥补”这些缺失值呢？可以考虑使用 combine_first 12345678In [43]: df1 = pd.DataFrame([[np.nan, 3., 5.], [-4.6, np.nan, np.nan], .....: [np.nan, 7., np.nan]]) .....: In [44]: df2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5., 1.6, 4]], .....: index=[1, 2]) .....: In [45]: result = df1.combine_first(df2) 例如上图中根据行列对应的值，df2 的值填充到得了 df1 缺失部分。 1.2 update 更新上述方法仅仅填充了缺失值，如果需要把所有的值进行更新替换，可以考虑使用 update，这次替换的不仅仅是NaN值，还有其他所有能在右表中找到的相同位置的值。 1In [46]: df1.update(df2) 2. 数据类型Pandas 读取数据之后不一定能够符合需要或者数据字典说明，因此需要通过相应的方法将数据类型进行转化。 2.1 astype 转换 Pandas 可以使用 astype 进行强制类型转换，转换需要具有几个常用条件：1）数据整洁性，这样可以容易被转换；2）数值类型转换字符串类型比较容易。但是需要注意 ValueError 报错类型： 1234# 报错：因为对于某些字符串类型或者逗号分隔无法顺利进行数值类型转换ValueError: could not convert string to float: '$15,000.00'ValueError: invalid literal for int() with base 10: 'Closed' 这个是说强之类型转换失败，上面两个错误：第一个是因为第一个有逗号及单位符号；第二个错误是因为 cloed 是字符串。 ⚠️另外需要注意的是，当使用 astype('bool') 转换为逻辑类型的时候，非空字符串都会被转换为 True 2.2 apply 调用函数该方法比较 elegent，因为它解决了 astype 对数据不整洁的转化报错的问题。其 核心逻辑 是利用利用函数（同样可以使用 lambda 匿名函数来解决问题）将数据直接解析转换为另一个数据类型。 123456789101112def convert_currency(val): &quot;&quot;&quot; Convert the string number value to a float - Remove $ - Remove commas - Convert to float type &quot;&quot;&quot; new_val = val.replace(',','').replace('$', '') return float(new_val) # 使用 apply 方法调用函数df['2016'].apply(convert_currency) 2.3 读取数据参数使用在明确某些字段是什么类型的情况下，可以在读取阶段通过参数调整。核心逻辑 是 pandas 的 read 方法中的 converters 参数，下面是 read_csv 示例 1234567# convert_currency 是一个函数df = pd.read_csv(&quot;sales_data_types.csv&quot;, dtype={'Customer Number': 'int'}, converters={'2016': convert_currency, 'Jan Units': lambda x: pd.to_numeric(x, errors='coerce'), 'Active': lambda x: np.where(x == &quot;Y&quot;, True, False) }) 2.4 特定功能性接口功能性方法是 pandas 自带方法，它们可以针对某些类型的数据进行转换，例如：pd.to_datetime()、pd.to_numerical","link":"/%5Bobject%20Object%5D/Python-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/"},{"title":"SQL 使用总结笔记","text":"1. Sqlite 导入 csv 数据不准确如果需要导入 csv 文件，需要调整 sqlite 模式并且需要申明分割符，步骤如下： 1234567891011121314# terminal 打开或者新建文件 data.sqlite3sqlite3 data.sqlite3-- 需要申明模式.mode csv-- 说明分隔符类型，例如 ;、, 等，这里申明为 逗号.separator &quot;,&quot;-- 导入数据 data.csv 为 data 表.import data.csv data-- 确认已经正确导入.schema data 2. 查询 schema 信息因为不同 DBMS 可以使用不同方法，但下面这个语句是一个比较通用的方法[^1]： 123-- 表名称需要加引号，可以查询列名称、数据类型、最大长度select column_name, data_type, character_maximum_lengthfrom INFORMATION_SCHEMA.COLUMNS where table_name = '&lt;name of table&gt;'; 其他的工具，例如 Sqlite3 中可以直接调用表达式来完成——.schema &lt;name of table&gt;；在 MySQL 或者 Mariadb 中可以使用 SHOW CREATE &lt;name of table&gt;。 3. 单引号和双引号使用单引号一般使用于字符串中，用于查询语句中构建的条件。例如： 123SELECT * FROM city_data WHERE country = 'United Kingdom'; 双引号用于表示列名称，例如： 1234-- country city 是一个包含了空格的列名称SELECT * FROM city_data WHERE &quot;country city&quot; = 'United Kingdom'; 4. JOIN 子句SQL 语句使用 JOIN 将多个表进行关联查询——这样可以将信息单独存放到一个结果表，筛选必要的苏剧减少数据存储量以及减少对象不同数据独立。 对于为何使用信息表独立开，这个是涉及到了数据库规范。需要考虑要素： 表存储了逻辑分组的数据 数据修改是否存在多表修改的问题 能快速高效地访问和操作数据 JOIN 子句可以将之前独立表添加到统一表中——主要需要使用 JOIN 来联合两张表而 ON 用于判断需要满足什么条件，可以使用别名来替代标名称。 1234SELECT t1.columns, t2.columnsFROM tb1 [AS] t1JOIN tb2 [AS] t2ON t1.column = t1.column; 示例的表结构如下： accounts 和 orders 表使用 JOIN 查询示例： JOIN 的类型包括 INNER JOIN 和 OUTER JOIN（包括了 LEFT OUTER JOIN 、 RIGHT OUTER JOIN 以及 FULL OUTER JOIN）。不同类型的 JOIN 查询数据文式图： 参考[^1]: psql - PostgreSQL “DESCRIBE TABLE”","link":"/%5Bobject%20Object%5D/SQL%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/"},{"title":"Ubuntu 服务器环境搭建笔记","text":"本文是对搭建数据分析等服务器环境，包括了在搭建过程中的大致步骤以及搭建环境使用的主要工具。 工具以及设备基础： 必要的硬件需求，例如本次使用了双机械硬盘作为存储方案 USB 启动盘工具 Rufus 系统 Ubuntu 20.4 1.安装 Ubuntu 系统 使用 Rufus 工具制作 Ubuntu 的 USB 启动盘 使用 U 盘启动过程，可以参考Install Ubuntu desktop | Ubuntu 。安装过程中需要注意分区选择主分区（Primary）还是逻辑分区，其中逻辑分区是挂在扩展分区（Extended）上。因为分区表中有四个分区，目录挂载方案中需要采用最多主分区方案是使用 P+P+P+E 的方案。 /boot 用于存放 启动程序 的位置，容量不用太大，配置了 200M。在选择启动引导器设备时，可以选择该挂载点的设备 Swap 根据内存的 2 倍进行配置 添加 保留BIOS启动区域，在 Ubuntu 中是一个特殊的用途空间，用于解决UEFI启动模式，该空间的文件类型biosgrub 其他的目录挂在根据需要和设备情况配置即可，整体的分区方案可以参考如下（安装环境是双机械硬盘 500G + 320G，320G 的设备 /dev/sdb 主要用于存储文档和用户目录。双硬盘都预留了未分区部分）： 分区/挂载 容量(Mb) 分区类型 分区用处 设备 /boot 2048 主分区 ext4 日志文件系统 /dev/sda EFI 分区 200 主分区 EFI 系统分区 /dev/sda biosgub 200 主分区 保留 BIOS 启动区域 /dev/sda Swap 20480 逻辑分区 SWAP 交换分区 /dev/sda / 102400 逻辑分区 ext4 日志文件系统 /dev/sda /var 71680 逻辑分区 ext4 日志文件系统 /dev/sda /usr 71680 逻辑分区 ext4 日志文件系统 /dev/sda /usr/local 20480 逻辑分区 ext4 日志文件系统 /dev/sda /opt 51200 逻辑分区 ext4 日志文件系统 /dev/sda /Documents 102400 逻辑分区 ext4 日志文件系统 /dev/sdb /home 102400 逻辑分区 ext4 日志文件系统 /dev/sdb 安装过程是直接安装了 GUI 界面，后续操作是通过命令行。可以根据需要选择是否开启图形界面： 开机默认进入命令行模式，输入命令：sudo systemctl set-default multi-user.target 。重启完成之后使用 Ctrl+Alt+F2~F6 可以选择使用不同的 tty。临时进入 GUI 可以输入 startx 之后可以使用 Ctrl+Alt+F7 进入图形界面 开机默认进入图形界面，使用命令 sudo systemctl set-default graphical.target 重启之后使用 Ctrl+Alt+F7 命令之后进入图形界面 2. Ubuntu 配置2.1 增加 root 用户Ubuntu 安装时没有配置 root 账户和密码，需要使用 root 用户时需要先配置root 使用 sudo passwd root 命令，输入 root 用户密码 使用 sudo vim /usr/share/lightdm/lightdm.conf.d/50-ubuntu-conf 后，在文件末尾添加 greeter-show-manual-login=true 以允许使用 root 用户登陆 2.2 SSH 配置 ssh 服务安装 使用命令 sudo apt install openssh-server 安装 ssh 客户端，使用 sudo apt install openssh-client 安装 ssh 服务端 修改 ssh 登陆端口，ssh 默认使用 22 端口，可以修改为其他端口。通过修改 /etc/ssh/sshd_config 文件修改 Port 的值 调整防火墙配置以允许 ssh 端口可以访问，通过命令 sudo ufw allow &lt;port&gt; 配置端口以及通过命令 sudo ufw reload 重启防火墙 进行远程登陆时使用命令申明端口 ssh -p &lt;port&gt; &lt;user&gt;@host 进行登陆 3. 开发环境搭建3.1 安装 anaconda国内下载安装源可以通过 anaconda 清华大学开源软件镜像站 | Tsinghua Open Source Mirror 下载以及添加 conda 国内的 channels: 使用 conda config --set show_channel_urls yes 添加 conda 配置文件 .condarc 使用 conda config --add channels &lt;url&gt; 添加需要的镜像地址 3.2 安装 RStudioRStudio 除了常用的本地 IDE 版本外，可以安装 RStudio Server 来实现浏览器方式使用 RStudio，该方法可以实现远程使用 RStudio。安装 RStudio Server 步骤基本上根据选择的平台之后，使用相应的命令即可完成。以下步骤为 Ubuntu 安装方法： 使用 sudo apt-get install r-base 安装基础的 R 应用 安装 server 需要使用 gdebi 工具，因此需要先使用命令 sudo apt-get install gdebi-core 下载选定的 server 包之后使用 sudo gdebi &lt;server_file&gt; 安装 默认的登陆端口是 8787，因此需要开放该端口以及重在防火墙 sudo ufw allow 8787 ，sudo ufw reload","link":"/%5Bobject%20Object%5D/Ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"title":"Matplotlib 可视化应用基础","text":"笔记是针对 Python 可视化工具 Matplotlib 基本信息总结，针对的版本是 3.3.1 TL;DR 图形是对象，所有对象都是 Artists 图形对象是具有层级关系 图像优化可以选择具体的对象进行分别优化 数据可视化建议，先简单摸索后优化 作为 Python 可视化中的重要工具，matplotlib 有广泛的应用——不仅是一个独立 package，而且常被作为其他可视化依赖工具——例如 pandas 和 seaborn 可视化均有相关依赖。 1. 图形对象matplotlib 中可视化元素都是对象，理解作为对象的方法以及元素对象在结构上的关系是有必要的。下图说明了表明了图形对象结构: 图摘自:Usage Guide — Matplotlib documentation 上图描述了图形的主要对象包括： Title： 图形标题 Spines： 图形边框 Axis： 图形坐标轴，包括 X 轴和 Y 轴 Axis label： 坐标轴标签 Axis tick： 坐标轴刻度 Tick label： 坐标轴刻度标签 Legend： 图形图例，用于说明图形某些信息，例如 circle 大小、颜色等对应数据等关系 其他 Artists：这个是一个泛指统称，指在绘图中表达的数据信息以及表达形式，例如散点图中的 circle，线性图中的 line，直方图中的 bar 这些信息也说明了，在进行绘图时需要考虑到有哪些对象可以使用，即可以被用于制图 1.1 图形对象层次关系以上的图说明了图形有哪些对象，应用上需要对图形的对象之间的关系进行一个初步认识： 图摘自:Python Plotting With Matplotlib (Guide) – Real Python 上图说明了图形对象的层次的关系： Figure：所有的图形对象都包括在该对象中，由其他对象组成构成了一个完整的图。它记录了所有的子对象包括 Axes 以及 “画布”对象 canvas Axes：包括了整个图形中主要了图形元素对象，例如 标题、坐标轴、图形信息。它包括了所有的 Artists 对象，是具有数据的图像区域 Title： 标题 Axis：坐标轴，用于处理图形中数据范围，它是一个类似线性数据的对象（Number-line-like Objects）。其下有 Locator 对象控制坐标轴刻度位置，Formatter 对象决定坐标轴刻度标签的格式等表现形式 Locator Formatter 其他 Artists：具体图形信息，例如图像的文本信息（Text 对象），图像的线性信息（Line2D 对象）饼状图以及其他变形图形信息（Patch 对象） Artists：所有的 Artist 对象包括了所有在图像中可见的对象（甚至包括 Figure， Axes 和 Axis 对象，包括上面提到的文本信息对象、信息对象等）。所有的 Artists 对象被绘制到“画布”上，当然是在 Figure 对象被渲染时。⚠️注意有些 Artist 对象是被绑定到了 Axes 对象中而不能被共享（例如数据信息对象） 上图部分信息展示了简介对象层次结构，下图更充分展现了完整对象结构： 2. 绘制图形前文对图形对象和对象层级关系进行了简要概述，下文将从实践的方面来了解 matplotlib 的可视化操作。 2.1 创建 Figure 对象Figure 对象是整个图形的“根基”，创建的过程中可以使用 figure、subplots 等多种方法得到实例化的 Figure。几种常用的初始化方法示例: 123456789101112&gt;&gt;&gt; # Load package&gt;&gt;&gt; from matplotlib import pyplot as plt&gt;&gt;&gt; import matplotlib as mpl&gt;&gt;&gt; # figure instance by using figure method&gt;&gt;&gt; fig1 = plt.figure() # 返回一个 Figure &lt;Figure size 432x288 with 0 Axes&gt;&gt;&gt;&gt; # figure instance by using subplots&gt;&gt;&gt; fig2, ax = plt.subplots(1,2)&gt;&gt;&gt; # check the type of figure&gt;&gt;&gt; type(fig1), type(fig2)(matplotlib.figure.Figure, matplotlib.figure.Figure) 以上两种方法中，直接使用 figure 方法相关参数如下： matplotlib.pyplot.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=&lt;class ‘matplotlib.figure.Figure’&gt;, clear=False, **kwargs) MatplotlibAPImatplotlib-pyplot-figure 使用 subplots 方法相关参数如下: matplotlib.pyplot.subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw) MatplotlibAPImatplotlib-pyplot-figure 2.2 使用 Axes 绘图图形的数据信息对象 Artists，可以通过 Axes 对象实现绘制。Axes 对象可以通过前文中 subplots 方法创建 figure 时得到，也可以通过 gca 方法得到该对象，其底层逻辑为： 图摘自:Python Plotting With Matplotlib (Guide) – Real Python 2.2.1 绘制基本图形获取 Axes 对象的常用方式如下： 1234&gt;&gt;&gt; # 方法一，直接通过 subplots 方法得到&gt;&gt;&gt; fig, ax = plt.subplots(*args, **kwargs)&gt;&gt;&gt; # 方法二，通过 gca 方法获取&gt;&gt;&gt; fig = plt.subplot(); ax = plt.gca() 通过 Axes 对象绘图的方式，是使用该对象下绘图方法实现；而其他标题、坐标轴等绘制，需要通过响应的 set 方法实现： 12345678910&gt;&gt;&gt; # 比较常用的是使用 figsize 调整整个图形大小&gt;&gt;&gt; height_width = 6&gt;&gt;&gt; fig = plt.figure(figsize=(height_width, height_width))&gt;&gt;&gt; # 获取 axes 对象&gt;&gt;&gt; ax = plt.gca()&gt;&gt;&gt; # 使用 axes 对象 bar 方法进行绘条形图&gt;&gt;&gt; x = [1, 2, 3, 4]&gt;&gt;&gt; height = [8, 9, 10, 6]&gt;&gt;&gt; ax.bar(x, height)&gt;&gt;&gt; plt.show() 图形效果如下： 2.2.2 修改标题以及轴标签完成图形基本绘制之后，继续对标题、坐标轴标签、刻度标签以及条形图的 Bar 调整： 123456789101112131415&gt;&gt;&gt; # 设置标题，还可以传入其他字体参数用于调整字体大小颜色等，传入 loc 传入可以控制标题位置&gt;&gt;&gt; ax.set_title(&quot;This is test plot&quot;, fontsize=20, color=&quot;red&quot;, loc=&quot;left&quot;)&gt;&gt;&gt; # 设置 x 标签，还可以传入其他字体参数用于调整字体大小颜色等&gt;&gt;&gt; ax.set_xlabel(&quot;X label&quot;, fontsize=16, color=&quot;green&quot;)&gt;&gt;&gt; # 设置 y 标签&gt;&gt;&gt; ax.set_ylabel(&quot;Y label&quot;, fontsize=25, color=&quot;cyan&quot;)&gt;&gt;&gt; # 设置 x 刻度标签&gt;&gt;&gt; ax.set_xticks(x)&gt;&gt;&gt; ax.set_xticklabels([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])&gt;&gt;&gt; # 设置背景颜色&gt;&gt;&gt; ax.set_facecolor(&quot;azure&quot;)&gt;&gt;&gt; # 调用 fig 显示图形&gt;&gt;&gt; fig 图形效果如下： 轴标签的调整涉及到，刻度位置、方向以及文本大小的调整。对于坐标刻度的标签字体等调整，需要使用 axes 对象下的 tick_params 方法，而非 axis 对象下的方法： Axes.tick_params(axis=’both’, **kwargs) axis : {'x', 'y', 'both'}, optional Which axis to apply the parameters to. color : color Tick color; accepts any mpl color spec. pad : float Distance in points between tick and label. labelsize : float or str Tick label font size in points or as a string (e.g., 'large'). labelcolor : color Tick label color; mpl color spec. colors : color Changes the tick color and the label color to the same value: mpl color spec. bottom, top, left, right : bool Whether to draw the respective ticks. labelbottom, labeltop, labelleft, labelright : bool Whether to draw the respective tick labels. labelrotation : float Tick label rotation MatplotlibAPImatplotlib-pyplot-text 使用的示例方法如下： 123456&gt;&gt;&gt; # 设置了 x 轴的颜色，字体大小和旋转&gt;&gt;&gt; ax.tick_params(axis=&quot;x&quot;, colors=&quot;deeppink&quot;, labelsize=12, labelrotation=40) &gt;&gt;&gt; # 设置了 y 轴的字体大小，填充大小以及坐标轴位置&gt;&gt;&gt; ax.tick_params(axis=&quot;y&quot;, labelsize=25, pad=20, labelright=True,right=True, left=False, labelleft=False) &gt;&gt;&gt; fig 示例效果如下: 2.2.3 修改 patch 颜色条形图中的 bar 对象可以通过 patches 属性获取，可以通过 patch 属性的 set_color() 方法修改图形颜色： 123456# 遍历 patch 对象的方式，可以来分别调整每个 bar 的颜色，注意后两个颜色是 16 进制数据colors = [&quot;aquamarine&quot;, &quot;olive&quot;, &quot;#1e90ff&quot;, &quot;#add8e6&quot;]&gt;&gt;&gt; for index, patch in enumerate(ax.patches):&gt;&gt;&gt; patch.set_color(colors[index])&gt;&gt;&gt; fig 图形效果如下: 2.2.4 数据标注将数据信息表达出来，需要使用 Axes 对象来控制 Text 对象，其中参数如下 Axes.text(x, y, s, fontdict=None, withdash=False, **kwargs) x, y : scalars The position to place the text. s : str The text MatplotlibAPImatplotlib-axes-text 使用示例如下： 12345&gt;&gt;&gt; # 需要使用的数据是 x 和 y 的位置以及数据。 y 的位置和数据 s 可以通过之前的 height 得到，x 坐标位置可以通过 x 数据得到&gt;&gt;&gt; for x_index, value in zip(x, height):&gt;&gt;&gt; ax.text(x=x_index-0.2, y=value+0.2, s=&quot;#{}&quot;.format(value), color=&quot;deepskyblue&quot;, fontsize=13) &gt;&gt;&gt; fig 结果如下： 在表达数据信息时，可能遇到不仅仅需要直接表现具体数据，而且需要图例来说明图形信息，因此需要对图形添加 legend。由于之前没有设置 legend 相关信息，需要先从 Axes 对象中得到 legend 信息，它包括了 handles 和 labels 两个对象，之后利用处理完对象之后，使用 legend 方法即可以完成图像图例。 Axes.get_legend_handles_labels(legend_handler_map=None) Return handles and labels for legend MatplotlibAPImatplotlib-axes-text 具体的使用示例如下: 123456789&gt;&gt;&gt; handles, labels = ax.get_legend_handles_labels()&gt;&gt;&gt; for patch in ax.patches:&gt;&gt;&gt; handles.append(patch)&gt;&gt;&gt; labels = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;]&gt;&gt;&gt; # 需要使用 legend 来完成 legend 设置&gt;&gt;&gt; ax.legend(handles, labels, loc=&quot;best&quot;)&gt;&gt;&gt; fig 示例效果如下： 上述是指导绘图的一般方式，而实际应用中方法和调用参数并不完全一致，解决问题的方式是没有完全标准的答案。例如 pandas 中 DataFrame 封装来 plot 方法可以快速绘图，此外还可以直接使用 pyplot 模块中相关方法绘图。 2.3 使用 pyplot 方法绘图从前文可知 Axes 绘图更精细，但是 plot 绘图的方式更快速，两者在图形调整上都具有相通性——绘图和调整图形都需要控制相应的对象 Axes 需要使用 set 等方式，而 plot 可以直接使用对象名称进行绘图。图形优化和调整的方面还是可以考虑之前的方案，使用 plt.gca() 方法来得到 Axes 对象进行调整。 2.3.1 绘制散点图及其调整示例一1234567891011121314151617181920212223242526272829303132333435363738&gt;&gt;&gt; # load package&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt;&gt;&gt;&gt; x = np.random.randint(100, size=40)&gt;&gt;&gt; y = np.array([i + np.random.randint(-10, 10) for i in x])&gt;&gt;&gt; &gt;&gt;&gt; y_series = pd.Series()&gt;&gt;&gt; marker = y_series.apply(lambda i: 'd' if i &gt; 50 else 'o')&gt;&gt;&gt; color = [&quot;fuchsia&quot; if i &gt; 10 else &quot;black&quot; for i in y]&gt;&gt;&gt; height_width = 5&gt;&gt;&gt; fig = plt.figure(figsize=(height_width, height_width))&gt;&gt;&gt; &gt;&gt;&gt; plt.scatter(x, y, c=color)&gt;&gt;&gt; # 设置标题&gt;&gt;&gt; plt.title(&quot;Test the scatter&quot;, fontsize=20, loc=&quot;left&quot;, color=&quot;cyan&quot;)&gt;&gt;&gt; &gt;&gt;&gt; # 设置轴标签&gt;&gt;&gt; plt.xlabel(&quot;Xlabel&quot;, fontsize=16, color=&quot;green&quot;)&gt;&gt;&gt; # 图像中中文字体乱码的解决可以通过传入 family 参数，值为系统中已有的中文字体名称即可&gt;&gt;&gt; plt.ylabel(&quot;Ylabel 标签&quot;, fontsize=12, color=&quot;grey&quot;)&gt;&gt;&gt; # plt.ylabel(&quot;Ylabel 标签&quot;, fontsize=12, color=&quot;grey&quot;, family=&quot;STHeiti&quot;)&gt;&gt;&gt; &gt;&gt;&gt; # 设置坐标轴刻度范围&gt;&gt;&gt; plt.xlim(-10, 150)&gt;&gt;&gt; plt.ylim(-10, 120)&gt;&gt;&gt; &gt;&gt;&gt; # 设置坐标轴刻度标签字体&gt;&gt;&gt; plt.tick_params(axis=&quot;x&quot;, labelsize=10, labelcolor=&quot;blue&quot;)&gt;&gt;&gt; &gt;&gt;&gt; # 设置 figure 和 axes 背景颜色&gt;&gt;&gt; fig.set_facecolor(&quot;azure&quot;)&gt;&gt;&gt; &gt;&gt;&gt; ax = plt.gca()&gt;&gt;&gt; ax.set_facecolor(&quot;beige&quot;)&gt;&gt;&gt; &gt;&gt;&gt; plt.show() 结果示例： 2.3.2 绘制散点图以及调整示例二1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; height_width = 5&gt;&gt;&gt; fig = plt.figure(figsize=(height_width, height_width))&gt;&gt;&gt; &gt;&gt;&gt; plt.scatter(x, y, c=color)&gt;&gt;&gt; # 设置标题&gt;&gt;&gt; plt.title(&quot;Test the scatter&quot;, fontsize=20, loc=&quot;left&quot;, color=&quot;cyan&quot;)&gt;&gt;&gt; &gt;&gt;&gt; # 设置轴标签&gt;&gt;&gt; plt.xlabel(&quot;Xlabel&quot;, fontsize=16, color=&quot;green&quot;)&gt;&gt;&gt; # 图像中中文字体乱码的解决可以通过传入 family 参数，值为系统中已有的中文字体名称即可&gt;&gt;&gt; # plt.ylabel(&quot;Ylabel 标签&quot;, fontsize=12, color=&quot;grey&quot;)&gt;&gt;&gt; plt.ylabel(&quot;Ylabel 标签&quot;, fontsize=12, color=&quot;black&quot;, family=&quot;STHeiti&quot;)&gt;&gt;&gt; &gt;&gt;&gt; # 设置坐标轴刻度范围&gt;&gt;&gt; plt.xlim(-10, 150)&gt;&gt;&gt; plt.ylim(-10, 120)&gt;&gt;&gt; &gt;&gt;&gt; # 设置坐标轴刻度标签字体&gt;&gt;&gt; plt.tick_params(axis=&quot;x&quot;, labelsize=10, labelcolor=&quot;blue&quot;)&gt;&gt;&gt; &gt;&gt;&gt; # 设置 figure 和 axes 背景颜色&gt;&gt;&gt; fig.set_facecolor(&quot;azure&quot;)&gt;&gt;&gt; &gt;&gt;&gt; ax = plt.gca()&gt;&gt;&gt; ax.set_facecolor(&quot;beige&quot;)&gt;&gt;&gt; &gt;&gt;&gt; # 删除边框，需要选择不同位置对 spines 分别调整——设置不可以见，即用 set_visible 方法传入 False 值&gt;&gt;&gt; for loc in [&quot;top&quot;, &quot;left&quot;, &quot;right&quot;, &quot;bottom&quot;]:&gt;&gt;&gt; ax.spines[loc].set_visible(False)&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; plt.show() 结果示例: 参考 Matplotlib User’s Guide Python Plotting With Matplotlib (Guide) – Real Python RGB颜色值与十六进制颜色码转换工具","link":"/%5Bobject%20Object%5D/Python-Matplotlib%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BA%94%E7%94%A8%E5%9F%BA%E7%A1%80/"},{"title":"时间序列分析背景","text":"一般进行数据分析通过步骤包括： 步骤1：定义问题 通常这是预测中最困难的步骤。要准确定义这个问题，需要了解怎样运用预测方法，谁需要这个预测，以及预测效果如何满足需要这个预测的机构。预测人员需要花费一定时间与所有参与收集数据、维护数据库和使用这个预测对未来进行规划的人沟通。 步骤2：收集信息 一般至少需要两种信息收集方式：(a) 统计数据，(b) 收集数据和进行预测方面专家的积累经验。通常情况下，要获得足够多的历史数据以构建良好的统计模型是很困难的。在这种情况下，可以判断预测方法。有时候，陈旧数据会因相应数据发生结构变化而失效，因而我们一般只选择使用较新的数据。然而，一个好的统计模型可以处理系统中的结构变化，因此不要轻易丢弃好的数据。 步骤3：初步（探索性）分析 总是以图形开头。有一致的模式吗？有明显的长期趋势吗？季节性重要吗？是否有证据表明商业周期存在？数据中是否包含需要专业知识解释的异常值？用于分析的变量之间的相关性有多强？目前已经开发了各种工具来帮助进行这种分析。 步骤4：选择及拟合模型 最佳模型的选择取决于历史数据的可用性、预测变量与各解释变量之间的相关性，以及预测的使用方式。比较两个或三个潜在的模型是很常见的。每个模型本身都基于人为提出的一组假设(显式和隐式)而建立，通常包含一个或多个参数，这些参数必须使用已知的历史数据进行估计。我们将讨论回归模型、指数平滑方法、Box-Jenkins ARIMA模型、动态回归模型、分层预测，以及其他各种方法，包括计数时间序列、神经网络和向量自回归。 步骤5：使用及评估预测模型 一旦模型及其参数确定后，该模型就可以用来进行预测。模型的预测效果只有用于预测的数据得到之后才能得到正确的评价。目前已经开发了许多方法来评估预测的准确性。在使用和进行预测时会存在很多组织结构问题。 1. 时序模型而在应用时间序列分析的模型包括，统计模型以及序列模型（例如, RNN）。 1.1 统计模型在常用的时序模型中，常用的类型包括直接用预测变量 、前期结果直接预测以及前期结果加预测变量预测——三种类型在函数 $\\ref{1.1}$ 有表达。其中数学表达式为：$$\\begin{equation}Y_{t+1} = \\label{1.1}\\begin{cases}f(X_{(t+1,1)}, X_{(t+1,2)},…, X_{(t+1,n)}, \\text{bias}) \\f(Y_t, Y_{t-1},…, \\text{bias}) \\f(Y_t, X_{(t+1,1)}, X_{(t+1,2)},…, X_{(t+1,n)}, \\text{bias})\\end{cases}\\end{equation}$$在上式 $\\ref{1.1}$ 中第二个等式表示影响系统的因素没有外部因素，第三个等式是前两个的混合模型，应用中可以被称作动态回归模型、面板数据模型以及纵向模型。 应用中基于规则的模型： 使用已知观测数据，直接用于预测未知数据: $\\hat{y}_{t+1}=y_t$。特点是使用的信息少 移动平均模型，利用过去 $k$ 时间段的数据预测未知数据: $\\hat{y}{t+1}=\\frac{1}{k}\\displaystyle\\sum{i=t-k}^t y_i$ 单变量统计模型，利用时序数据自身 ${y_t, y_{t-1},…,y_2,y_1}$ 预测未知的数据 $y_{t+1}$，同时会引入外部数据信息进行相关分析：$y_{t+1}\\leftarrow {y_t,y_{t-1},…,y_2,y_1, X_t,X_{t-1},…,X_2,X_1}$ ，其中 $X$ 为外部数据。该类型的模型多个类别： ARMA 和 ARIMA 两个模型都分别包括自身数据项和误差项构成模型 多元时间序列模型，通过滞后的 $p$ 数据以及外部数据，对 $K$ 个序列进行分析。这样模型实际是对 $K$ 个序列进行了分别建模。模型效果如下： 上述表示的是模型的结果，对于理解 $K$ 个序列数据可能不方便，参考下图 $K$ 个序列可以看作是下图中 $K$ 个线\u0012[^1]： 在实际应用中需要特别注意数据序列之间存在相关性。 VAR 向量自回归模型，在使用时需要注意使用条件：1）$K$ 个序列的模型分析，因为它能创建序列相关模型；2）仅能对迟滞化序列（Stationary Series）建模；3）对于非迟滞化序列需要通过简单差（Differencing）分转换为迟滞化序列 在使用 Python 的进行计算 VAR 过程中，需要指定 order 参数用于指定 lag ： 123model = sm.tsa.VARMAX(y, order=(3, 0), tread=&quot;c&quot;)model_result = model.fit(maxiter=1000, disp=False)model_result.summary() 在模型训练完之后，可以对模型进行绘图诊断，直接使用 model_result.plot_diagnostics()。对模型进行筛选主要是针对 oder 参数 贝叶斯结构化时间序列模型，解决一个系列的模型而非单一模型，以及在解决数据较少需要冷启动时可以应用 1.2 RNN 模型 2. 其他说明 传统统计学和机器学习模型在时序分析中的缺点是没有完全充分利用数据 时序建模的一般情况下期望数据是 Stationarity，即具有相对稳定的周期性变化。相对稳定的数据，具有比较好的统计学属性，可以方便进行建模预测 关于 Dickey-Fuller 测试，零假设是数据不是 Stationary，这点和常用的零假设具有一些差异 Term Horizon: 指需要预测的数据范围, $\\hat{y}_{t+h}$，其中 $h$ 表示预测的范围","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90part1/"},{"title":"SQLAlchemy技巧","text":"SQLAlchemy 是为 Python 编程语言提供的开源 SQL 工具包及对象关系映射器，在关系型数据管理中有广泛应用。该文是包括了一些使用过程中的一些总结经验。包括了以下相关内容 添加日期自动更新的 triger Decimal 控制数据不一致 1. 日期自动更新在创建数据库时，某些事务发生更新需要同步更新时间戳。在创建 Table 时，SQLAlchemy 默认是没有让日期发生自动更新。因此在使用更新时需要调整参数，可以使用 sqlalchemy 中的 func 模块中日期处理方法。具体的示例代码如下： 123456789101112131415161718192021222324252627282930import sqlalchemyimport datetimefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import func# declarative BaseBase = declarative_base()class TempItem(Base): &quot;&quot;&quot;外源信息临时表 外源信息临时表，用于存储需要影视信息种子源。表名是 `series_temp`，schema 信息可以通过`douban_series.sql` 文件中确认 &quot;&quot;&quot; __tablename__ = &quot;series_temp&quot; id = sqlalchemy.Column(sqlalchemy.BigInteger, primary_key=True, autoincrement=True) series_id = sqlalchemy.Column(sqlalchemy.VARCHAR(length=20, convert_unicode=True), nullable=False, unique=True) title = sqlalchemy.Column(sqlalchemy.VARCHAR(150, convert_unicode=True, collation=&quot;utf8mb4&quot;), nullable=False) main_tag = sqlalchemy.Column(sqlalchemy.VARCHAR(10, convert_unicode=True, collation=&quot;utf8mb4&quot;), nullable=True) status = sqlalchemy.Column(sqlalchemy.Boolean, nullable=False, default=False) create_time = sqlalchemy.Column(sqlalchemy.DateTime, nullable=False, server_default=func.now()) update_time = sqlalchemy.Column(sqlalchemy.DateTime, nullable=False, server_default=func.now(), onupdate=func.now()) def __repr__(self): format = &quot;&lt;Data Model Object: %s at %s&gt;&quot; return format % (self.__class__.__tablename__, hex(id(self))) def __str__(self): string = f&quot;Item data:\\n\\t ID: {self.id} Series_ID: {self.series_id}&quot; + \\ f&quot; Title: {self.title} Status: {self.status}&quot; return string 另一种方法是直接将 使用 Column 中 default 参数是调整为 default=datetime.datetime.now，注意使用传入的是一个方法，而非实际值（未加括号） 2. 解决数据写入读取一致在写入数据时可能出现 123.456789，但存储到数据库以及读取数据时变为 123.457。该问题主要时因为数据精度的问题，常规来说数据类型可以使用 Double 替换 Float 方式解决，但也会带来数据占据位数增加的问题。推荐使用定点类型（DECIMAL），DECIMAL 有两个参数,第一个参数用于指定一共多少位数，第二个参数用于指定小数点后最多多少位数。DECIMAL(4,2) 表示一共存储4位数字，小数点后最多有两位","link":"/%5Bobject%20Object%5D/Python-SQLAlchemy%E6%8A%80%E5%B7%A7/"},{"title":"自然语言语言模型","text":"1. 背景语言模型，其中最重要的是能够表达一个语言序列，而在验证序列是否是“自然的”。对应一个句子序列 $P(E)=P(w_1, w_2, \\cdots, w_n)$ 的联合概率表示的是序列中从第一个单词到最后一个单词到序列，使用这种形式直接创建这个分布概率模型的难度较大，因此采用另一种形式——以单词组合的方式作为第二方案。￼以 “she went home” 为例子，改用条件概率的模式——表达计算单词随着单词变化的概率，这样间接实现序列概率计算。在这个例子中，明确使用 表示句子结尾，这样可以用于最终判断在 “she went home” 之后是否结束。因此对应的概率表达式为： $P(E)=P(w_1)P(w_1|w_2)P(w_3|(w_1,w_2))=\\prod_{(t=1)}^{(T=1)}P(w_t|w_1^{t-1}), w_{T+1} \\text{是}$ 1.1 基于计数的 N-Gram 语言模型基于计数的语言模型，是以最大似然数估计的方式来构建模型（Maximum Likelihood Estimation)： $P_{ML}{(w_t|w_1^{t-1})}=\\frac{C(w_1^{t})}{C(w_1^{(t-1)})^*}$上面的意思中表示的是以出现第一个单词 $w_1$ 到 $w_{(t-1)}$ 的前序（prefix）情况下，下一个单词为 $w_t$。需要注意这样创建的语言模型是基于已经遇到过的数据，因此存在“过拟合”的问题。￼假设用上面构建的模型去预测”i am from utah”，其结果会是 0，因为根据上面的例子 $P(w_4=\\text{utah}|(w_1=\\text{i}, w_2=\\text{am}, w_3=\\text{from})$ 是 0，导致整个结果是 0。因此这种模式直接就失去了语言模型评估语言的“自然性”的意义。为了解决这种问题，目前有两种解决方案：1）马尔可夫假设类似的原理，当前单词的出现仅由最近出现的单词决定——采用 N-gram。N-gram 模型参数由给定的 N - 1 个单词确定下一个单词，例如 unigram，的参数仅由自身确定，bigram 是由上一个单词确定，而 trigram 是由上两个单词确定。根据这个假设更新公式 $P(w_t|w_1^{t-1})\\approx P_{ML}(w_t|w_{t-n+1}^{t-1})=\\theta_{w_{t-n+1}^t}$，其中 $n$ 即表示是 Gram 是几元，而 $\\theta$ 即是对应于 N-Gram 下给定的前 $n-1$ 个词条件下的下一个单词的概率。 不论是上面提到的那种方法，都可能存在从训练语料中得到组合存在限制，测试时可能出现某些组合不存在的问题。为了避免和出现 概率值情况时，导致整个句子序列的概率为 ，因此引入了 平滑（Smoothing）的方式来修正该问题。例如其中最简洁的方式——插值法（Interpolation），直接将 Unigram 和 Bigram 合并考虑以达到平滑的效果:$P(w_t|w_{t-1})=(1-\\alpha)P_{ML}(w_t|w_{t-1})+\\alpha P_{ML}(W_t)$。这种方式是相对来说更稳健地构建低频词概率模型之一。对于需要更复杂的上下文语言模型，例如 N 为 3，4 等的条件的概率需要使用递归的方式来构建$$\\rm{对应于 TriGram 语法概率} \\P(w_t|w_{t-2}w_{t-1})=\\alpha_1 P(w_t|w_{t-2}w_{t-1}) +\\alpha_2 P(w_t|w_{t-1}) +\\alpha_3 P(w_t) \\\\displaystyle \\sum_{i}\\alpha_i = 1$$其中 $\\alpha$ 是作为超参数，在训练过程中是使用保留语料库（Held-out corpus）中学习，是附加的训练语料库——但并没有用于设置 N Gram 语法的计数，是用于设置其他的参数。因此在处理数据上通过训练语料嫁给你 N Gram 语法的概率固定下来，在通过保留语料库训练 $\\alpha$ 1.1.1 Bigram 模型示例 &lt;s&gt; I am sam &lt;/s&gt; &lt;s&gt; Sam I am &lt;/s&gt; &lt;s&gt; I do not like green eggs and ham &lt;/s&gt; 根据 NGram 公式 $P(w_t|w_1^{t-1})\\approx P_{ML}(w_t|w_{t-n+1}^{t-1})=\\frac{C(w_{t-n+1}^{t-1}w_t)}{C(w_{t-n+1}^{t-1})}$ ，右侧为实际统计计算方式 $C(w_{t-n+1}^{n-1}w_t)$ 表示 从 $t-n+1$ 到 $t-1$ 的单词序列 $w_{(t-n+1)}$ 计数和 单词 $w_t$ 的联合事件。 NGram 训练语料重要性从两个语料训练的结果来看，不同语料训练出来的“英语句子‘模型存在差异。因此在训练时使用大语料库，多来源语料库是有必要的。毕竟N-gram 的统计模型不能在简单语料库中模拟出各种不同 风格 2. 平滑在有限的训练数据集中，通过最大似然数估计来训练 N Gram 语言模型，得到的结果中必然会有稀疏性的问题。导致训练预料库存在着大量零概率 N 元语法，但实际上也会存在某些非零的概率，这种非零计数比较小的时候，最大似然数估计会产生非常糟糕的结果。因此引入的平滑的方式来解决小数据集的可变形造成的糟糕的估计结果。其内核是削减高计数的概率，用以弥补零计数的概率，从而使得概率分布不至于过于参差不齐 2.1 Laplace 平滑——加一平滑这是一个简单的平滑方法，是在归一化为概率之前，将所有的计数加一。一般的加一方式为 $P_{Laplace}(w_i)=\\frac{c_i+1}{N+V}$，其中 表示的是词汇表中单词个数。但在使用中计算概率的方式可以只调整分子不增加分母，也可以不同时地调整分子与分母。对于平滑之后对于分子的影响，可以通过调整计数 （Adjusted Count）$c^\\star$ 来量化。同 MLE 用单词总数量 $N$ 来进行归一化，计算调整计数为概率的话，需要添加一个归一化因子 $\\frac{N}{N+V}$，最终表达式为：$$c_i^\\star=(c_i+1)\\frac{N}{N+V} \\P_i^\\star=(c_i+1)\\frac{1}{N+V} \\\\text{即同 MLE 一样，使用 N 进行归一化处理，得到概率}P_i^\\star$$另一种通过相对比较的方法来解释平滑——打折(Discounting)，通过吧非零的计数调低分派给哪些为零的计数。因此前面 $c^\\star$ 是被计作打折的计数，而用相对折扣(Discount) 的方式来描述平滑 $d_c=\\frac{c^\\star}{c}$，其中 $c$ 是原始计数。 2.1.1 Smoothing 示例示例来源于 SALP 中的 Berkeley Restaurant Project 例子，其中包括了单词数量 $V$ 为 $1446$， i want to eat chinese food lunch spend i 5 827 0 9 0 0 0 2 want 2 0 608 1 6 6 5 1 to 2 0 4 686 2 0 6 211 eat 0 0 2 0 16 2 42 0 chinese 1 0 0 0 0 82 1 0 food 15 0 15 0 1 4 0 0 lunch 2 0 0 0 0 1 0 0 spend 1 0 1 0 0 0 0 0 以 Bigram 的方式计算出相应的概率值 $\\P(w_n|w_{n-1})=\\frac{C(w_{n-1}w_n}{C(w_{n-1})}$ 如下： i want to eat chinese food lunch spend i 0.002 0.330 0.0036 0 0 0 0.00079 want 0.00220 0.66 0.0011 0.00650.0065 0.0054 0.0011 to 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087 eat 0 0 0.0027 0 0.021 0.0027 0.0560 chinese 0.00630 0 0 0 0.52 0.0063 0 food 0.014 0 0.0140 0.00092 0.0037 0 0 lunch 0.00590 0 0 0 0.0029 0 0 spend 0.00360 0.0036 0 0 0 0 0 采用 Add-one smoothing 方式，调整计数： i want to eat chinese food lunch spend i 6 828 1 10 1 1 1 3 want 3 1 609 2 7 7 6 2 to 3 1 5 687 3 1 7 212 eat 1 1 3 1 17 3 43 1 chinese 2 1 1 1 1 83 2 1 food 16 1 16 1 2 5 1 1 lunch 3 1 1 1 1 2 1 1 spend 2 1 2 1 1 1 1 1 调整之后的 Bigram 概率值 $P_{Laplace}(w_i)=\\frac{c_i+1}{N+V}$ ：| | i | want | to | eat | chinese | food | lunch | spend || ——- | ——- | ——- | ——- | ——- | ——- | ——- | ——- | ——- || i | 0.0015 | 0.21 | 0.00025 | 0.0025 | 0.00025 | 0.00025 | 0.00025 | 0.00075 || want | 0.0013 | 0.00042 | 0.26 | 0.00084 | 0.0029 | 0.0029 | 0.0025 | 0.00084 || to | 0.00078 | 0.00026 | 0.0013 | 0.18 | 0.00078 | 0.00026 | 0.0018 | 0.055 || eat | 0.00046 | 0.00046 | 0.0014 | 0.00046 | 0.0078 | 0.0014 | 0.02 | 0.00046 || chinese | 0.0012 | 0.00062 | 0.00062 | 0.00062 | 0.00062 | 0.052 | 0.0012 | 0.00062 || food | 0.0063 | 0.00039 | 0.0063 | 0.00039 | 0.00079 | 0.002 | 0.00039 | 0.00039 || lunch | 0.0017 | 0.00056 | 0.00056 | 0.00056 | 0.00056 | 0.0011 | 0.00056 | 0.00056 || spend | 0.0012 | 0.00058 | 0.0012 | 0.00058 | 0.00058 | 0.00058 | 0.00058 | 0.00058 |如果对平滑方法对影响进行分析，即平滑之后调整的计数情况 $c^\\star(w_{n-1}w_n)=\\frac{[C(w_{n-1}w_n)+1]\\times C(w_{n-1})}{C(w_{n-1}+V)}$:| | i | want | to | eat | chinese | food | lunch | spend || ——- | —- | —– | —– | —– | ——- | —- | —– | —– || i | 3.8 | 527 | 0.64 | 6.4 | 0.64 | 0.64 | 0.64 | 1.9 || want | 1.2 | 0.39 | 238 | 0.78 | 2.7 | 2.7 | 2.3 | 0.78 || to | 1.9 | 0.63 | 3.1 | 430 | 1.9 | 0.63 | 4.4 | 133 || eat | 0.34 | 0.34 | 1 | 0.34 | 5.8 | 1 | 15 | 0.34 || chinese | 0.2 | 0.098 | 0.098 | 0.098 | 0.098 | 8.2 | 0.2 | 0.098 || food | 6.9 | 0.43 | 6.9 | 0.43 | 0.86 | 2.2 | 0.43 | 0.43 || lunch | 0.57 | 0.19 | 0.19 | 0.19 | 0.19 | 0.38 | 0.19 | 0.19 || spend | 0.32 | 0.16 | 0.32 | 0.16 | 0.16 | 0.16 | 0.16 | 0.16 | 可以看出调整之后对原来的计数数量影响较大，前缀词例如 $C(want\\ to)$ 的数量从 $609$ 变为了 $238$，体现在 $P(to|want)$ 的概率降到 $0.26$，而后缀词则在增加 $ 3. 评估指标外在评测：在语言模型中评估模型的最佳方式是，将语言模型嵌入到具体的应用任务测试应用的总体性能的方式，即端到端的外在评测(Extrinsic Evaluation)，也称为现实评测（in vivo evaluation)。内在评测(Intrinsic Evaluation)，是与应用任务无关的模型质量评测。 前者能显性评估两个模型的性能，但是评测消耗的代价很高，应该常采用内在评测。困惑度(Perplexity) 是对于 N 元语法模型常见的内在评测度量指标，虽然这个指标不能保证任务模型性能得到真实改进，但是应用任务的性能改进是和困惑度改进具有相关关系。而且它是一个快速的检验算法。 困惑度(Perplexity， PP) 是语言模型指派给测试集的概率函数。对一个句子序列来说，就是一个语言模型下的序列概率函数。困惑度可以理解为，如果每个时间步都根据语言模型计算的概率分布随机挑词，那么平均情况下，挑多少个词才能挑到正确的那个。其公式如下：$$PP(W)=P(w_1w_2\\cdots w_n)^{-\\frac{1}{N}} \\=\\sqrt[N]{\\frac{1}{P(w_1w_2\\cdots w_n)}} \\=\\sqrt[N]{\\frac{1}{\\displaystyle{\\prod_{i=1}^N}P(w_i|(w_1\\cdots w_{i-1}))}} \\\\text{第三个等式是使用链式展开}$$ 另一种研究困惑度的方式，是通过加权平均转移因子(Weighted Average Branching Factor)，它的表示基础是转移因子(Branching Factor)——任何一个单词后面可能连续的单词数量。举个以数字字符串例子来说，每个数字字符出现概率为 $\\frac{1}{10}$，以这种方式得到的困惑去为 $PP(W)=(\\frac{1}{10}^N)^{\\frac{-1}{N}}=10$。但是实际情况下 0 发生的概率比较高，例如训练数据集中 0 发生了 91 次，而其他数字数据发生了 1 次。那么在测试数据中 0003000，测试集中的预测掉一个数字为 0 的困惑度比较小，也就是有较高的概率为 0。在这个例子中，虽然转移因子依然是 10，但是困惑度或者使用加权平均转移因子更小。","link":"/%5Bobject%20Object%5D/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%83%8C%E6%99%AF/"},{"title":"2021-04-19","text":"只有背景信息，没有跌宕起伏的故事来串联，是如此的空乏。","link":"/%5Bobject%20Object%5D/%E6%89%AF%E6%B7%A1%E9%9B%86-2021-04-19/"},{"title":"SQL 笔记Part1: DBMS","text":"1. 背景常用语境下的数据库一般是说的 DBMS，实际数据库是存储的数据集合。而又因为数据存储形式差异，DBMS 有多种类型： 层次数据库（Hierarchical Database，HDB）最早研制成功的数据库系统，它把数据通过层次结构（树形结构）的方式表现出来，代表是 IMS（Information Management System）数据库 关系数据库（Relational Database，RDB） 关系型数据库也采用由行和列组成的二维表来管理数据，此外使用的语言是 SQL（Structured Query Language）。代表性的关系型数据库有 Oracle Database、SQL Server、DB2、PostgreSQL 和 MySQL 非关系型数据库 也就是常说的 NoSQL，实际上是表示的一大类数据库： 面向文档（Document-Oriented）数据库 面向文档数据库会将数据以文档的形式存储。每个文档都是一系列数据项的集合。每个数据项都有一个名称与对应的值，值既可以是简单的数据类型，如字符串、数字和日期等；也可以是复杂的类型，如有序列表和关联对象 &emsp;&emsp;数据存储的最小单位是文档，同一个表中存储的文档属性可以是不同的，数据可以使用XML、JSON或者JSONB等多种形式存储。具有代表性的面向文档数据库有 MongDB 和 CouchDB 列存储（Column-oriented）数据库 数据存储存在列族（column family）中，列族用来存储经常被一起查询的相关数据。例如，列存储数据库通常用应用于分布式存储的海量数据。具有代表性的列存储数据库有 Cassandra 和 HBase XML数据库（XML Database，XMLDB） 是一种支持对 XML（标准通用标记语言下的一个应用）格式文档进行存储和查询等操作的数据管理系统 键值存储系统（Key-Value Store，KVS） 用来保存查询所使用的主键（Key）和值（Value）的组合的数据库。具有代表性的键值存储数据库有 Redis、Memcached 和 MemcachedDB 2. 基于关系数据库 SQL 操作SQL 对于数据的操作包括 DDL、DML 以及 DCL，具体的意义和实用的相关关键字： DDL（Data Definition Language，数据定义语言） 用来创建或者删除存储数据用的数据库以及数据库中的表等对象。DDL 包含以下几种指令: CREATE 创建数据库和表 12345678910111213# 用于创建数据库CREATE DATABASE &lt;数据库名称&gt;; # 创建表CREATE TABLE &lt; 表名 &gt;( &lt; 列名 1&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , &lt; 列名 2&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , &lt; 列名 3&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , &lt; 列名 4&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , . . . &lt; 该表的约束 1&gt; , &lt; 该表的约束 2&gt; ,……); 其中常用的数据类型包括了 INT、CHAR、VARCHAR、DATE 以及 TEXT，其他类型可能因 DBMS 差异而不同[^1] DROP ： 删除数据库和表等对象，使用 DROP TABLE &lt;表名&gt;; 删除表，使用 DROP DATABASE &lt;数据库名&gt; 删除数据库 ALTER ： 修改数据库和表等对象的结构 使用 ALTER TABLE &lt;表名&gt; [ADD|DROP] COLUMN &lt;列名 &gt;; DML（Data Manipulation Language，数据操纵语言） 用来查询或者变更表中的记录。DML 包含以下几种指令: SELECT ：查询表中的数据 INSERT ：向表中插入新数据 UPDATE ：更新表中的数据 DELETE ：删除表中的数据 DCL（Data Control Language，数据控制语言） 用来确认或者取消对数据库中的数据进行的变更。除此之外，还可以对 RDBMS 的用户是否有权限操作数据库中的对象（数据库表等）进行设定。DCL 包含以下几种指令: COMMIT ： 确认对数据库中的数据进行的变更 ROLLBACK ： 取消对数据库中的数据进行的变更 GRANT ： 赋予用户操作权限 REVOKE ： 取消用户的操作权限 备注[^1]: MySQL 的数据类型 MySQL 的数据类型有大概可以分为 5 种，分别是整数类型、浮点数类型和定点数类型、日期和时间类型、字符串类型、二进制类型： 数值类型 包括 TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，浮点数类型包括 FLOAT 和 DOUBLE，定点数类型为 DECIMAL。 日期/时间类型 包括 YEAR、TIME、DATE、DATETIME 和 TIMESTAMP 字符串类型 包括 CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM 和 SET 二进制类型 包括 BIT、BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB 参考 SQL 训练","link":"/%5Bobject%20Object%5D/SQL-SQL%E7%AC%94%E8%AE%B0Part1/"},{"title":"AB测试笔记总结Part2","text":"总结这篇文章，主要在学习 AB 测试的过程中遇到了两个方面的问题。其中之一是，课程讲解是将 AB 测试的流程交叉到分析过程、概念讲解等过程中，以致于对 AB 测试的流程没有形成一个完整的体系；另一个问题是，AB 测试中用到的统计学相关知识。在课程中统计学的讲解和公式，表面上是很违背“直觉的”，所以需要一个合适的切入角度去理解 AB 测试中用到的统计学知识。第二部分是对 AB 测试第一部分中补充信息。 1. Boostrap 信息1.1 Boostrap 注意事项与真实世界相比，通过自助法获得的置信区间更加乐观。这是因为我们没有关于真实世界的参数化模型，对于真实世界还有一些不了解的情况。思考下尝试了解最大值分布的极端情形：置信区间永远无法包含大于最大观察值的值，并且下限小于最大观察值是不合理的。但是，很有可能有一些未观察到的值大于我们观察到的值，尤其是像示例中所演示的偏斜数据。 但是这并不能否定自助法的优势。自助流程很简单直白。由于不对数据分布做出假设，所以适用于任何情形。结果应该与标准检验的不相上下。但是需要投入计算精力，并且输出取决于输入数据。例如，对于上述示例中第 90 百分位上的 95% 置信区间，推断的区间只能捕获原始生成分布中约 83% 的第 90 百分位数值。但是使用更复杂的二项假设对观察到的数据编制索引，只能使结果提高一个百分点，达到 84%。这两种方法都取决于生成的具体数据：不同的 5000 个数据点将生成不同的区间，准确率也不同。 百分位置信区间的二项式方法参考文档：1、2 1.2 Bootstrap 置换检验应用置换检验是一种重新抽样检验，用于比较两组或多组之间的结果变量的值。在置换检验中，我们对组标签进行重新抽样。原理是，在零假设下，所有组的结果分布应该一样，无论是对照组还是实验组。所以，我们可以将所有数据值当做一个大的小组，并模拟零假设。将标签随机地分配给数据点（同时保持原始小组成员比例）可以得出零假设的一个模拟结果。 剩下的步骤与标准假设检验中使用的抽样方法类似，但是我们尚未指定要从中抽样的参考分布，我们直接从收集的数据中抽样。将标签随机分配给所有数据并多次记录结果统计量后，我们将实际观察到的统计量与模拟统计量进行比较。我们看看有多少模拟统计值和实际观察到的统计值一样极端或更极端，并算出 p 值，然后得出结论。 请在下面的单元格中实现置换检验，检验与对照组相比，实验组的次数第 90 百分位是否统计显著性地更小： 初始化一个空列表，用于将样本分位数的差异存储为 sample_diffs。 为每个试验创建一个循环： 首先通过随机重排数据点标签（即原始数据中的应变量），生成一个置换样本。（可以使用random.permutation。） 然后，根据置换的标签计算分配给每组的数据点的第 q 分位数。将分位数差异附加到 sample_diffs 列表中。 收集置换样本的分位数差异后，计算实际数据观察到的差异。然后，看看有多少置换样本差异小于或大于观察到的差异，并计算 p 值，具体取决于期望的备择假设。 核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243def quantile_permtest(x, y, q, alternative = 'less', n_trials = 10000): &quot;&quot;&quot; Compute a confidence interval for a quantile of a dataset using a bootstrap method. Input parameters: x: 1-D array-like of data for independent / grouping feature as 0s and 1s y: 1-D array-like of data for dependent / output feature q: quantile to be estimated, must be between 0 and 1 alternative: type of test to perform, {'less', 'greater'} n_trials: number of permutation trials to perform Output value: p: estimated p-value of test &quot;&quot;&quot; # initialize storage of bootstrapped sample quantiles sample_diffs = [] # For each trial... for _ in range(n_trials): # randomly permute the grouping labels labels = np.random.permutation(y) # compute the difference in quantiles cond_q = np.percentile(x[labels == 0], 100 * q) exp_q = np.percentile(x[labels == 1], 100 * q) # and add the value to the list of sampled differences sample_diffs.append(exp_q - cond_q) # compute observed statistic cond_q = np.percentile(x[y == 0], 100 * q) exp_q = np.percentile(x[y == 1], 100 * q) obs_diff = exp_q - cond_q # compute a p-value if alternative == 'less': hits = elif alternative == 'greater': hits = return (hits / n_trials) 2. 多指标分析针对多指标或者多轮的多重比较时，需要注意显著性水平是否被放大。如果存在显著性水平放大，那么就需要注意使用相关的校正方法进行校正，例如 Bonferroni 校正 。同时，还需要注意考虑校正是否具有保守的问题，例如当多个指标之间并没有相关性（例如 CTR 和唯一用户数量之间即具有相关性），那么使用 Bonferroni 校正就存在过于保守的问题。 在对多指标分析方面，还有其他非保守性方法，如封闭测试程序、 Boole-Bonferroni 联合校正 以及 Holm-Bonferroni 方案 。同样在使用不同这些方法时，需要确认假设方案的正确性 2.1 Bonferroni Correction 与 Šidák Correction如果你要跟踪多个评估指标，那么需要知道单个指标的 $\\mathrm{I}$ 型错误率使总体出现 $\\mathrm{I}$ 型错误的概率是多少。最简单的情形是：假设有 n 个独立的评估指标，如果其中一个指标具有统计显著性结果，则足以表示操控措施成功了。在这种情况下，只是出现一个 I 型错误的概率等于 $\\alpha_{over} = 1 - (1-\\alpha_{ind})^n$，下图展示了单个指标的错误率 $\\alpha_{ind} = .05$ 和 $\\alpha_{ind} = .01$: ] 为了防止这种情况，我们需要对单个检验错误率引入一个校正系数，使总体错误率几乎达到理想水平。一种保守方法是用总体错误率除以被测指标的数量： $\\alpha_{ind} = \\alpha_{over}/n$ 称之为帮费罗尼校正。如果假设指标相互独立，可以通过 Šidák 校正获得更好的结果： $\\alpha_{ind} = 1-(1-\\alpha_{over})^{1/n}$ Šidák 校正所画的线比帮费罗尼校正的稍微高一些 在现实中，评估情形很少这么直观。指标之间很有可能关系，而不是相互独立。如果正相关，那么知道一个指标的结果，相关指标很有可能也会指向相同的方向。在这种情形下，上述校正方法就过于保守了，导致总体错误率比理想水平更低（如果是负相关，那么错误率可能上升或下降，取决于执行的检验类型）。 此外，可能需要多个指标都展现出统计显著性，才能表示实验成功了，否则就会存在不同程度的成功，取决于哪些指标似乎受到操控措施的影响。一个指标可能不足以说明值得部署实验中测试的更改。降低单个错误率将使真正的显著效果呈现出统计显著性。也就是说，降低 $\\mathrm{I}$ 型错误率还会提高 $\\mathrm{II}$ 型错误率，这是另一种保守变化。 最终，在选择错误控制措施时，需要作出小小的权衡。如果完全保守地采用上述某个简单校正方法，那么可能会导致没有实施实际上对指标有影响的更改。需要考虑指标之间的依赖程度，并且什么样的结果表明检验成功了，从而使错误率达到正确的平衡效果。如果你需要所有指标都出现显著变化才能继续，那么可能根本就不需要校正系数。还可以使用虚拟测试结果、自举法和置换法来设定显著性阈值。最后请注意，实际显著性可能最为重要，会覆盖其他统计显著性结果。 虽然此页面的侧重点是介绍评估指标，但是这些注意事项也适用于不变指标。检验的不变指标越多，就越有可能某些检验会显示出统计显著性差异， 即使检验的小组是从对等的总体中抽样出来的。但是，不建议对单个检验应用校正系数，因为我们要避免之后在解释数据时出现更大的问题。正如之前提到的，如果有一个不变指标显示统计显著性差异，也不用紧张，但是可能会产生后续问题，也许会对分析造成影响。 3. 提前结束提前窥视并作出不在计划之内的提前决策，会带来严重的风险。如果你没有考虑窥视对错误率的影响，那么尽量不要提前查看结果，仅在实验结束时分析结果。这也是为何在收集任何数据之前提前设计实验很重要的另一个原因。 但有些方法可以让我们在设计实验时提前做出决策，例如通过调节单个检验级错误率，使总体错误率不变。对于连续跟踪，此页面描述了关于错误率指标的一般法则：跟踪每个小组的成功次数，一旦计数之和或差异超过某个阈值，则结束实验。更一般的情况是，序贯概率比检验等检验可以提前结束实验，前提是从统计学角度而言，某个指标不会越过统计显著性界限","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-AB%E6%B5%8B%E8%AF%95%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93Part2/"},{"title":"AB测试笔记总结Part1","text":"总结这篇文章，主要在学习 Udacity 数据分析课程中 AB 测试部分遇到了两个方面的问题。其中之一是，课程讲解是将 AB 测试的流程交叉到分析过程、概念讲解等过程中，以致于对 AB 测试的流程没有形成一个完整的体系；另一个问题是，AB 测试中用到的统计学相关知识。在课程中统计学的讲解和公式，表面上是很违背“直觉的”，所以需要一个合适的切入角度去理解 AB 测试中用到的统计学知识。 因此本次根据课程内容，网上查阅的相关资料对 AB 测试的流程进行一个梳理。同时在最后将统计学方面中较难理解的部分，进行一个梳理，提供一个可行的切入角度。 1. AB 测试的流程在明白 AB 测试的流程之前，需要先从 AB 测试是什么和为什么使用的角度进行说明。简单来说 AB 测试就是为了验证在先验条件的存在的情况下，进行新的变更是否合理和可行以达到优化的目的。使用 AB 测试的方式能能够度量变更对某些指标的变化，是变更更具有合理性依据更充分。 AB 测试也存在 不适用的场景：1）对没有明确参照的试验，AB 测试是基于先验条件的优化，如果没有一个参照对比是无法进行测试。2）数据获取时间长，AB 测试一般都是进行小规模快速的试验，所以对于数据获取的单周期较长的试验不太适用。 对于这类不适用场景情况下，可以采取的解决方案：1）通过查询和观察分析日志，进行假设检验，了解用户行为改变原因，使用其他试验方法来完成随机化和试验，进行前瞻性分析。2）其他方法，例如用户体验研究、焦点小组、调查以及人为评价——可以完成深入的定性数据。可以参考 Additional Techniques for Brainstorming and Validating Metrics 对于可以使用 AB 测试的试验，一般流程可以归结为一下几个步骤${^{[1]}}$： 选择和描述指标 确认显著性水平（${\\alpha\\ level}$ ），统计功效（${statistical\\ power=1-{\\beta}}$），实际显著性水平（${practical\\ significance\\ level}$） 计算需要的样本容量 对试验组和控制组进行分流，运行测试 分析结果以及得出有效的结论 下面将从以上五个流程步骤，进行分别梳理。 1.1 选择和描述指标选择合适的指标，需要理解指标 敏感性 和 稳健性 ${^{[2]}}$ 的良好直觉的基础上，通过相应的方式来衡量指标是否可用。在 AB 测试过程中，主要有两类指标： 不变指标（invariant metrics）：在试验中选取的这样的特征，它能够确认试验组和对照组的容量是否满足相等，而且它不能受到变更的直接明显的影响。它的 评估方式 是通过 完整性检查（sanity check）来确认是否符合不变指标的要求。一般包括 总体规模指标 和 不会变化的指标。 评估指标（evaluation metrics）：在试验中选择的特征，它能够反应出变更效果。总体上来说，它指的是分为 4 个类别的指标：1）计数和合计类指标；2）分布类型指标（例如平均值、中位数以及百分位数等）；3）概率性和比率型指标（probability &amp; rates，例如 CTP 和 CTR）；4）比例型指标，它是指利用两个指标计算出的百分数，合并的一个新指标。 在建立合适的指标的过程中，需要确认高级业务指标——在针对多团队工作的时候，这样可以统一目标。通常首先应该是简单明确的阐释指标以使指标能够被容易理解；之后对指标细节进行细化，确定如何明确测量指标；再进一步进行数据测量，确认该指标使用方法。对于指标确认还需要注意，对众多指标需要进行相应的评估——因为针对使用目标不同，可能会需要对指标进行调整。当然在团队中的指标使用时，需要注意使用复合指标以避免同一个团队使用不同指标造成的目的分散。复合指标一般被称为目标函数或者 总体评估标准（ OEC 详情见该文章）。 1.2 选择显著性水平、统计功效、实际显著性水平${^{[3]}}$在 AB 测试中，通常使用的统计显著性水平为 ${0.05}$，统计功效设置为 ${0.80}$，而实际显著性水平需要根据不同的试验进行设定。这三者一方面在后续中确认试验规模中需要使用；另一方面，需要使用三者进行统计学分析，得出合适的结论。其中实际显著性水平，主要是说明了当检测到变更时，期望多大的变化量才会实施变更——对于这点是基于战略层面以及实际的收益比进行考量。 1.3 计算试验样本容量在分析指标的变异性的时候，可以发现指标选择、分流单元（unit of diversion）的选择以及总体的选择都会影响结果。确认样本容量${^{[4]}}$的时候需要从三者进行考虑： 主体（Subject）：试验中的分流单元说明了试验的主体，即试验中需要用到的测试和比较对象。分流单元主要用的对象${^{[5]}}$是：1）用户 ID；2）匿名 ID，例如 Cookie；3）Event。其他不常用的有设备 ID，IP 等。 总体（Population）：这个是为了理解试验中使用的主体来自于合适的总体，保证分流的准确性，也是为了确保试验的有效性。例如，在实际情况下，变更时针对某个国家、某个语言下的变更——具体可以参考 队列（Cohort）。 保证试验得到快速反馈 因为 AB 测试的特点是快速得到有效的试验反馈，然后要保证试验分析的有效性同时获得良好的显著性分析。因此需要考虑怎么确保使用合适的 ${\\alpha}$、${\\beta}$ 以及实际显著性的情况下，使用合适样本容量。 在课程中使用了在线计算器，利用确定的参数来确认样本容量。使用工具时请注意以下概念：基准转换率（ baseline conversion rate ）—— 指变更之后的预计点击概率；最低可探测效果（ minimum detectable effect ）—— 指实际显著性，相当于 ${d_{min}}$ 。最低可探测效果可以选择绝对数值变更和相对数值变更，但是一般选择绝对数值变更；其他参数 ${\\alpha}$ 和 ${\\beta}$ 根据试验初期确定的值——一般情况下 ${\\alpha}$ 一般为 $5%$和 ${\\beta}$ 一般设置为 $80%$。 1.4 试验组和对照组分配及运行试验在进行了以上流程之后，需要考虑怎么样运行试验——主要指需要试验持续时间以及试验实施时间两个方面；同时在已经明确了样本容量之后，考虑到需要“抽样”的方式进行多次试验，所以通常把试验样本的容量进行拆分成多次进行试验${^{[6]}}$。这样导致需要结合分配给试验的流量比例和试验样本规模，综合考虑试验 持续时间（Duration），曝光时机（Exposure）以及 学习效应（Learning Effect）${^{[7]}}$。 1.5 分析试验结果得出有效结论AB 测试的目的就是为了验证变更是否可行，但是在试验过程中可能收到多种因素的影响都可能导致结果不可信，因此需要去验证试验的数据是否可信。在分析结果的过程，包括以下内容： 完整性检查（Sanity check） 在完整性检查是通过验证 不变指标 在试验中是否发生了变化。完整性检查是第一步，如果无法通过完整性检查，那么就不能继续进行统计分析。需要检查失败原因，以便后续进行试验以及提供经验教训。一般分析原因，可能用到回顾性分析和分析学习效应影响 分析数据 进行参数型统计分析，就是检验是否有否具有显著性差异。对单一评估指标进行分析，有助于了解试验中存在的问题（例如设计是否合理）以及了解用户对变更的反应。完成参数性统计分析之后，需要从非参数型分析的方法进行总体分析。需要 注意：当面对参数型假设检验和非参数型符号检验结论冲突时，可以检查数据是否存在辛普森悖论（Simpson’s Paradox）——一般发生这种情况可能的原因：1）试验设置不正确；2）变更在新用户和老用户之间存在差异。 对多评估指标分析，需要注意实际非显著性的指标被当作显著性——导致存在这个问题的原因是非相关的多个指标增加了最后结论的显著性${^{[8]}}$。针对这类显著性被放大的解决方法：1）Bootstrap；2）Bonferroni Correction，它是将显著性水平除以指标个数，以降低单个指标的显著性水平。但是该方法可能导致分析结果过于保守——对相关性指标不建议使用；3）控制 Familysise Error Rate (FWER)；4）控制 False Discovery Rate(FDR)。注意：在实际分析中可能指标之间的显著性方向不一样——例如，在试验假设中期望是 DAU 和 用户在线时间都增加，但是实际分析可能是 DAU 减少了而用户在线时间增加。对于这类问题，一般是通过设置总体评估标准（即 OEC，Overall Evaluation Criterion），但是 OEC 是帮助理解商业目标以及平衡指标的时间和点击问题，但是它并不能帮助做出变更决定。 在完成以上的统计显著性分析之后，需要通过实际显著性分析来确认变更的实际影响以及实际可实施的可能性。最后得出有效结论需要从理解变更，变更的影响以及产生结果的原因等方面着手阐释，并最后得出是否可以执行变更的结论。 1.6 其他需要思考的角度试验中涉及的伦理问题都是影响试验的重要因素，在试验设计初期需要考虑。 1.6.1 试验设计中的伦理考虑AB 测试的伦理包括AB测试中的道德规范、如何保护用户。在进行伦理分析的时候，需要从以下几个原则进行分析：风险（ Risk ）、益处（ Benefit ）、选择（ choice ）、隐私（ Privacy ） 风险：试验参与者会承受那些风险 主要界限在于风险是否超过了“最低风险”。 最低风险的定义为参与者在日常生活中会遇到的伤害的概率和程度，包括身体、心理、情感、社会和经济方面。 如果风险超过最低风险，那就需要获得知情同意。我们将在后面讨论知情同意。在大多数（并非全部）在线实验中，这些实验是否会导致超过最低风险的伤害是非常有争议的。 如果我们改变某个教育网站的课程排名，或更改某个在线游戏的用户界面，参与者会暴露在什么风险之下呢？与健康或财务相关的网站或应用程序当然除外。 比如，在 Facebook 的实验中，参与者是否真的暴露在超过最低风险的伤害之下是有争议的： 显示的所有内容无论如何都会进入他们的信息流，问题就在于删掉一些文章是否会增加风险。 益处：试验结果可能会产生哪些好处 即使风险很小，但是研究结果有什么意义呢？大多数 A/B 测试的益处在于改进产品。 在其他社会科学中，在于理解人的境况以提供帮助，比如教育和发展。医学界的测试风险往往较高，但却能带来健康状况的改善。能够说明完成研究的益处是非常重要的。 选择：试验参与者是否还有其他选择 要测试一个搜索引擎的变化，参与者始终可以选择使用其他搜索引擎。 主要问题在于参与者的选择越少，强制性以及参与者是否能选择参与与否的问题就越多，以及这些问题如何在风险和益处之间实现平衡。 例如，医学临床试验要测试治疗癌症的新药，鉴于大多数参与者面临的另一个主要选择是死亡，那么在知情同意的情况下，参与者的风险还是很高的。 对于在线实验，需要考虑的问题是用户可以使用的其他服务有哪些，以及转换服务的成本是多少，包括时间、金钱、信息等。 隐私或数据敏感性：试验参与者期待怎样的隐私保护 针对收集到的数据内容，包括隐私和保密性的期望，这包括了以下几个方面的内容：1）参与者是否了解哪些数据被收集了；2）公开这些数据是否会带来伤害；3）参与者是否期望相关数据被当作隐私和保密信息 当然针对现有公共环境中的试验或者数据获取于公共数据，那也就不涉及保密性的问题。另一方面如果由收集了新的数据，那么又需要考虑相关问题： 收集了哪些数据？敏感性如何？是否包括财务和健康数据？ 收集的数据是否会与个人联系起来，也就是说是否具有个人可识别性？ 数据是如何处理的，采取了什么安全措施？参与者可以期望什么样的保密级别？ 数据公之于众后会对个人带来什么伤害，包括健康、心理、情绪、社会和财务方面？ 例如很多时候，从观察到的“公共”行为、调查和采访中收集的数据，如果数据没有个人可识别性，则被视为不需要 IRB 审查（参考： NSF 常见问题解答） 总体来说，在进行试验数据采集时，需要注意到以下问题： 对于新收集和存储的数据，数据的敏感性如何？处理数据时采取了什么内部防御措施？例如设置了何种访问控制？如何捕捉和管理违反该防御措施的行为等？ 然后，如何使用收集的数据？如何保护参与者的数据？如何向参与者保证为了此研究而向他们收集的数据不会被用于其他目的？随着数据敏感性的增强，这一点越来越重要。 最后，哪些数据会更加广泛地公开，是否会给参与者带来任何额外风险？ 大多数研究，由于在线服务的性质，往往风险较小，问题多在于可识别性、隐私、机密/安全性方面的数据收集。 因此，由公司外的中立第三方来做这些决定或许要好过于结果中存在既得利益的人。 在线研究的一个越来越大的风险在于偏见及歧视的可能性，如阶梯计价以及对于特定人群来说这算不算歧视。关于数据收集的内部审核： 参与者面临的风险是否降到了最低？ 参与者了解收集的数据是什么吗？ 这些数据具有可识别性吗？ 如何处理数据？ 内部审核流程建立： 参与 A/B 测试的每名员工应接受伦理和参与者保护方面的教育。 很明显除了我们谈到的诚信、胜任能力和责任外，还有其他伦理方面，而且往往比保护 A/B 测试的参与者更加广泛（引自美国计算机协会 (ACM) 道德规范)。 所有数据，无论是否具有可识别性，都应安全存储，仅向需要这些数据来完成工作的人提供访问权限，且应该对访问限时。 应针对哪些数据使用是可接受的和不可接受的制定政策。而且，所有数据使用均应记录在案和定期审计，以确保未违反规定。 对于可能存在超过最低风险或数据敏感性问题的处理，应制定明确的上报流程。 2. AB 测试中统计学内容首先需要明确一点，在课程中 AB 测试使用的统计学显著性检验方法为 ${z}$ 检验 ${^{[9]}}$。课程中用到的统计学内容的步骤：1）试验过程中显著性分析；2）试验结论显著性分析过程；3）非参数型的符号检验，进行展开解释。 2.1 试验过程中显著性分析——完整性检查完整性检查是利用不变指标来检查试验是否有效的过程。首先在进行流量分流到试验组和对照组对象的过程中，是随机分配到两者中（即 ${0.5}$ 的概率）——这里符合一个 二项分布 的特征。因此只需要分析对照组或试验组是否满足这个二项分布的置信区间，相应的计算公式如下： 期望：${E=n*p}$ 标准偏差：Standard Deviation，${SD=\\sqrt{np(1-p)}}$ 标准差：Standart Error, ${SE=\\frac{SD}{n}=\\sqrt{\\frac{p*(1-p)}{n}}}$ 边际值：Margin，${margin=SEz^}$ 置信区间：Confidence Interval，${CI\\in[p-margin,\\ p+margin]}$ 实际概率：指对照组或控制组占总体的比例（这里仅对控制组作计算），${p^*=\\frac{n_{contr}}{n}}$ 以上公式中，${n}$ 指 ${n_{total}}$，为试验组和对照组的样本容量求和 ${n_{total}=n_{contr}+n_{exp}}$；${z^*}$ 指的是在置信水平 ${\\alpha}$ 的条件下得到的 ${z_{score}}$。 检查过程中需要分别计算每个不变指标的值，比较某个指标 ${p^*}$ 是否在该指标的置信区间 ${CI}$ 中，如果满足要求那么说明该指标通过了完整性检查。在实际中，可以通过另一种方式来计算——${p_{exp}}$ 与 ${CI_{contr}\\in[p_{contr}-margin_{contr}, \\ p_{contr}+margin_{contr}]}$ 比较的方式（实际中可以利用 ${p_{contr}}$ 和 与试验组进行比较）。以对照组的计算方式如下： 概率：${p_{exp}=\\frac{x_{exp}}{n_{exp}}}$，${p_{contr}=\\frac{x_{contr}}{n_{contr}}}$ 标准差：Standart Error, ${SE_{contr}=\\sqrt{\\frac{p_{contr}*(1-p_{contr})}{n_{contr}}}}$ 边际值：Margin，${margin_{contr}=SE_{contr}z^}$ 置信区间：Confidence Interval，${CI\\in[p_{contr}-margin_{contr},\\ p_{contr}+margin_{contr}]}$ 通过这种方式进行分析完整性检验，如果 ${p_{exp}}$ 在 ${CI}$ 中那么就通过了完整性检验，否者即不能通过完整性检验。 2.2 试验显著性分析过程——显著性分析（效应分析）在进行显著分析之前，需要分析指标的分组单元和分析单元是否一致${^{[10]}}$——当两者一致时说明分析方差和实证方差匹配。显著性分析主要包括统计学显著性分析和实际显著性分析，下面对两者分别说明： 统计显著性 Statistics Significance 统计显著性分析是针对评估指标的分析，为了检验试验组和对照组中产生了差异性。在试验中，零和假设为试验组和对照组之间概率没有差异性即 ${\\hat{d}=\\hat{p}{exp}-\\hat{p}{contr}=0}$；备选假设两者概率存在差异。如果利用差异值 ${\\hat{d}}$ 计算相应的置信区间，假设区间内不包括 ${0}$，说明两者具有统计显著性，那么就可以拒绝接受零和假设；反之，则不可以拒绝零和假设。分析过程是基于双样本差异分析，具体过程如下： 差异值：Difference, ${\\hat{d}=\\hat{p}{exp}-\\hat{p}{contr}}$ 合并概率：Pool Probability 指试验组和对照组中发生发生占总体的概率 ${p_{pool}=\\frac{x_{cont}+x_{exp}}{n_{contr}+n_{exp}}}$ 合并标准差：Pool Standard Error，一般合并方差的计算使用合并标准偏差来计算，公式是 ${SE_{pool}=\\sqrt{SD_{pool}^2/n_{contr}+SD_{pool}^2/n_{exp}}}$，该公式中 ${SD_{pool}=\\sqrt{p_{pool}*(1-p_{pool})}}$ 边际值: Margin，${margin=SE_{pool}z^}$ 置信区间：Confidence Interval，${CI\\in[\\hat{d}-margin,\\ \\hat{d}+margin]}$ 实际显著性 Practical Significance 在完成统计显著性分析之后，才能进行实际显著性分析——也就是说一般分析的时候，统计显著性是实际显著性的基础条件。而实际显著性分析比较简单，一般指需要通过比较设定的 ${d_{min}}$ 值与统计显著性的置信区间——如果在在置信区间内，那么说明拒绝实际显著性假设；反之，则说明具有实际显著性。 2.3 符号检验——非参数型检验方法符号检验（即 Sign Test）,主要依据的是假设了试验成功的概率是 50% ，同时根据试验是一个二项式分布的试验。以此条件计算出概率，如果计算出的概率（ ${p_{value}}$ ）小于假设的 ${p_{critical}}$ 值，那么就是统计学显著。计算概率的方式可以使用在线计算工具。 3. 参考 A Summary of Udacity A/B Testing Course 敏感性 和 稳健性：即 sensitivity 以及 robustness，他们用于评价指标是否可用的一个参考。一般可以通过以下方式进行测量：i)使用 AA 测试；ii）回顾性分析（retrospective analysis）。一般来说这两者是逆相关，所以对于两者需要一个合适的平衡 三者统计学解释，可以参考 ${\\alpha}$ 和 ${\\beta}$，以及如何理解统计学中的 Power ： 显著性水平：在统计学假设试验中，${\\alpha}$ 表示了在零和假设 ${H_0}$ 是真的情况下，发生拒绝 ${H_0}$ 的概率；是判断发生 ${Type\\ Error\\ I}$ (即 ${P_{reject\\ null\\ hypothesis|true\\ null\\ hypothesis}}$) 的概率。通过 ${\\alpha}$ 可以计算出置信水平（ ${Confidence\\ Level=1-\\alpha}$ ） 统计功效：其统计学定义是零和假设 ${H_0}$ 错误的情况下，成功拒绝 ${H_0}$ 的概率（即 ${P_{reject\\ null\\ hypothesis|false\\ null\\ hypothesis}}$）。这里涉及到另一个术语 ${\\beta}$，它和统计功效的关系是 ${\\beta=1-statistics\\ power}$，是判断发生 ${Type\\ Error\\ II}$ (即 ${P_{not\\ reject\\ null\\ hypothesis|false\\ null\\ hypothesis}}$) 的概率 实际显著性：在 AB 测试中，该显著性主要是基于商业角度等方面考量，用于判断在变更带来多大的变化量的时候可以实施变更。一般是试验结合实际，进行设定的 ${d_{min}}$ 样本容量对统计学分析的时候，对显著性分析，统计功效的影响可以参见可视化模型Understanding Statistical Power and Significance Testing 对三种常用的分流对象来说，需要从三个角度去考量：1）作引流单元是否影响用户体验一致性；2）变更是否明显直接呈现在用户面前，例如后端服务器变化；3）分析评估中是否需要考虑用户行为作为分析角度。对于三个角度都是否定的角度，那么可以参考使用 event 来分流——例如服务器变化测量视频加载时间变更 一般情况下，很少使用单次周期的试验，通常都是使用多周期的试验。这样的好处是当面对 学习效应（learning effect） 的时候，试验能够安全得到有效的数据；同时从商业角度考量，对于新功能测试一般都不太希望得到竞争对手的干扰以及市场关注；同时也是为了避免测试样本容量全部不暴露在某个集中的时间短，造成数据不能得到有效数据，例如：节假日期间流量；另一方面，是体现该方法的优势——当进行多个试验任务时或针对相同功能展开的多个试验任务，如针对不同任务设置不同水平参数、相同功能的不同类型的试验，针对这类试验能够方便地进行纵向对比增加试验结论可信性 学习效应：即 Learning Effect，指的是当对新的变更时，因为用户差异以及用户需要花费成本去学习新的变化而导致初期表现出结果波动较大；随着时间的变化，用户行为才可能稳定 该原因的统计学解释，如下： ${P_{one\\ significant\\ result}=1-P_{non\\ significant\\ results}}$，其中 ${P_{non\\ significant\\ results}}$ 的计算需要结合指标个数： ${P_{non\\ significant\\ results}=(1-\\alpha)^n}$ ，其中 ${n}$ 为指标个数。 这里的基础条件：首先在试验中使用到的总体（Population）是足够大，同时进行分析的样本容量（Sample Size）都是大于 30（因为一般的 ${t}$ 检验的是适用于样本容量较小的试验）。因为样本容量足够大，这里可以假设通过样本几乎可以估计出总体的参数。 分析单元：它只是在分析某些指标时用到的 分母，是分析过程中需要确认每次的事件是否独立。例如：计算点击率指标时，用点击量除以页面查看量，其中分母页面查看量，即是分析单元。 RPubs - A/B Testing - Udacity Course Final Project 这里是别人对 AB 测试的流程分析，实现的方式是使用了 R 语言","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-AB%E6%B5%8B%E8%AF%95%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93Part1/"},{"title":"R 基础笔记","text":"R 和 Python 一样都是解释性、动态类型语言，因此存在相同点和差异点，为了方便学习 R 将结合 Python 的角度来了解 R。 1. 与 Python 基础应用比较以下是对 Python 和 R 基础应用的比较：| 目的 | R 方法 | Python 方法 || :——–: | :————-: | :————-: || 寻求帮助 | 使用 ?mean 表示查找 mean函数帮助，相当于使用help(&quot;mean&quot;)??plotting 表示寻求 plotting 帮助，相当于使用 help.search(&quot;plotting&quot;)typeof(object) 查看对象类型，此外还可以使用 mode | help(object) 查看函数和模块用途dir(object) 查看属性列表[^1]type(object) 查看对象类型 || 包管理 | install.packages(&quot;package_name&quot;) update.packages(ask=FALSE)remove.package(&quot;package_name&quot;) | pip install package_namepip update package_namepip remove pakcage_name || 调用包 | library(package_name) ，包名称不需要加引号 | 使用 import 语句 || 赋值方式 | 可以使用 &lt;- 或者 =，推荐使用 &lt;- | 使用 = || 变量名 | 区分大小写，可以使用 . 作为变量名一部分；如果是 . 做第一个字符，第二个字符只能是字母[^3] | 区分大小写 || 数据类型 | 基本数据对象类型：数值型、字符型、复数型、布尔型、向量其他结构对象：因子、数组、矩阵、数据框、列表、时间序列 | 基本数据对象：数值型、字符型、布尔型容器对象：列表、元组、字典、集合 || 算数运算符 | 保留了 +, -, *, /；指数标识符可以使用 ^ 或者 ** | 保留 +, -, *, /, %；但指数标识符只能使用 ** || 逻辑运算符 | &gt;, &lt;, &gt;=, &lt;=, 以及 !=；逻辑连接词 &amp;, |成员检查 %in% | &gt;, &lt;, &gt;=, &lt;=, 以及 !=；逻辑连接词 and(&amp;), or(|)成员检查 in || 代码块 | 多数情况下需要使用 {} 来区分 | 使用缩进的方式控制代码块 || 注释 | 使用 # 进行注释 | 使用 # 进行注释 || 运行脚本 | Rscript file.R | python file.py | 2. R 基本应用 工作目录 R 启动后有一个默认的工作目录，可以通过 getwd() 获取当前工作目录的绝对路径；如果需要切换工作目录，可以使用 setwd() 寻求帮助 R 的帮助功能非常丰富，除了上面讲到了常用的寻求帮助方法，还有其他方法可以使用： example(function_name) 已知一个函数名，需要确认正确使用示例 args(function_name) 获取函数中参数列表 ls() 显示对象名称 apropos(&quot;object_name&quot;) 查找匹配其输入的变量以及函数，它可以传入一个正则表达式 options() 确认环境变量 function_name 输入函数名，而不调用（即没有输入 ()）时可以查看函数代码——有时不能使用时，可以使用其他方法[^2] list.files() 显示工作目录下的文件 其他辅助功能，例如 Tab 自动补全 3. 数据处理和分析3.1 常用分析方法Python 使用 pandas 创建的 DataFrame 可以通过 head(), tail() 等方法进行访问数据，在 R 中同样存在相应的方法： head(object) tail(object) str(object) colnames(object) rownames(object) name(object) 用于返回对象的名称 summary(object) 可以使用其他 package 来完成相应的目的，psych 中的 describe 方法可以得到更加详细的统计学信息 以上方法能够了解数据的一般性信息。如果需要对数据值进行分析，R 有专门的方法可以用于计算相应的统计值： mean() median() cor() mfv() 计算众数，需要加载 modeest 包来使用该函数 table() 用于类别性数据计数统计 by() 按照某个变量分类对数据进行统计分析 如果对数据的列进行筛选，那么需要使用 $ 去访问对象的列名，对于使用切片的方式访问数据和 Python 相同，可以使用 [] 进行切片 df[, c(2, 4)]——注意以 1 为起始，负数索引不是逆向索引而是不选择对应的索引；此外数据筛选同样可以采用掩码来方式。除此之外，还可以使用函数（这里可以结合使用 dplyr 的package 来处理）来完成相应的工作： subset() filter() select() 同样在使用 R 的过程中需要对数据进行处理，例如删除缺失值、数据融合 na.omit() 用于删除具有缺失值数据 merge() 用于合并数据 此外对于数据处理完成之后，可能还需要进行保存。相应的保存函数如下： write.table() 3.2 其他分析方法3.2.1 分面可视化分面中，常用两种方法来进行分析，一种是 facet_wrap(~vriable_name) 和 另一种是facet_grid(vertical~horizontal)。这两种方法需要注意，前者如果是多个变量将导致标签位置都在列上面，如下： 3.2.2 可视化优化在进行可视化分析的时候，需要使用其他视觉信息来表达数据信息，例如点大小，色阶以及色调等方式。在使用方面可以通过调整相应的参数来进行修改图形信息，例如 ggplot 绘图中，可以通过传入 size 参数来调整图形大小， color 参数来调整颜色信息；使用 scale_colour_brewer() 来调整图形颜色以及使用添加相应的 legend 信息。 参考 Python dir() 函数 153 分钟学会 R Google’s R Style Guide ggplot 主题文档 An Introduction to reshape2 Converting data between wide and long format scale_colour_brewer • ggplot2 Legends (ggplot2)","link":"/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-R%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/"},{"title":"机器学习特征工程技巧","text":"一些特征工程的指南方法： 线性模型能够自然地学习求和以及差值的关系，但对于复杂的关系较弱，搭建相关的特征时可以从该角度上处理 比率关系在多数模型中都是难以训练的，因此搭建比率关系能够简单快速的提升模型效果 线性模型和神经网络对于 Normalized 的特征，能够取得较好效果。而基于树的模型例如随机森林以及 XGBoost，对于是否 Normalized 影响较小 树模型能够学习到特征组合的近似效果，但是对于小样本数据提前处理特征组合进行训练是有意义的 树模型不能对信息进行聚合统计，因此使用计数统计的方法是非常有效的方法","link":"/%5Bobject%20Object%5D/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%8A%80%E5%B7%A7/"},{"title":"2021-05-30","text":"线上数据全在数仓完成，涉及线下数据再使用 ETL。 对于大平台数据以及 BI 呈现的关系，还是需要弄清楚两者关系，否者就遇问题解决问题。这样的劣势会产生资源不能整合，浪费资源在 ETL 过程中。","link":"/%5Bobject%20Object%5D/%E6%89%AF%E6%B7%A1%E9%9B%86-2021-05-30/"},{"title":"[统计学习]第五章决策树","text":"决策树是一种解决分类和回归问题的方法，它是基于特征对实例进行划分以生成树结构。决策树的模型实现需要从特征选择、树生成以及剪枝等三个部分，特征选择依赖于不同的算法作出选择。在应用上决策树可以生成基于特征的规则，这种条件规则可以被应用于生产场景。 1. ID3 算法ID3 算法（Quinlan 1986）是一个自顶向下构造决策树来进行学习，构造过程需要解决的问题是“哪个特征作为树节点”。特征选择的方法是通过计算信息增益的方法进行，决策树构建是一个递归的过程。 1.1 信息增益在理解信息增益之前需要弄清楚熵（Entropy）的含义。熵在信息论和概率统计中衡量随机变量的不确定性，其意义上是对随机变量的纯度体现。 对一个离散随机变量 $X$，对每一个类别可以得到的概率为 $P(X=x_i)=p_i, i=1,2,\\cdots,n$，对于该变量的熵的计算： $H(p)=-\\displaystyle{\\sum_{i=1}^n(p_i \\log p_i)}$ 熵越大，那么说明随机变量的纯度越低，它的不确定性越大。而信息增益（Information Gain）是表示的是已知随机变量的信息下，使得目标的不确定性减少的程度。信息增益的数学表达式为 $G(Y, X)=H(Y)-H(Y|X)$，即在条件熵的降低了原有的熵大小: $$\\begin{align}G(Y,X)=&amp;H(Y) - H(Y|X) \\H(Y|X)=&amp;\\sum_{i=1}^n(p_i H(Y|X=x_i)) \\H(Y)=&amp;-\\sum_{i=1}^n(p_i \\log p_i)\\end{align}$$ 1.2 信息增益筛选特征下图是以是否打网球及其他特征统计信息，以此信息作为计算示例： 是否打网球的类别比为 $9:5$，那么在根结点上得到的熵值为 $H(\\text{PlayTennis})=-(\\frac{5}{14}\\log\\frac{5}{14} + \\frac{9}{14}\\log\\frac{9}{14})$ 计算选择不同特征的信息增益*","link":"/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%94%E7%AB%A0%E5%86%B3%E7%AD%96%E6%A0%91/"}],"tags":[{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Note","slug":"Note","link":"/tags/Note/"},{"name":"note","slug":"note","link":"/tags/note/"},{"name":"machinelearning","slug":"machinelearning","link":"/tags/machinelearning/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"统计学习","slug":"统计学习","link":"/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"},{"name":"DataAnalysis","slug":"DataAnalysis","link":"/tags/DataAnalysis/"},{"name":"PySpark","slug":"PySpark","link":"/tags/PySpark/"},{"name":"GPG","slug":"GPG","link":"/tags/GPG/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"费孝通","slug":"费孝通","link":"/tags/%E8%B4%B9%E5%AD%9D%E9%80%9A/"},{"name":"社科","slug":"社科","link":"/tags/%E7%A4%BE%E7%A7%91/"},{"name":"摄影","slug":"摄影","link":"/tags/%E6%91%84%E5%BD%B1/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"},{"name":"Markdonw","slug":"Markdonw","link":"/tags/Markdonw/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"TimeSeries","slug":"TimeSeries","link":"/tags/TimeSeries/"},{"name":"Paper","slug":"Paper","link":"/tags/Paper/"},{"name":"Machinelearning","slug":"Machinelearning","link":"/tags/Machinelearning/"},{"name":"Trick","slug":"Trick","link":"/tags/Trick/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"Statistic","slug":"Statistic","link":"/tags/Statistic/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"FeatureEngineering","slug":"FeatureEngineering","link":"/tags/FeatureEngineering/"}],"categories":[{"name":"Trick","slug":"Trick","link":"/categories/Trick/"},{"name":"Note","slug":"Note","link":"/categories/Note/"},{"name":"DataScience","slug":"DataScience","link":"/categories/DataScience/"},{"name":"MachineLearning","slug":"DataScience/MachineLearning","link":"/categories/DataScience/MachineLearning/"},{"name":"Statistics","slug":"DataScience/MachineLearning/Statistics","link":"/categories/DataScience/MachineLearning/Statistics/"},{"name":"DataAnalysis","slug":"DataScience/DataAnalysis","link":"/categories/DataScience/DataAnalysis/"},{"name":"Trick","slug":"DataScience/Trick","link":"/categories/DataScience/Trick/"},{"name":"Others","slug":"Others","link":"/categories/Others/"},{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Nonsense","slug":"Nonsense","link":"/categories/Nonsense/"}]}