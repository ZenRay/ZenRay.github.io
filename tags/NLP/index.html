<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>标签: NLP - DataSci/&gt;</title><meta property="og:type" content="blog"><meta property="og:title" content="DataSci/&gt;"><meta property="og:url" content="https://zenray.github.io/"><meta property="og:site_name" content="DataSci/&gt;"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zenray.github.io/img/og_image.png"><meta property="article:author" content="ZenRay"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zenray.github.io"},"headline":"DataSci/>","image":["https://zenray.github.io/img/og_image.png"],"author":{"@type":"Person","name":"ZenRay"},"description":"Data Science"}</script><link rel="icon" href="/img/favicon_alpha.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/zenburn.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.1.1"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo_alpha.svg" alt="DataSci/&gt;" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/categories/DataScience">数据科学</a><a class="navbar-item" href="/categories/Paper">论文阅读</a><a class="navbar-item" href="/categories/Note">笔记</a><a class="navbar-item" href="/categories/Trick">技巧</a><a class="navbar-item" href="/categories/Others">其他</a><a class="navbar-item" href="/categories/Nonsense">扯淡集</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ZenRay"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">NLP</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-02-19T15:07:07.000Z" title="2021-02-19T15:07:07.000Z">2021-02-19</time>发表</span><span class="level-item"><time dateTime="2021-04-19T15:08:51.943Z" title="2021-04-19T15:08:51.943Z">2021-04-19</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/DataScience/">DataScience</a></span><span class="level-item">18 分钟读完 (大约2666个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%5Bobject%20Object%5D/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%83%8C%E6%99%AF/">自然语言语言模型</a></h1><div class="content"><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>语言模型，其中最重要的是能够表达一个语言序列，而在验证序列是否是“自然的”。对应一个句子序列 $P(E)=P(w_1, w_2, \cdots, w_n)$ 的联合概率表示的是序列中从第一个单词到最后一个单词到序列，使用这种形式直接创建这个分布概率模型的难度较大，因此采用另一种形式——以单词组合的方式作为第二方案。<br>￼<img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gppg7ng6l2j30ml07774u.jpg" style="zoom:80%;" /><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gppg8f2yqjj30hz06cq3i.jpg"><br>以 “she went home” 为例子，改用条件概率的模式——表达计算单词随着单词变化的概率，这样间接实现序列概率计算。在这个例子中，明确使用 </s> 表示句子结尾，这样可以用于最终判断在 “she went home” 之后是否结束。因此对应的概率表达式为：</p>
<p>$P(E)=P(w_1)P(w_1|w_2)P(w_3|(w_1,w_2))=\prod_{(t=1)}^{(T=1)}P(w_t|w_1^{t-1}), w_{T+1} \text{是</s>}$</p>
<h2 id="1-1-基于计数的-N-Gram-语言模型"><a href="#1-1-基于计数的-N-Gram-语言模型" class="headerlink" title="1.1 基于计数的 N-Gram 语言模型"></a>1.1 基于计数的 N-Gram 语言模型</h2><p>基于计数的语言模型，是以最大似然数估计的方式来构建模型（Maximum Likelihood Estimation)：</p>
<p>$P_{ML}{(w_t|w_1^{t-1})}=\frac{C(w_1^{t})}{C(w_1^{(t-1)})^*}$<br>上面的意思中表示的是以出现第一个单词 $w_1$ 到 $w_{(t-1)}$ 的前序（prefix）情况下，下一个单词为 $w_t$。<br>需要注意这样创建的语言模型是基于已经遇到过的数据，因此存在“过拟合”的问题。<br>￼<img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gppg7ng6l2j30ml07774u.jpg" style="zoom:80%;" /><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gppg8f2yqjj30hz06cq3i.jpg"><br>假设用上面构建的模型去预测”i am from utah”，其结果会是 0，因为根据上面的例子 $P(w_4=\text{utah}|(w_1=\text{i}, w_2=\text{am}, w_3=\text{from})$ 是 0，导致整个结果是 0。因此这种模式直接就失去了语言模型评估语言的“自然性”的意义。为了解决这种问题，目前有两种解决方案：1）马尔可夫假设类似的原理，当前单词的出现仅由最近出现的单词决定——采用 N-gram。N-gram 模型参数由给定的 N - 1 个单词确定下一个单词，例如 unigram，的参数仅由自身确定，bigram 是由上一个单词确定，而 trigram 是由上两个单词确定。根据这个假设更新公式 $P(w_t|w_1^{t-1})\approx P_{ML}(w_t|w_{t-n+1}^{t-1})=\theta_{w_{t-n+1}^t}$，其中 $n$ 即表示是 Gram 是几元，而 $\theta$ 即是对应于 N-Gram 下给定的前 $n-1$ 个词条件下的下一个单词的概率。</p>
<p>不论是上面提到的那种方法，都可能存在从训练语料中得到组合存在限制，测试时可能出现某些组合不存在的问题。为了避免和出现  概率值情况时，导致整个句子序列的概率为 ，因此引入了 平滑（Smoothing）的方式来修正该问题。例如其中最简洁的方式——插值法（Interpolation），直接将 Unigram 和 Bigram 合并考虑以达到平滑的效果:$P(w_t|w_{t-1})=(1-\alpha)P_{ML}(w_t|w_{t-1})+\alpha P_{ML}(W_t)$。这种方式是相对来说更稳健地构建低频词概率模型之一。对于需要更复杂的上下文语言模型，例如 N 为 3，4 等的条件的概率需要使用递归的方式来构建<br>$$<br>\rm{对应于 TriGram 语法概率} \<br>P(w_t|w_{t-2}w_{t-1})=\alpha_1 P(w_t|w_{t-2}w_{t-1}) +\alpha_2 P(w_t|w_{t-1}) +\alpha_3 P(w_t) \<br>\displaystyle \sum_{i}\alpha_i = 1<br>$$<br>其中 $\alpha$ 是作为超参数，在训练过程中是使用保留语料库（Held-out corpus）中学习，是附加的训练语料库——但并没有用于设置 N Gram 语法的计数，是用于设置其他的参数。因此在处理数据上通过训练语料嫁给你 N Gram 语法的概率固定下来，在通过保留语料库训练 $\alpha$</p>
<h4 id="1-1-1-Bigram-模型示例"><a href="#1-1-1-Bigram-模型示例" class="headerlink" title="1.1.1 Bigram 模型示例"></a>1.1.1 Bigram 模型示例</h4><ol>
<li><code>&lt;s&gt; I am sam &lt;/s&gt; </code></li>
<li><code>&lt;s&gt; Sam I am &lt;/s&gt;</code></li>
<li><code>&lt;s&gt; I do not like green eggs and ham &lt;/s&gt;</code></li>
</ol>
<p>根据 NGram 公式 $P(w_t|w_1^{t-1})\approx P_{ML}(w_t|w_{t-n+1}^{t-1})=\frac{C(w_{t-n+1}^{t-1}w_t)}{C(w_{t-n+1}^{t-1})}$ ，右侧为实际统计计算方式 $C(w_{t-n+1}^{n-1}w_t)$ 表示 从 $t-n+1$ 到 $t-1$ 的单词序列 $w_{(t-n+1)}$ 计数和 单词 $w_t$ 的联合事件。</p>
<p><strong>NGram 训练语料重要性</strong><br>从两个语料训练的结果来看，不同语料训练出来的“英语句子‘模型存在差异。因此在训练时使用大语料库，多来源语料库是有必要的。毕竟N-gram 的统计模型不能在简单语料库中模拟出各种不同 风格</p>
<h2 id="2-平滑"><a href="#2-平滑" class="headerlink" title="2. 平滑"></a>2. 平滑</h2><p>在有限的训练数据集中，通过最大似然数估计来训练 N Gram 语言模型，得到的结果中必然会有稀疏性的问题。导致训练预料库存在着大量零概率 N 元语法，但实际上也会存在某些非零的概率，这种非零计数比较小的时候，最大似然数估计会产生非常糟糕的结果。因此引入的平滑的方式来解决小数据集的可变形造成的糟糕的估计结果。其内核是削减高计数的概率，用以弥补零计数的概率，从而使得概率分布不至于过于参差不齐</p>
<h3 id="2-1-Laplace-平滑——加一平滑"><a href="#2-1-Laplace-平滑——加一平滑" class="headerlink" title="2.1 Laplace 平滑——加一平滑"></a>2.1 Laplace 平滑——加一平滑</h3><p>这是一个简单的平滑方法，是在归一化为概率之前，将所有的计数加一。一般的加一方式为 $P_{Laplace}(w_i)=\frac{c_i+1}{N+V}$，其中  表示的是词汇表中单词个数。但在使用中计算概率的方式可以只调整分子不增加分母，也可以不同时地调整分子与分母。对于平滑之后对于分子的影响，可以通过调整计数 （Adjusted Count）$c^\star$ 来量化。同 MLE 用单词总数量 $N$ 来进行归一化，计算调整计数为概率的话，需要添加一个归一化因子 $\frac{N}{N+V}$，最终表达式为：<br>$$<br>c_i^\star=(c_i+1)\frac{N}{N+V} \<br>P_i^\star=(c_i+1)\frac{1}{N+V} \<br>\text{即同 MLE 一样，使用 N 进行归一化处理，得到概率}P_i^\star<br>$$<br>另一种通过相对比较的方法来解释平滑——打折(Discounting)，通过吧非零的计数调低分派给哪些为零的计数。因此前面 $c^\star$ 是被计作打折的计数，而用相对折扣(Discount) 的方式来描述平滑 $d_c=\frac{c^\star}{c}$，其中 $c$ 是原始计数。</p>
<h4 id="2-1-1-Smoothing-示例"><a href="#2-1-1-Smoothing-示例" class="headerlink" title="2.1.1 Smoothing 示例"></a>2.1.1 Smoothing 示例</h4><p>示例来源于 SALP 中的 Berkeley Restaurant Project 例子，其中包括了单词数量 $V$ 为 $1446$，</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
</tr>
</thead>
<tbody><tr>
<td>i</td>
<td>5</td>
<td>827</td>
<td>0</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>want</td>
<td>2</td>
<td>0</td>
<td>608</td>
<td>1</td>
<td>6</td>
<td>6</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>to</td>
<td>2</td>
<td>0</td>
<td>4</td>
<td>686</td>
<td>2</td>
<td>0</td>
<td>6</td>
<td>211</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>16</td>
<td>2</td>
<td>42</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>82</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>15</td>
<td>0</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p>以 Bigram 的方式计算出相应的概率值 $\P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n}{C(w_{n-1})}$ 如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
</tr>
</thead>
<tbody><tr>
<td>i</td>
<td>0.002</td>
<td>0.330</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.00079</td>
<td></td>
</tr>
<tr>
<td>want</td>
<td>0.00220</td>
<td>0.66</td>
<td>0.0011</td>
<td>0.00650.0065</td>
<td>0.0054</td>
<td>0.0011</td>
<td></td>
<td></td>
</tr>
<tr>
<td>to</td>
<td>0.00083</td>
<td>0</td>
<td>0.0017</td>
<td>0.28</td>
<td>0.00083</td>
<td>0</td>
<td>0.0025</td>
<td>0.087</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>0.0027</td>
<td>0</td>
<td>0.021</td>
<td>0.0027</td>
<td>0.0560</td>
<td></td>
</tr>
<tr>
<td>chinese</td>
<td>0.00630</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.52</td>
<td>0.0063</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>food</td>
<td>0.014</td>
<td>0</td>
<td>0.0140</td>
<td>0.00092</td>
<td>0.0037</td>
<td>0</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>lunch</td>
<td>0.00590</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.0029</td>
<td>0</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>spend</td>
<td>0.00360</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
</tr>
</tbody></table>
<p>采用 Add-one smoothing 方式，调整计数：</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
</tr>
</thead>
<tbody><tr>
<td>i</td>
<td>6</td>
<td>828</td>
<td>1</td>
<td>10</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>want</td>
<td>3</td>
<td>1</td>
<td>609</td>
<td>2</td>
<td>7</td>
<td>7</td>
<td>6</td>
<td>2</td>
</tr>
<tr>
<td>to</td>
<td>3</td>
<td>1</td>
<td>5</td>
<td>687</td>
<td>3</td>
<td>1</td>
<td>7</td>
<td>212</td>
</tr>
<tr>
<td>eat</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>17</td>
<td>3</td>
<td>43</td>
<td>1</td>
</tr>
<tr>
<td>chinese</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>83</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>food</td>
<td>16</td>
<td>1</td>
<td>16</td>
<td>1</td>
<td>2</td>
<td>5</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>lunch</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>spend</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>调整之后的 Bigram 概率值 $P_{Laplace}(w_i)=\frac{c_i+1}{N+V}$ ：<br>|         | i       | want    | to      | eat     | chinese | food    | lunch   | spend   |<br>| ——- | ——- | ——- | ——- | ——- | ——- | ——- | ——- | ——- |<br>| i       | 0.0015  | 0.21    | 0.00025 | 0.0025  | 0.00025 | 0.00025 | 0.00025 | 0.00075 |<br>| want    | 0.0013  | 0.00042 | 0.26    | 0.00084 | 0.0029  | 0.0029  | 0.0025  | 0.00084 |<br>| to      | 0.00078 | 0.00026 | 0.0013  | 0.18    | 0.00078 | 0.00026 | 0.0018  | 0.055   |<br>| eat     | 0.00046 | 0.00046 | 0.0014  | 0.00046 | 0.0078  | 0.0014  | 0.02    | 0.00046 |<br>| chinese | 0.0012  | 0.00062 | 0.00062 | 0.00062 | 0.00062 | 0.052   | 0.0012  | 0.00062 |<br>| food    | 0.0063  | 0.00039 | 0.0063  | 0.00039 | 0.00079 | 0.002   | 0.00039 | 0.00039 |<br>| lunch   | 0.0017  | 0.00056 | 0.00056 | 0.00056 | 0.00056 | 0.0011  | 0.00056 | 0.00056 |<br>| spend   | 0.0012  | 0.00058 | 0.0012  | 0.00058 | 0.00058 | 0.00058 | 0.00058 | 0.00058 |<br>如果对平滑方法对影响进行分析，即平滑之后调整的计数情况 $c^\star(w_{n-1}w_n)=\frac{[C(w_{n-1}w_n)+1]\times C(w_{n-1})}{C(w_{n-1}+V)}$:<br>|         | i    | want  | to    | eat   | chinese | food | lunch | spend |<br>| ——- | —- | —– | —– | —– | ——- | —- | —– | —– |<br>| i       | 3.8  | 527   | 0.64  | 6.4   | 0.64    | 0.64 | 0.64  | 1.9   |<br>| want    | 1.2  | 0.39  | 238   | 0.78  | 2.7     | 2.7  | 2.3   | 0.78  |<br>| to      | 1.9  | 0.63  | 3.1   | 430   | 1.9     | 0.63 | 4.4   | 133   |<br>| eat     | 0.34 | 0.34  | 1     | 0.34  | 5.8     | 1    | 15    | 0.34  |<br>| chinese | 0.2  | 0.098 | 0.098 | 0.098 | 0.098   | 8.2  | 0.2   | 0.098 |<br>| food    | 6.9  | 0.43  | 6.9   | 0.43  | 0.86    | 2.2  | 0.43  | 0.43  |<br>| lunch   | 0.57 | 0.19  | 0.19  | 0.19  | 0.19    | 0.38 | 0.19  | 0.19  |<br>| spend   | 0.32 | 0.16  | 0.32  | 0.16  | 0.16    | 0.16 | 0.16  | 0.16  |</p>
<p>可以看出调整之后对原来的计数数量影响较大，前缀词例如 $C(want\ to)$ 的数量从 $609$ 变为了 $238$，体现在 $P(to|want)$ 的概率降到 $0.26$，而后缀词则在增加 $</p>
<h1 id="3-评估指标"><a href="#3-评估指标" class="headerlink" title="3. 评估指标"></a>3. 评估指标</h1><p>外在评测：在语言模型中评估模型的最佳方式是，将语言模型嵌入到具体的应用任务测试应用的总体性能的方式，即端到端的外在评测(Extrinsic Evaluation)，也称为现实评测（in vivo evaluation)。<br>内在评测(Intrinsic Evaluation)，是与应用任务无关的模型质量评测。</p>
<p>前者能显性评估两个模型的性能，但是评测消耗的代价很高，应该常采用内在评测。困惑度(Perplexity) 是对于 N 元语法模型常见的内在评测度量指标，虽然这个指标不能保证任务模型性能得到真实改进，但是应用任务的性能改进是和困惑度改进具有相关关系。而且它是一个快速的检验算法。</p>
<p>困惑度(Perplexity， PP) 是语言模型指派给测试集的概率函数。对一个句子序列来说，就是一个语言模型下的序列概率函数。困惑度可以理解为，如果每个时间步都根据语言模型计算的概率分布随机挑词，那么平均情况下，挑多少个词才能挑到正确的那个。其公式如下：<br>$$<br>PP(W)=P(w_1w_2\cdots w_n)^{-\frac{1}{N}} \<br>=\sqrt[N]{\frac{1}{P(w_1w_2\cdots w_n)}} \<br>=\sqrt[N]{\frac{1}{\displaystyle{\prod_{i=1}^N}P(w_i|(w_1\cdots w_{i-1}))}} \<br>\text{第三个等式是使用链式展开}<br>$$</p>
<p>另一种研究困惑度的方式，是通过加权平均转移因子(Weighted Average Branching Factor)，它的表示基础是转移因子(Branching Factor)——任何一个单词后面可能连续的单词数量。举个以数字字符串例子来说，每个数字字符出现概率为 $\frac{1}{10}$，以这种方式得到的困惑去为 $PP(W)=(\frac{1}{10}^N)^{\frac{-1}{N}}=10$。但是实际情况下 0 发生的概率比较高，例如训练数据集中 0 发生了 91 次，而其他数字数据发生了 1 次。那么在测试数据中 <code>0003000</code>，测试集中的预测掉一个数字为 0 的困惑度比较小，也就是有较高的概率为 0。在这个例子中，虽然转移因子依然是 10，但是困惑度或者使用加权平均转移因子更小。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-18T02:54:47.000Z" title="2021-01-18T02:54:47.000Z">2021-01-18</time>发表</span><span class="level-item"><time dateTime="2021-04-11T10:44:15.854Z" title="2021-04-11T10:44:15.854Z">2021-04-11</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/DataScience/">DataScience</a><span> / </span><a class="link-muted" href="/categories/DataScience/Trick/">Trick</a></span><span class="level-item">2 分钟读完 (大约358个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%5Bobject%20Object%5D/NLP-StanfordNLP%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/">Python 调用 StanfordNLP 服务</a></h1><div class="content"><p>Stanford NLP Group 提供了自然语言软件处理工具，目前（2021年）能够非常非常方便快捷的调用 POS、NER 等功能。本例是以 Server 方式提供服务，让 Python 能够调用相关服务。相关的步骤如下：</p></div><a class="article-more button is-small size-small" href="/%5Bobject%20Object%5D/NLP-StanfordNLP%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/#more">阅读更多</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/%5Bobject%20Object%5D/%E8%AE%BA%E6%96%87-AttentionIsAllYouNeed/"><img class="fill" src="/gallery/thumbnails/self_attention.png" alt="Attention Is All You Need 笔记"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-11T08:05:53.000Z" title="2021-01-11T08:05:53.000Z">2021-01-11</time>发表</span><span class="level-item"><time dateTime="2021-04-11T13:09:48.845Z" title="2021-04-11T13:09:48.845Z">2021-04-11</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a></span><span class="level-item">10 分钟读完 (大约1470个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%5Bobject%20Object%5D/%E8%AE%BA%E6%96%87-AttentionIsAllYouNeed/">Attention Is All You Need 笔记</a></h1><div class="content"><h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h1><p>在解决翻译问题时，如果句子长度过长 Encoder-Decoder 的模型也不能得到良好的结果。这样的问题就是长序列会遇到的问题，其主要原因是序列过长时，向后处理序列时前面的序列会出现“遗忘”问题。通过 Attention 的方式可以缓解这种问题，增加长记忆的能力。</p></div><a class="article-more button is-small size-small" href="/%5Bobject%20Object%5D/%E8%AE%BA%E6%96%87-AttentionIsAllYouNeed/#more">阅读更多</a></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/24636410?s=60&amp;v=4" alt="ZenRay"></figure><p class="title is-size-4 is-block line-height-inherit">ZenRay</p><p class="is-size-6 is-block">Zen</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Cheng Du</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">54</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">10</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">30</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ZenRay" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github1" href="https://github.com/ZenRay"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github2" href="https://github.com/RayRi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/ray_z018/"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/DataAnalysis/"><span class="tag">DataAnalysis</span><span class="tag is-grey-lightest">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeatureEngineering/"><span class="tag">FeatureEngineering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPG/"><span class="tag">GPG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GitHub/"><span class="tag">GitHub</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HBase/"><span class="tag">HBase</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MachineLearning/"><span class="tag">MachineLearning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machinelearning/"><span class="tag">Machinelearning</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdonw/"><span class="tag">Markdonw</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Note/"><span class="tag">Note</span><span class="tag is-grey-lightest">29</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pandas/"><span class="tag">Pandas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper/"><span class="tag">Paper</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PySpark/"><span class="tag">PySpark</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/R/"><span class="tag">R</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistic/"><span class="tag">Statistic</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TimeSeries/"><span class="tag">TimeSeries</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Trick/"><span class="tag">Trick</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machinelearning/"><span class="tag">machinelearning</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/note/"><span class="tag">note</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%91%84%E5%BD%B1/"><span class="tag">摄影</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A4%BE%E7%A7%91/"><span class="tag">社科</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"><span class="tag">统计学习</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%B9%E5%AD%9D%E9%80%9A/"><span class="tag">费孝通</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/04/"><span class="level-start"><span class="level-item">四月 2021</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/03/"><span class="level-start"><span class="level-item">三月 2021</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/02/"><span class="level-start"><span class="level-item">二月 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">十月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">九月 2020</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/DataScience/"><span class="level-start"><span class="level-item">DataScience</span></span><span class="level-end"><span class="level-item tag">30</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/DataScience/DataAnalysis/"><span class="level-start"><span class="level-item">DataAnalysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/DataScience/MachineLearning/"><span class="level-start"><span class="level-item">MachineLearning</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/DataScience/MachineLearning/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/DataScience/Trick/"><span class="level-start"><span class="level-item">Trick</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Nonsense/"><span class="level-start"><span class="level-item">Nonsense</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Note/"><span class="level-start"><span class="level-item">Note</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Others/"><span class="level-start"><span class="level-item">Others</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Trick/"><span class="level-start"><span class="level-item">Trick</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><a class="media-left" href="/%5Bobject%20Object%5D/%E6%89%AF%E6%B7%A1%E9%9B%86-2021-05-30/"><p class="image is-64x64"><img class="fill" src="https://tva1.sinaimg.cn/large/008i3skNgy1gr0usvp1qij31400u0b2d.jpg" alt="2021-05-30"></p></a><div class="media-content size-small"><p><time dateTime="2021-05-30T15:05:05.000Z">2021-05-30</time></p><p class="title is-6"><a class="link-muted" href="/%5Bobject%20Object%5D/%E6%89%AF%E6%B7%A1%E9%9B%86-2021-05-30/">2021-05-30</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Nonsense/">Nonsense</a></p></div></article><article class="media"><a class="media-left" href="/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%94%E7%AB%A0%E5%86%B3%E7%AD%96%E6%A0%91/"><p class="image is-64x64"><img class="fill" src="https://tva1.sinaimg.cn/large/008i3skNgy1gq7c2w73omj30bu07idgc.jpg" alt="[统计学习]第五章决策树"></p></a><div class="media-content size-small"><p><time dateTime="2021-05-05T01:54:52.000Z">2021-05-05</time></p><p class="title is-6"><a class="link-muted" href="/%5Bobject%20Object%5D/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%94%E7%AB%A0%E5%86%B3%E7%AD%96%E6%A0%91/">[统计学习]第五章决策树</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/DataScience/">DataScience</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-04-25T02:31:51.000Z">2021-04-25</time></p><p class="title is-6"><a class="link-muted" href="/%5Bobject%20Object%5D/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-R%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/">R 基础笔记</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/DataScience/">DataScience</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-04-21T09:42:10.000Z">2021-04-21</time></p><p class="title is-6"><a class="link-muted" href="/%5Bobject%20Object%5D/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%8A%80%E5%B7%A7/">机器学习特征工程技巧</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/DataScience/">DataScience</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-04-19T17:33:33.000Z">2021-04-20</time></p><p class="title is-6"><a class="link-muted" href="/%5Bobject%20Object%5D/SQL-SQL%E7%AC%94%E8%AE%B0Part1/">SQL 笔记Part1: DBMS</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/DataScience/">DataScience</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo_alpha.svg" alt="DataSci/&gt;" height="28"></a><p class="size-small"><span>&copy; 2021 ZenRay</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ZenRay"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>